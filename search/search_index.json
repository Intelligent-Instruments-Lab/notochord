{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"notochord","text":""},{"location":"#notochord-documentation-paper-video","title":"Notochord (Documentation | Paper | Video)","text":"<p>Notochord is a neural network model for MIDI performances. This package contains the training and inference model implemented in pytorch, as well as interactive MIDI processing apps using iipyper. </p> <p>API Reference</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Using your python environment manager of choice (e.g. virtualenv, conda), make a new environment with a Python version at least 3.10. Then <code>pip install notochord</code>.</p> <p>For developing <code>notochord</code>, see our dev repo</p>"},{"location":"#install-fluidsynth-optional","title":"Install fluidsynth (optional)","text":"<p>fluidsynth is a General MIDI synthesizer which you can install from the package manager. On macOS: <pre><code>brew install fluidsynth\n</code></pre> fluidsynth needs a soundfont to run, like this one: https://drive.google.com/file/d/1-cwBWZIYYTxFwzcWFaoGA7Kjx5SEjVAa/view</p> <p>You can run fluidsynth in a terminal. For example, <code>fluidsynth -v -o midi.portname=\"fluidsynth\" -o synth.midi-bank-select=mma ~/'Downloads/soundfonts/Timbres of Heaven (XGM) 4.00(G).sf2'</code></p>"},{"location":"#notochord-homunculus","title":"Notochord Homunculus","text":"<p>Notochord includes several iipyper apps which can be run in a terminal. They have a clickable text-mode user interface and connect directly to MIDI ports, so you can wire them up to your controllers, DAW, etc.</p> <p>The <code>homunculus</code> provides a text-based graphical interface to manage multiple input, harmonizing or autonomous notochord voices: <pre><code>notochord homunculus\n</code></pre> You can set the MIDI in and out ports with <code>--midi-in</code> and <code>--midi-out</code>. If you use a General MIDI synthesizer like fluidsynth, you can add <code>--send-pc</code> to also send program change messages. More information in the Homunculus docs, or run <code>notochord homunculus --help</code></p> <p>If you are using fluidsynth as above, try: <pre><code>notochord homunculus --send-pc --midi-out fluidsynth --thru\n</code></pre></p> <p>Note: on windows, there are no virtual MIDI ports and no system MIDI loopback, so you may need to attach some MIDI devices or run a loopback driver like loopMIDI before starting the app.</p> <p>If you pass homunculus a MIDI file using the <code>--midi-prompt</code> flag, it will play as if continuing after the end of that file.</p> <p>Adding the <code>--punch-in</code> flag will automatically switch voices to input mode when MIDI is received and back to auto after some time passes.</p>"},{"location":"#python-api","title":"Python API","text":"<p>See the docs for <code>Notochord.feed</code> and <code>Notochord.query</code> for the low-level Notochord inference API which can be used from Python code. <code>notochord/app/simple_harmonizer.py</code> provides a minimal example of how to build an interactive app.</p>"},{"location":"#osc-server","title":"OSC server","text":"<p>You can also expose the inference API over Open Sound Control: <pre><code>notochord server\n</code></pre> this will run notochord and listen continously for OSC messages.</p>"},{"location":"#tidal-interface","title":"Tidal interface","text":"<p>see <code>notochord/tidalcycles</code> in iil-examples repo (updated examples coming soon):</p> <p>add <code>Notochord.hs</code> to your tidal boot file. Probably replace the <code>tidal &lt;- startTidal</code> line with something like: <pre><code>:script ~/iil-examples/notochord/tidalcycles/Notochord.hs\n\nlet sdOscMap = (superdirtTarget, [superdirtShape])\nlet oscMap = [sdOscMap,ncOscMap]\n\ntidal &lt;- startStream defaultConfig {cFrameTimespan = 1/240} oscMap\n</code></pre></p> <p>In a terminal, start the python server as described above.</p> <p>In Supercollider, step through <code>examples/notochord/tidalcycles/tidal-notochord-demo.scd</code> which will receive from Tidal, talk to the python server, and send MIDI on to a synthesizer. There are two options, either send to fluidsynth to synthesize General MIDI, or specify your own mapping of instruments to channels and send on to your own DAW or synth.</p>"},{"location":"#train-your-own-notochord-model-gpu-recommended","title":"Train your own Notochord model (GPU recommended)","text":"<p>preprocess the data: <pre><code>python notochord/scripts/lakh_prep.py --data_path /path/to/midi/files --dest_path /path/to/data/storage\n</code></pre> launch a training job: <pre><code>python notochord/train.py --data_dir /path/to/data/storage --log_dir /path/for/tensorboard/logs --model_dir /path/for/checkpoints --results_dir /path/for/other/logs train\n</code></pre> progress can be monitored via tensorboard.</p>"},{"location":"reference/notochord/__init__/","title":"init","text":""},{"location":"reference/notochord/data/","title":"Data","text":""},{"location":"reference/notochord/data/#notochord.data.MIDIDataset","title":"<code>MIDIDataset</code>","text":"<p>             Bases: <code>Dataset</code></p> Source code in <code>src/notochord/data.py</code> <pre><code>class MIDIDataset(Dataset):\n    def __init__(self, data_dir, batch_len, \n        transpose=5, speed=0.1, glob='**/*.pkl', test_len=2048,\n        onsets_only=False, remap_instruments=True):\n        \"\"\"\n        \"\"\"\n        super().__init__()\n        dirs = data_dir.split(',')\n        self.files = []\n        for d in dirs:\n            self.files.extend(list(Path(d).glob(glob)))\n            # print(self.files)\n        self.batch_len = batch_len\n        self.transpose = transpose\n        self.speed = speed\n        self.start_token = 128\n        self.n_anon = 32 # this needs to match n_instruments in model.py\n        self.prog_start_token = 0\n        self.testing = False\n        self.max_test_len = test_len\n        self.onsets_only = onsets_only\n        self.remap_instruments = remap_instruments\n\n    def __len__(self):\n        return len(self.files)\n\n    def is_melodic(self, program):\n        orig_program = program%1000\n        return (orig_program&lt;=128) | (orig_program&gt;256)\n\n    def is_anon(self, program):\n        return program &gt; 256\n\n    def _remap_anonymous_instruments(self, program: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Randomly map instruments to additional \u2018anonymous\u2019 melodic and drum identities\n        with a probability of 10% per instrument, without replacement. \n        Also map any parts &gt; 256 to appropriate anonymous ids.\n        \"\"\"\n        is_melodic = self.is_melodic(program)\n        is_anon = self.is_anon(program)\n        named_melodic = list(program.masked_select(is_melodic &amp; ~is_anon).unique())\n        anon_melodic = list(program.masked_select(is_melodic &amp; is_anon).unique())\n        named_drum = list(program.masked_select(~is_melodic &amp; ~is_anon).unique())\n        anon_drum = list(program.masked_select(~is_melodic &amp; is_anon).unique())\n\n        anon_melodic_start = 257\n        anon_drum_start = anon_melodic_start + self.n_anon\n        perm_anon_melodic = torch.randperm(self.n_anon) + anon_melodic_start \n        perm_anon_drum = torch.randperm(self.n_anon) + anon_drum_start \n\n        for pr in named_melodic:\n            if torch.rand((1,)) &lt; 0.1:\n                anon_melodic.append(pr)\n        for pr in named_drum:\n            if torch.rand((1,)) &lt; 0.1:\n                anon_drum.append(pr)\n\n        new_program = program.clone()\n\n        if len(anon_melodic)&gt;self.n_anon:\n            print(f'warning: {len(anon_melodic)} &gt; {self.n_anon} anon melodic instruments')\n        if len(anon_drum)&gt;self.n_anon:\n            print(f'warning: {len(anon_drum)} &gt; {self.n_anon} anon drum instruments')\n\n        i = 0\n        for pr in anon_melodic:\n            new_program[program==pr] = perm_anon_melodic[i%self.n_anon]\n            i += 1\n        i = 0\n        for pr in anon_drum:\n            new_program[program==pr] = perm_anon_drum[i%self.n_anon]\n            i += 1\n\n        # print(new_program.unique())\n\n        return new_program\n\n    def __getitem__(self, idx):\n        f = self.files[idx]\n        item = torch.load(f)\n        program = item['program'] # 1-d LongTensor of MIDI programs\n        # 0 is unused\n        # (128-256 are drums)\n        # 257+ are 'true anonymous' (no program change on track)\n        # (drums with no PC are just mapped to 129)\n        # N + 1000*K is the Kth additional part for instrument N\n        pitch = item['pitch'] # 1-d LongTensor of MIDI pitches 0-127\n        time = item['time'] # 1-d DoubleTensor of absolute times in seconds\n        velocity = item['velocity'] # 1-d LongTensor of MIDI velocities 0-127\n\n        assert len(pitch) == len(time)\n\n        if self.onsets_only:\n            b = velocity &gt; 0\n            program, pitch, time, velocity = (\n                program[b], pitch[b], time[b], velocity[b])\n\n        program, pitch, time, velocity = self.data_augmentation(\n            program, pitch, time, velocity)\n\n        # sort (using argsort on time and indexing the rest)\n        # compute delta time\n        time, idx = time.sort()\n        time = torch.cat((time.new_zeros((1,)), time)).diff(1).float()\n        velocity = velocity[idx]\n        program = program[idx]\n        pitch = pitch[idx]\n\n        # pad with start tokens, zeros\n        # always pad with batch_len so that end tokens don't appear in a biased\n        # location\n        pad = 0 if self.testing else self.batch_len-1#max(0, self.batch_len-len(pitch))\n        program = torch.cat((\n            program.new_full((1,), self.prog_start_token),\n            program,\n            program.new_zeros((pad,))))\n        pitch = torch.cat((\n            pitch.new_full((1,), self.start_token),\n            pitch,\n            pitch.new_zeros((pad,))))\n        time = torch.cat((\n            time.new_zeros((1,)),\n            time,\n            time.new_zeros((pad,))))\n        velocity = torch.cat((\n            velocity.new_zeros((1,)),\n            velocity,\n            velocity.new_zeros((pad,))))\n        # end signal: nonzero for last event\n        end = torch.zeros_like(program)\n        end[-pad-1:] = 1\n        # compute binary mask for the loss\n        mask = torch.ones_like(program, dtype=torch.bool)\n        if pad &gt; 0:\n            mask[-pad:] = False\n\n        if self.testing:\n            sl = slice(0, self.max_test_len)\n        else:\n            # random slice\n            i = random.randint(0, len(pitch)-self.batch_len)\n            sl = slice(i, i+self.batch_len)\n        program = program[sl]\n        pitch = pitch[sl]\n        time = time[sl]\n        velocity = velocity[sl]\n        end = end[sl]\n        mask = mask[sl]\n\n        return {\n            'mask':mask,\n            'end':end,\n            'instrument':program,\n            'pitch':pitch,\n            'time':time,\n            'velocity':velocity\n        }\n\n    def velocity_dequantize(self, velocity):\n        velocity = velocity.float()\n        velocity = (\n            velocity + \n            (torch.rand_like(velocity, dtype=torch.float)-0.5) * ((velocity&gt;0) &amp; (velocity&lt;127)).float()\n            ).clamp(0., 127.)\n        return velocity\n\n    def velocity_curve(self, velocity):\n        # take care not to map any positive values closer to 0 than 1\n        to_curve = (velocity &gt;= 0.5)\n        velocity[to_curve] -= 0.5\n        velocity[to_curve] /= 126.5\n        velocity[to_curve] = velocity[to_curve] ** (2**(torch.randn((1,))/3))\n        velocity[to_curve] *= 126.5\n        velocity[to_curve] += 0.5\n        return velocity\n\n    def data_augmentation(self, program, pitch, time, velocity):\n        \"\"\"override this in subclass for different data augmentation\"\"\"\n        # random transpose avoiding out of range notes\n        transpose_down = min(self.transpose, pitch.min().item())\n        transpose_up = min(self.transpose, 127-pitch.max())\n        transpose = (\n            random.randint(-transpose_down, transpose_up)\n            * self.is_melodic(program).long() # don't transpose drums\n        )\n        pitch = pitch + transpose\n\n        # scramble anonymous and extra parts to 'anonymous melodic' and 'anonymous drum' parts\n        if self.remap_instruments:\n            program = self._remap_anonymous_instruments(program)\n\n        time_margin = 1e-3\n\n        # dequantize: add noise up to +/- margin\n        # move note-ons later, note-offs earlier\n        time = (time + \n            torch.rand_like(time) * ((velocity==0).double()*2-1) * time_margin\n        )\n        # random augment tempo\n        time = time * (1 + random.random()*self.speed*2 - self.speed)\n\n        velocity = self.velocity_dequantize(velocity)\n        velocity = self.velocity_curve(velocity)\n\n        return program, pitch, time, velocity\n</code></pre>"},{"location":"reference/notochord/data/#notochord.data.MIDIDataset.__init__","title":"<code>__init__(data_dir, batch_len, transpose=5, speed=0.1, glob='**/*.pkl', test_len=2048, onsets_only=False, remap_instruments=True)</code>","text":"Source code in <code>src/notochord/data.py</code> <pre><code>def __init__(self, data_dir, batch_len, \n    transpose=5, speed=0.1, glob='**/*.pkl', test_len=2048,\n    onsets_only=False, remap_instruments=True):\n    \"\"\"\n    \"\"\"\n    super().__init__()\n    dirs = data_dir.split(',')\n    self.files = []\n    for d in dirs:\n        self.files.extend(list(Path(d).glob(glob)))\n        # print(self.files)\n    self.batch_len = batch_len\n    self.transpose = transpose\n    self.speed = speed\n    self.start_token = 128\n    self.n_anon = 32 # this needs to match n_instruments in model.py\n    self.prog_start_token = 0\n    self.testing = False\n    self.max_test_len = test_len\n    self.onsets_only = onsets_only\n    self.remap_instruments = remap_instruments\n</code></pre>"},{"location":"reference/notochord/data/#notochord.data.MIDIDataset.data_augmentation","title":"<code>data_augmentation(program, pitch, time, velocity)</code>","text":"<p>override this in subclass for different data augmentation</p> Source code in <code>src/notochord/data.py</code> <pre><code>def data_augmentation(self, program, pitch, time, velocity):\n    \"\"\"override this in subclass for different data augmentation\"\"\"\n    # random transpose avoiding out of range notes\n    transpose_down = min(self.transpose, pitch.min().item())\n    transpose_up = min(self.transpose, 127-pitch.max())\n    transpose = (\n        random.randint(-transpose_down, transpose_up)\n        * self.is_melodic(program).long() # don't transpose drums\n    )\n    pitch = pitch + transpose\n\n    # scramble anonymous and extra parts to 'anonymous melodic' and 'anonymous drum' parts\n    if self.remap_instruments:\n        program = self._remap_anonymous_instruments(program)\n\n    time_margin = 1e-3\n\n    # dequantize: add noise up to +/- margin\n    # move note-ons later, note-offs earlier\n    time = (time + \n        torch.rand_like(time) * ((velocity==0).double()*2-1) * time_margin\n    )\n    # random augment tempo\n    time = time * (1 + random.random()*self.speed*2 - self.speed)\n\n    velocity = self.velocity_dequantize(velocity)\n    velocity = self.velocity_curve(velocity)\n\n    return program, pitch, time, velocity\n</code></pre>"},{"location":"reference/notochord/distributions/","title":"Distributions","text":""},{"location":"reference/notochord/distributions/#notochord.distributions.CensoredMixtureLogistic","title":"<code>CensoredMixtureLogistic</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>src/notochord/distributions.py</code> <pre><code>class CensoredMixtureLogistic(nn.Module):\n    def __init__(self, n, res=1e-2, lo='-inf', hi='inf', \n            sharp_bounds=(1e-4,2e3), init=None):\n        super().__init__()\n        self.n = n\n        self.res = res\n        self.sharp_bounds = sharp_bounds\n        self.register_buffer('lo', torch.tensor(float(lo)))\n        self.register_buffer('hi', torch.tensor(float(hi)))\n        # TODO: init is not general-purpose\n        if init=='time':\n            self.bias = nn.Parameter(torch.cat((\n                torch.zeros(n), torch.logspace(-3,1,n), torch.zeros(n)\n                )))\n        elif init=='velocity':\n            self.bias = nn.Parameter(torch.cat((\n                torch.zeros(n), torch.linspace(0,127,n), torch.zeros(n)\n                )))\n        else:\n            self.bias = nn.Parameter(torch.cat((\n                torch.zeros(n), torch.randn(n), torch.zeros(n)\n                )))\n\n    @property\n    def n_params(self):\n        return self.n*3\n\n    def get_params(self, h):\n        assert h.shape[-1] == self.n_params\n        h = h+self.bias\n        # get parameters from unconstrained hidden state:\n        logit_pi, loc, log_s = torch.chunk(h, 3, -1)\n        # mixture coefficients\n        log_pi = logit_pi - logit_pi.logsumexp(-1,keepdim=True)\n        # location\n        loc = loc.clamp(self.lo-10*self.res, self.hi+10*self.res)\n        # sharpness\n        s = F.softplus(log_s).clamp(*self.sharp_bounds)\n        return log_pi, loc, s\n\n\n    def forward(self, h, x):\n        \"\"\"log prob of x under distribution parameterized by h\n        Args:\n            h: Tensor[...,n_params]\n            x: Tensor[...]\n            \"...\" dims must broadcast\n        \"\"\"\n        log_pi, loc, s = self.get_params(h)    \n\n        d = self.res/2\n        x = x.clamp(self.lo, self.hi)[...,None]\n        x_ = (x - loc) * s\n        sd = s*d\n\n        # # censoring\n        lo_cens = x &lt;= self.lo+d\n        hi_cens = x &gt;= self.hi-d\n        ones = torch.ones_like(s)\n        zeros = torch.zeros_like(s)\n\n        diff_term = torch.where(lo_cens | hi_cens, \n            ones, sd.exp() - (-sd).exp()\n            ).log()\n        minus_sp_term = torch.where(hi_cens, -sd, F.softplus(-sd-x_))\n        plus_sp_term = torch.where(lo_cens, zeros, x_ + F.softplus(sd-x_))\n\n        log_delta_cdf = diff_term - minus_sp_term - plus_sp_term\n\n        # log prob\n        r = {\n            'log_prob': (log_pi + log_delta_cdf).logsumexp(-1)\n        }\n        # diagnostics\n        with torch.no_grad():\n            ent = D.Categorical(logits=log_pi).entropy()\n            r |= {\n                # 'min_sharpness': s.min(),\n                'max_sharpness': s.max(),\n                'mean_sharpness': (s*log_pi.exp()).sum(-1).mean(),\n                # 'min_entropy': ent.min(),\n                # 'max_entropy': ent.max(),\n                'mean_cmp_entropy': ent.mean(),\n                'marginal_cmp_entropy': D.Categorical(\n                    log_pi.exp().mean(list(range(log_pi.ndim-1)))).entropy(),\n                # 'min_loc': loc.min(),\n                # 'max_loc': loc.max()\n            }\n        return r\n\n    def cdf(self, h, x):\n        \"\"\"\n        Args:\n            h: Tensor[...,n_params]\n            x: Tensor[...]\n                `h` should broadcast with `x[...,None]`\n        Returns:\n            cdf: Tensor[...] (shape of `x` broadcasted with `h[...,0]`)\n        \"\"\"\n        if not isinstance(x, torch.Tensor):\n            x = torch.tensor(x)\n        log_pi, loc, s = self.get_params(h)  \n        cdfs = self.cdf_components(loc, s, x)\n        cdf = (cdfs * log_pi.softmax(-1)).sum(-1)\n        return cdf\n\n    def cdf_components(self, loc, s, x):\n        x_ = (x[...,None] - loc) * s\n        return x_.sigmoid()       \n\n    # TODO: 'discrete_sample' method which would re-quantize and then allow\n    # e.g. nucleus sampling on the categorical distribution?\n    def sample(self, h, truncate=None, shape=None, \n        weight_top_p=None, component_temp=None, bias=None, \n        truncate_quantile=None, quantile_k=128, eps=1e-9\n        ):\n        \"\"\"\n        Args:\n            h: Tensor[..., n_params]\n            truncate: Optional[Tuple[float, float]]. lower and upper bound for truncation.\n            shape: Optional[int]. additional sample shape to be prepended to dims.\n            weight_top_p: top_p (\"nucleus\") filtering for mixture weights.\n                default is 1 (no change to distribution). 0 would sample top\n                component (after truncation) only.\n            component_temp: Optional[float]. sampling temperature of each mixture \n                component. default is 1. 0 would sample component location only,\n                ignoring sharpness.\n            bias: applied outside of truncation but inside of clamping,\n                useful e.g. for latency correction when sampling delta-time\n            truncate_quantile: truncate the distribution again by quantile,\n                after the effects of truncate, weight_top_p and component_temp.\n            quantile_k: truncate_quantile is implemented by drawing this many\n                samples and sorting them  \n        Returns:\n            Tensor[*shape,...] (h without last dimension, prepended with `shape`)\n        \"\"\"\n        if truncate_quantile == (0,1):\n            truncate_quantile = None\n\n        if truncate is None:\n            truncate = (-torch.inf, torch.inf)\n        # early out in the single possibility case\n        if truncate[0] == truncate[1]:\n            return torch.tensor(\n                [truncate[0]]*shape if shape is not None else truncate[0])\n        truncate = torch.tensor(truncate)\n\n        if shape is None:\n            unwrap = True\n            shape = 1\n        else:\n            unwrap = False\n\n        if truncate_quantile is not None:\n            # draw k samples\n            shape = shape * quantile_k\n\n        if component_temp is None:\n            component_temp = 1\n\n        if bias is None:\n            bias = 0\n\n        log_pi, loc, s = self.get_params(h)\n        s = s/component_temp\n        scale = 1/s\n\n        # cdfs: [...,bound,component]\n        cdfs = self.cdf_components(loc[...,None,:], s[...,None,:], truncate) \n        # prob. mass of each component witin bounds\n        trunc_probs = cdfs[...,1,:] - cdfs[...,0,:] # [...,component]\n        probs = log_pi.exp() * trunc_probs # reweighted mixture component probs\n        if weight_top_p is not None:\n            # reweight with top_p\n            probs = reweight_top_p(probs+eps, weight_top_p)\n\n        probs = probs.clamp(eps, 1)\n        c = D.Categorical(probs).sample((shape,))\n        # move sample dimension first\n        loc = loc.movedim(-1, 0).gather(0, c)\n        scale = scale.movedim(-1, 0).gather(0, c)\n        upper = cdfs[...,1,:].movedim(-1, 0).gather(0, c)\n        lower = cdfs[...,0,:].movedim(-1, 0).gather(0, c)\n\n        u = torch.rand(shape, *h.shape[:-1])\n        # truncate\n        u = u * (upper-lower) + lower\n\n        u = u.clamp(eps, 1-eps)\n\n        x = loc + bias - scale * (1/u - 1).log()\n\n        if truncate_quantile is not None:\n            x = x.sort(dim=0).values\n\n            # print(truncate_quantile, list(x_.item() for x_ in x))\n\n            idx = categorical_sample(\n                x.new_zeros(x.shape[0]), \n                truncate_quantile=truncate_quantile)\n\n            x = x[idx]\n\n        x = x.clamp(self.lo, self.hi)\n        x = x.clamp(*truncate)\n        return x[0] if unwrap else x\n</code></pre>"},{"location":"reference/notochord/distributions/#notochord.distributions.CensoredMixtureLogistic.cdf","title":"<code>cdf(h, x)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>h</code> <p>Tensor[...,n_params]</p> required <code>x</code> <p>Tensor[...] <code>h</code> should broadcast with <code>x[...,None]</code></p> required <p>Returns:     cdf: Tensor[...] (shape of <code>x</code> broadcasted with <code>h[...,0]</code>)</p> Source code in <code>src/notochord/distributions.py</code> <pre><code>def cdf(self, h, x):\n    \"\"\"\n    Args:\n        h: Tensor[...,n_params]\n        x: Tensor[...]\n            `h` should broadcast with `x[...,None]`\n    Returns:\n        cdf: Tensor[...] (shape of `x` broadcasted with `h[...,0]`)\n    \"\"\"\n    if not isinstance(x, torch.Tensor):\n        x = torch.tensor(x)\n    log_pi, loc, s = self.get_params(h)  \n    cdfs = self.cdf_components(loc, s, x)\n    cdf = (cdfs * log_pi.softmax(-1)).sum(-1)\n    return cdf\n</code></pre>"},{"location":"reference/notochord/distributions/#notochord.distributions.CensoredMixtureLogistic.forward","title":"<code>forward(h, x)</code>","text":"<p>log prob of x under distribution parameterized by h Args:     h: Tensor[...,n_params]     x: Tensor[...]     \"...\" dims must broadcast</p> Source code in <code>src/notochord/distributions.py</code> <pre><code>def forward(self, h, x):\n    \"\"\"log prob of x under distribution parameterized by h\n    Args:\n        h: Tensor[...,n_params]\n        x: Tensor[...]\n        \"...\" dims must broadcast\n    \"\"\"\n    log_pi, loc, s = self.get_params(h)    \n\n    d = self.res/2\n    x = x.clamp(self.lo, self.hi)[...,None]\n    x_ = (x - loc) * s\n    sd = s*d\n\n    # # censoring\n    lo_cens = x &lt;= self.lo+d\n    hi_cens = x &gt;= self.hi-d\n    ones = torch.ones_like(s)\n    zeros = torch.zeros_like(s)\n\n    diff_term = torch.where(lo_cens | hi_cens, \n        ones, sd.exp() - (-sd).exp()\n        ).log()\n    minus_sp_term = torch.where(hi_cens, -sd, F.softplus(-sd-x_))\n    plus_sp_term = torch.where(lo_cens, zeros, x_ + F.softplus(sd-x_))\n\n    log_delta_cdf = diff_term - minus_sp_term - plus_sp_term\n\n    # log prob\n    r = {\n        'log_prob': (log_pi + log_delta_cdf).logsumexp(-1)\n    }\n    # diagnostics\n    with torch.no_grad():\n        ent = D.Categorical(logits=log_pi).entropy()\n        r |= {\n            # 'min_sharpness': s.min(),\n            'max_sharpness': s.max(),\n            'mean_sharpness': (s*log_pi.exp()).sum(-1).mean(),\n            # 'min_entropy': ent.min(),\n            # 'max_entropy': ent.max(),\n            'mean_cmp_entropy': ent.mean(),\n            'marginal_cmp_entropy': D.Categorical(\n                log_pi.exp().mean(list(range(log_pi.ndim-1)))).entropy(),\n            # 'min_loc': loc.min(),\n            # 'max_loc': loc.max()\n        }\n    return r\n</code></pre>"},{"location":"reference/notochord/distributions/#notochord.distributions.CensoredMixtureLogistic.sample","title":"<code>sample(h, truncate=None, shape=None, weight_top_p=None, component_temp=None, bias=None, truncate_quantile=None, quantile_k=128, eps=1e-09)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>h</code> <p>Tensor[..., n_params]</p> required <code>truncate</code> <p>Optional[Tuple[float, float]]. lower and upper bound for truncation.</p> <code>None</code> <code>shape</code> <p>Optional[int]. additional sample shape to be prepended to dims.</p> <code>None</code> <code>weight_top_p</code> <p>top_p (\"nucleus\") filtering for mixture weights. default is 1 (no change to distribution). 0 would sample top component (after truncation) only.</p> <code>None</code> <code>component_temp</code> <p>Optional[float]. sampling temperature of each mixture  component. default is 1. 0 would sample component location only, ignoring sharpness.</p> <code>None</code> <code>bias</code> <p>applied outside of truncation but inside of clamping, useful e.g. for latency correction when sampling delta-time</p> <code>None</code> <code>truncate_quantile</code> <p>truncate the distribution again by quantile, after the effects of truncate, weight_top_p and component_temp.</p> <code>None</code> <code>quantile_k</code> <p>truncate_quantile is implemented by drawing this many samples and sorting them  </p> <code>128</code> <p>Returns:     Tensor[*shape,...] (h without last dimension, prepended with <code>shape</code>)</p> Source code in <code>src/notochord/distributions.py</code> <pre><code>def sample(self, h, truncate=None, shape=None, \n    weight_top_p=None, component_temp=None, bias=None, \n    truncate_quantile=None, quantile_k=128, eps=1e-9\n    ):\n    \"\"\"\n    Args:\n        h: Tensor[..., n_params]\n        truncate: Optional[Tuple[float, float]]. lower and upper bound for truncation.\n        shape: Optional[int]. additional sample shape to be prepended to dims.\n        weight_top_p: top_p (\"nucleus\") filtering for mixture weights.\n            default is 1 (no change to distribution). 0 would sample top\n            component (after truncation) only.\n        component_temp: Optional[float]. sampling temperature of each mixture \n            component. default is 1. 0 would sample component location only,\n            ignoring sharpness.\n        bias: applied outside of truncation but inside of clamping,\n            useful e.g. for latency correction when sampling delta-time\n        truncate_quantile: truncate the distribution again by quantile,\n            after the effects of truncate, weight_top_p and component_temp.\n        quantile_k: truncate_quantile is implemented by drawing this many\n            samples and sorting them  \n    Returns:\n        Tensor[*shape,...] (h without last dimension, prepended with `shape`)\n    \"\"\"\n    if truncate_quantile == (0,1):\n        truncate_quantile = None\n\n    if truncate is None:\n        truncate = (-torch.inf, torch.inf)\n    # early out in the single possibility case\n    if truncate[0] == truncate[1]:\n        return torch.tensor(\n            [truncate[0]]*shape if shape is not None else truncate[0])\n    truncate = torch.tensor(truncate)\n\n    if shape is None:\n        unwrap = True\n        shape = 1\n    else:\n        unwrap = False\n\n    if truncate_quantile is not None:\n        # draw k samples\n        shape = shape * quantile_k\n\n    if component_temp is None:\n        component_temp = 1\n\n    if bias is None:\n        bias = 0\n\n    log_pi, loc, s = self.get_params(h)\n    s = s/component_temp\n    scale = 1/s\n\n    # cdfs: [...,bound,component]\n    cdfs = self.cdf_components(loc[...,None,:], s[...,None,:], truncate) \n    # prob. mass of each component witin bounds\n    trunc_probs = cdfs[...,1,:] - cdfs[...,0,:] # [...,component]\n    probs = log_pi.exp() * trunc_probs # reweighted mixture component probs\n    if weight_top_p is not None:\n        # reweight with top_p\n        probs = reweight_top_p(probs+eps, weight_top_p)\n\n    probs = probs.clamp(eps, 1)\n    c = D.Categorical(probs).sample((shape,))\n    # move sample dimension first\n    loc = loc.movedim(-1, 0).gather(0, c)\n    scale = scale.movedim(-1, 0).gather(0, c)\n    upper = cdfs[...,1,:].movedim(-1, 0).gather(0, c)\n    lower = cdfs[...,0,:].movedim(-1, 0).gather(0, c)\n\n    u = torch.rand(shape, *h.shape[:-1])\n    # truncate\n    u = u * (upper-lower) + lower\n\n    u = u.clamp(eps, 1-eps)\n\n    x = loc + bias - scale * (1/u - 1).log()\n\n    if truncate_quantile is not None:\n        x = x.sort(dim=0).values\n\n        # print(truncate_quantile, list(x_.item() for x_ in x))\n\n        idx = categorical_sample(\n            x.new_zeros(x.shape[0]), \n            truncate_quantile=truncate_quantile)\n\n        x = x[idx]\n\n    x = x.clamp(self.lo, self.hi)\n    x = x.clamp(*truncate)\n    return x[0] if unwrap else x\n</code></pre>"},{"location":"reference/notochord/distributions/#notochord.distributions.CensoredMixturePointyBoi","title":"<code>CensoredMixturePointyBoi</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>src/notochord/distributions.py</code> <pre><code>class CensoredMixturePointyBoi(nn.Module):\n    def __init__(self, n, res=1e-2, lo='-inf', hi='inf', sharp_bounds=(1e-5,2e3)):\n        super().__init__()\n        self.n = n\n        self.res = res\n        self.sharp_bounds = sharp_bounds\n        # self.register_buffer('max_sharp', torch.tensor(float(max_sharp)))\n        self.register_buffer('lo', torch.tensor(float(lo)))\n        self.register_buffer('hi', torch.tensor(float(hi)))\n        # TODO: init is not general-purpose\n        self.bias = nn.Parameter(torch.cat((\n            torch.zeros(n), torch.logspace(-3,1,n), torch.zeros(n)\n            )))\n\n    @property\n    def n_params(self):\n        return self.n*3\n\n    def get_params(self, h):\n        assert h.shape[-1] == self.n_params\n        h = h+self.bias\n        # get parameters fron unconstrained hidden state:\n        logit_pi, loc, log_s = torch.chunk(h, 3, -1)\n        # mixture coefficients\n        log_pi = logit_pi - logit_pi.logsumexp(-1,keepdim=True)\n        # location\n        loc = loc.clamp(self.lo-10*self.res, self.hi+10*self.res)\n        # sharpness\n        # s = log_s.exp()\n        # s = torch.min(F.softplus(log_s), self.max_sharp)\n        s = F.softplus(log_s).clamp(*self.sharp_bounds)\n        # s = log_s.exp().clamp(*self.sharp_bounds)\n        return log_pi, loc, s\n\n    def forward(self, h, x):\n        \"\"\"log prob of x under distribution parameterized by h\"\"\"\n        log_pi, loc, s = self.get_params(h)    \n\n        x = x.clamp(self.lo, self.hi)[...,None]\n        xp, xm = x+self.res/2, x-self.res/2\n\n        # numerical crimes follow\n\n        # censoring\n        lo_cens = x &lt;= self.lo\n        xm_ = torch.where(lo_cens, -h.new_ones([]), (xm-loc)*s)\n        axm_ = torch.where(lo_cens, h.new_zeros([]), xm_.abs())\n        hi_cens = x &gt;= self.hi\n        xp_ = torch.where(hi_cens, h.new_ones([]), (xp-loc)*s)\n        axp_ = torch.where(hi_cens, h.new_zeros([]), xp_.abs())\n\n        log_delta_cdf = (\n            (xp_ - xm_ + xp_*axm_ - axp_*xm_).log() \n            - (axp_ + axm_ + axp_*axm_).log1p() \n            - math.log(2))\n\n        # log prob\n        r = {\n            'log_prob': (log_pi + log_delta_cdf).logsumexp(-1)\n        }\n        # diagnostics\n        with torch.no_grad():\n            ent = D.Categorical(logits=log_pi).entropy()\n            r |= {\n                'min_sharpness': s.min(),\n                'max_sharpness': s.max(),\n                'min_entropy': ent.min(),\n                'max_entropy': ent.max(),\n                'marginal_entropy': D.Categorical(\n                    log_pi.exp().mean(list(range(log_pi.ndim-1)))).entropy(),\n                'min_loc': loc.min(),\n                'max_loc': loc.max()\n            }\n        return r\n\n    def cdf(self, h, x):\n        log_pi, loc, s = self.get_params(h)  \n        x_ = (x[...,None] - loc) * s \n        cdfs = x_ / (1+x_.abs()) * 0.5 + 0.5\n        cdf = (cdfs * log_pi.softmax(-1)).sum(-1)\n        return cdf\n\n\n    def sample(self, h, shape=1):\n        \"\"\"\n        Args:\n            shape: additional sample shape to be prepended to dims\n        \"\"\"\n        # if shape is None: shape = []\n\n        log_pi, loc, s = self.get_params(h)\n        c = D.Categorical(logits=log_pi).sample((shape,))\n        # move sample dimension first\n        loc = loc.movedim(-1, 0).gather(0, c)\n        s = s.movedim(-1, 0).gather(0, c)\n\n        u = torch.rand(shape, *h.shape[:-1])*2-1\n        x_ = u / (1 - u.abs())\n        x = x_ / s + loc\n\n        return x.clamp(self.lo, self.hi)\n</code></pre>"},{"location":"reference/notochord/distributions/#notochord.distributions.CensoredMixturePointyBoi.forward","title":"<code>forward(h, x)</code>","text":"<p>log prob of x under distribution parameterized by h</p> Source code in <code>src/notochord/distributions.py</code> <pre><code>def forward(self, h, x):\n    \"\"\"log prob of x under distribution parameterized by h\"\"\"\n    log_pi, loc, s = self.get_params(h)    \n\n    x = x.clamp(self.lo, self.hi)[...,None]\n    xp, xm = x+self.res/2, x-self.res/2\n\n    # numerical crimes follow\n\n    # censoring\n    lo_cens = x &lt;= self.lo\n    xm_ = torch.where(lo_cens, -h.new_ones([]), (xm-loc)*s)\n    axm_ = torch.where(lo_cens, h.new_zeros([]), xm_.abs())\n    hi_cens = x &gt;= self.hi\n    xp_ = torch.where(hi_cens, h.new_ones([]), (xp-loc)*s)\n    axp_ = torch.where(hi_cens, h.new_zeros([]), xp_.abs())\n\n    log_delta_cdf = (\n        (xp_ - xm_ + xp_*axm_ - axp_*xm_).log() \n        - (axp_ + axm_ + axp_*axm_).log1p() \n        - math.log(2))\n\n    # log prob\n    r = {\n        'log_prob': (log_pi + log_delta_cdf).logsumexp(-1)\n    }\n    # diagnostics\n    with torch.no_grad():\n        ent = D.Categorical(logits=log_pi).entropy()\n        r |= {\n            'min_sharpness': s.min(),\n            'max_sharpness': s.max(),\n            'min_entropy': ent.min(),\n            'max_entropy': ent.max(),\n            'marginal_entropy': D.Categorical(\n                log_pi.exp().mean(list(range(log_pi.ndim-1)))).entropy(),\n            'min_loc': loc.min(),\n            'max_loc': loc.max()\n        }\n    return r\n</code></pre>"},{"location":"reference/notochord/distributions/#notochord.distributions.CensoredMixturePointyBoi.sample","title":"<code>sample(h, shape=1)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>shape</code> <p>additional sample shape to be prepended to dims</p> <code>1</code> Source code in <code>src/notochord/distributions.py</code> <pre><code>def sample(self, h, shape=1):\n    \"\"\"\n    Args:\n        shape: additional sample shape to be prepended to dims\n    \"\"\"\n    # if shape is None: shape = []\n\n    log_pi, loc, s = self.get_params(h)\n    c = D.Categorical(logits=log_pi).sample((shape,))\n    # move sample dimension first\n    loc = loc.movedim(-1, 0).gather(0, c)\n    s = s.movedim(-1, 0).gather(0, c)\n\n    u = torch.rand(shape, *h.shape[:-1])*2-1\n    x_ = u / (1 - u.abs())\n    x = x_ / s + loc\n\n    return x.clamp(self.lo, self.hi)\n</code></pre>"},{"location":"reference/notochord/distributions/#notochord.distributions.categorical_sample","title":"<code>categorical_sample(logits, whitelist=None, index=None, top_p=None, truncate_quantile=None)</code>","text":"<p>if whitelist is a dictionary, it maps to weights</p> Source code in <code>src/notochord/distributions.py</code> <pre><code>def categorical_sample(\n        logits, whitelist=None, index=None, top_p=None, \n        truncate_quantile=None\n        ):\n    \"\"\"if whitelist is a dictionary, it maps to weights\"\"\"\n    # if logits.isnan().any():\n    #     raise Exception('start '+str(logits))\n\n    if whitelist is not None:\n        idx = list(whitelist)\n        preserve_logits = logits[...,idx]\n        if isinstance(whitelist, dict):\n            preserve_logits += torch.tensor(list(whitelist.values())).log()\n        logits = torch.full_like(logits, -torch.inf)\n        logits[..., idx] = preserve_logits\n\n    if index is not None:\n        return logits.argsort(-1, True)[..., index]\n\n    probs = logits.softmax(-1)\n\n    # if probs.isnan().any():\n        # raise Exception('whitelist '+str(probs))\n\n    if top_p is not None:\n        probs = reweight_top_p(probs, top_p)\n\n    # if probs.isnan().any():\n        # raise Exception('top p '+str(probs))\n\n    if truncate_quantile is not None:\n        q_lo, q_hi = truncate_quantile\n        q_lo = max(0., 0. if q_lo is None else q_lo)\n        q_hi = min(1., 1. if q_hi is None else q_hi)\n        # print(q_lo, q_hi)\n        zero = torch.zeros_like(probs)\n        zcs = (1-probs.flip(-1).cumsum(-1).flip(-1))\n        cs = probs.cumsum(-1)\n        # truncate up to q_lo\n        probs -= (q_lo-zcs).clip(zero, probs)\n        # truncate from q_hi\n        probs -= (cs-q_hi).clip(zero, probs)\n\n    # if probs.isnan().any():\n        # raise Exception('trunc quant '+str(probs))\n\n    probs.nan_to_num_(1e-5)\n\n    return D.Categorical(probs).sample()\n</code></pre>"},{"location":"reference/notochord/distributions/#notochord.distributions.reweight_quantile","title":"<code>reweight_quantile(probs, min_q=0, max_q=1)</code>","text":"<p>reweight ordinal discrete distribution to have mass only between quantiles</p> Source code in <code>src/notochord/distributions.py</code> <pre><code>def reweight_quantile(probs, min_q=0, max_q=1):\n    \"\"\"\n    reweight ordinal discrete distribution to have mass only between quantiles\n    \"\"\"\n    # TODO\n    cdf = probs.cumsum(-1)\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/notochord/distributions/#notochord.distributions.reweight_top_p","title":"<code>reweight_top_p(probs, top_p)</code>","text":"<p>given tensor of probabilities, apply top p / \"nucleus\" filtering, or temperature if <code>top_p</code> is greater than 1</p> Source code in <code>src/notochord/distributions.py</code> <pre><code>def reweight_top_p(probs, top_p):\n    \"\"\"\n    given tensor of probabilities, apply top p / \"nucleus\" filtering,\n    or temperature if `top_p` is greater than 1\n    \"\"\"\n    if top_p &gt; 1:\n        probs = probs**(1/top_p)\n        return probs / probs.sum(-1)\n\n    # NOTE: this is fudged slightly, it doesn't 'interpolate' the cutoff bin\n    desc_probs, idx = probs.sort(-1, descending=True)\n    iidx = idx.argsort(-1)\n    cumprob = desc_probs.cumsum(-1)\n    # first index where cumprob &gt;= top_p is the last index we don't zero\n    to_zero = (cumprob &gt;= top_p).roll(1, -1)\n    to_zero[...,0] = False\n    # unsort\n    to_zero = to_zero.gather(-1, iidx)\n    weighted_probs = torch.zeros_like(probs).where(to_zero, probs)\n    return weighted_probs / weighted_probs.sum(-1, keepdim=True)\n</code></pre>"},{"location":"reference/notochord/model/","title":"Model","text":""},{"location":"reference/notochord/model/#notochord.model.InstrumentData","title":"<code>InstrumentData</code>","text":"<p>channels: set of MIDI channels this instrument appeared on (0-indexed) pitches: set of MIDI pitches this instrument played velocities: set of MIDI velocities this instrument played orig_inst: the Notochord instrument corresponding to the MIDI program this     instrument was derived from -- i.e. if this is an anonymous instrument     because it collided with the same instrument on another channel,     this annotates that original instrument notes: count of noteOn events after processing shortened: number of notes shorted to avoid pitch collisions dropped: number of notes dropped to avoid pitch collisions</p> Source code in <code>src/notochord/model.py</code> <pre><code>class InstrumentData:\n    \"\"\"\n    channels: set of MIDI channels this instrument appeared on (0-indexed)\n    pitches: set of MIDI pitches this instrument played\n    velocities: set of MIDI velocities this instrument played\n    orig_inst: the Notochord instrument corresponding to the MIDI program this\n        instrument was derived from -- i.e. if this is an anonymous instrument\n        because it collided with the same instrument on another channel,\n        this annotates that original instrument\n    notes: count of noteOn events after processing\n    shortened: number of notes shorted to avoid pitch collisions\n    dropped: number of notes dropped to avoid pitch collisions\n    \"\"\"\n    def __init__(self):\n        self.channels = set()\n        self.pitches = set()\n        self.velocities = set()\n        self.orig_inst = None\n        self.notes = 0\n        self.shortened = 0\n        self.dropped = 0\n    def __repr__(self):\n        pr = f' ({min(self.pitches)}-{max(self.pitches)})' if self.notes else ''\n        return f'(notes={self.notes} shortened={self.shortened} dropped={self.dropped} orig_inst={self.orig_inst} channels={self.channels} pitches={len(self.pitches)}{pr})'\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.MixEmbedding","title":"<code>MixEmbedding</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>src/notochord/model.py</code> <pre><code>class MixEmbedding(nn.Module):\n    def __init__(self, n, domain=(0,1)):\n        \"\"\"\n        Args:\n            n (int): number of channels\n            domain (Tuple[float])\n        \"\"\"\n        super().__init__()\n        self.domain = domain\n        self.lo = nn.Parameter(torch.randn(n))\n        self.hi = nn.Parameter(torch.randn(n))\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Tensor[...]\n        Returns:\n            Tensor[...,n]\n        \"\"\"\n        x = (x - self.domain[0])/(self.domain[1] - self.domain[0])\n        x = x[...,None]\n        return self.hi * x + self.lo * (1-x)\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.MixEmbedding.__init__","title":"<code>__init__(n, domain=(0, 1))</code>","text":"<p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>number of channels</p> required Source code in <code>src/notochord/model.py</code> <pre><code>def __init__(self, n, domain=(0,1)):\n    \"\"\"\n    Args:\n        n (int): number of channels\n        domain (Tuple[float])\n    \"\"\"\n    super().__init__()\n    self.domain = domain\n    self.lo = nn.Parameter(torch.randn(n))\n    self.hi = nn.Parameter(torch.randn(n))\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.MixEmbedding.forward","title":"<code>forward(x)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <p>Tensor[...]</p> required <p>Returns:     Tensor[...,n]</p> Source code in <code>src/notochord/model.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Args:\n        x: Tensor[...]\n    Returns:\n        Tensor[...,n]\n    \"\"\"\n    x = (x - self.domain[0])/(self.domain[1] - self.domain[0])\n    x = x[...,None]\n    return self.hi * x + self.lo * (1-x)\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.Notochord","title":"<code>Notochord</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>src/notochord/model.py</code> <pre><code>class Notochord(nn.Module):\n    # note: use named arguments only for benefit of training script\n    def __init__(self, \n            emb_size=256, \n            rnn_hidden=2048, rnn_layers=1, kind='gru', \n            mlp_layers=0,\n            dropout=0.1, norm=None,\n            num_pitches=128, \n            num_instruments=320,\n            time_sines=128, vel_sines=128,\n            time_bounds=(0,10), time_components=32, time_res=1e-2,\n            vel_components=16\n            ):\n        \"\"\"\n        \"\"\"\n        super().__init__()\n\n        self.step = 0\n        self.current_time = 0\n        self.held_notes = {}\n\n        self.note_dim = 4 # instrument, pitch, time, velocity\n\n        self.instrument_start_token = 0\n        self.instrument_domain = num_instruments+1\n\n        self.pitch_start_token = num_pitches\n        self.pitch_domain = num_pitches+1\n\n        self.max_dt = time_bounds[1]\n        self.time_dist = CensoredMixtureLogistic(\n            time_components, time_res, \n            sharp_bounds=(1e-4,2e3),\n            lo=time_bounds[0], hi=time_bounds[1], init='time')\n        self.vel_dist = CensoredMixtureLogistic(\n            vel_components, 1.0,\n            sharp_bounds=(1e-3,128),\n            lo=0, hi=127, init='velocity')\n\n        # embeddings for inputs\n        self.instrument_emb = nn.Embedding(self.instrument_domain, emb_size)\n        self.pitch_emb = nn.Embedding(self.pitch_domain, emb_size)\n        self.time_emb = (#torch.jit.script(\n            SineEmbedding(\n            time_sines, emb_size, 1e-3, 30, scale='log'))\n        # self.vel_emb = MixEmbedding(emb_size, (0, 127))\n        self.vel_emb = (#torch.jit.script(\n            SineEmbedding(\n            vel_sines, emb_size, 2, 512, scale='lin'))\n\n        # RNN backbone\n        self.rnn = GenericRNN(kind, \n            emb_size, rnn_hidden, \n            num_layers=rnn_layers, batch_first=True, dropout=dropout)\n\n        # learnable initial RNN state\n        self.initial_state = nn.ParameterList([\n             # layer x batch x hidden\n            nn.Parameter(torch.randn(rnn_layers,1,rnn_hidden)*rnn_hidden**-0.5)\n            for _ in range(2 if kind=='lstm' else 1)\n        ])\n\n        mlp_cls = GLUMLP#lambda *a: torch.jit.script(GLUMLP(*a))\n        # projection from RNN state to distribution parameters\n        self.h_proj = mlp_cls(\n                rnn_hidden, emb_size, emb_size, \n                mlp_layers, dropout, norm)\n        self.projections = nn.ModuleList([\n            mlp_cls(\n                emb_size, emb_size, self.instrument_domain, \n                mlp_layers, dropout, norm),\n            mlp_cls(\n                emb_size, emb_size, self.pitch_domain, \n                mlp_layers, dropout, norm),\n            mlp_cls(\n                emb_size, emb_size, self.time_dist.n_params,\n                mlp_layers, dropout, norm),\n            mlp_cls(\n                emb_size, emb_size, self.vel_dist.n_params, \n                mlp_layers, dropout, norm),\n        ])\n\n        self.end_proj = nn.Linear(rnn_hidden, 2)\n\n        with torch.no_grad():\n            for p in self.projections:\n                p.net[-1].weight.mul_(1e-2)\n            self.end_proj.weight.mul(1e-2)\n\n        # persistent RNN state for inference\n        for n,t in zip(self.cell_state_names(), self.initial_state):\n            self.register_buffer(n, t.clone())\n        self.step = 0\n\n        # volatile hidden states for caching purposes\n        self.h = None\n        self.h_query = None\n\n        self._default_note_map = {\n            i:range(128) for i in range(1,self.instrument_domain+1)}      \n\n    def cell_state_names(self):\n        return tuple(f'cell_state_{i}' for i in range(len(self.initial_state)))\n\n    def held_map(self):\n        \"\"\"\n            currently held notes as a map from instrument to pitch set\n        \"\"\"\n        held_map = {}\n        for i,p in self.held_notes:\n            if i not in held_map:\n                held_map[i] = set()\n            held_map[i].add(p)\n        return held_map\n\n    def get_note_maps(self, \n        note_on_map=None, note_off_map=None, \n        min_polyphony=None, max_polyphony=None):\n        \"\"\"common logic for v-first sampling\"\"\"\n        # convert {(i,p):t} to {i:[p]}\n        held_map = self.held_map()\n\n        # get default note_on_map (anything)\n        if note_on_map is None:\n            note_on_map = copy.copy(self._default_note_map)\n        else:\n            note_on_map = copy.deepcopy(note_on_map)\n\n        # note offs can be any from the note_on instruments by default\n        # but users can also supply this themselves\n        if note_off_map is None:\n            note_off_map = {\n                i: held_map[i] \n                for i in note_on_map\n                if i in held_map}\n        else:\n            note_on_map = copy.deepcopy(note_off_map)\n\n        # exclude held notes for note on\n        for i in held_map:\n            if i in note_on_map:\n                note_on_map[i] = set(note_on_map[i]) - held_map[i]\n\n        # exclude non-held notes for note off\n        note_off_map = {\n            i: set(note_off_map[i]) &amp; held_map[i]\n            for i in note_off_map\n            if i in held_map\n        }\n\n        # TODO: \n        # allow breaking polyphony constraint when it results in no options?\n        # may not work to check here as more constraints applied downstream\n        # could track number/degree of constraint violations instead of simply\n        # removing pitches -- later sample from the top stratum only\n\n        max_poly = get_from_scalar_or_dict(max_polyphony, torch.inf)\n        min_poly = get_from_scalar_or_dict(min_polyphony, 0)\n\n        # prevent note on if polyphony exceeded\n        for i in list(note_on_map):\n            if len(held_map.get(i, [])) &gt;= max_poly(i):\n                note_on_map.pop(i)\n\n        # prevent note off if below minimum polyphony\n        for i in list(note_off_map):\n            if len(held_map[i]) &lt;= min_poly(i):\n                note_off_map.pop(i)\n\n        return note_on_map, note_off_map\n\n    @property\n    def cell_state(self):\n        return tuple(getattr(self, n) for n in self.cell_state_names())\n\n    @property\n    def embeddings(self):\n        return (\n            self.instrument_emb,\n            self.pitch_emb,\n            self.time_emb,\n            self.vel_emb\n        )\n\n    def forward(self, instruments, pitches, times, velocities, ends,\n            validation=False, ar_mask=None):\n        \"\"\"\n        teacher-forced probabilistic loss and diagnostics for training.\n\n        Args:\n            instruments: LongTensor[batch, time]\n            pitches: LongTensor[batch, time]\n            times: FloatTensor[batch, time]\n            velocities: FloatTensor[batch, time]\n            ends: LongTensor[batch, time]\n            validation: bool (computes some extra diagnostics)\n            ar_mask: Optional[Tensor[note_dim x note_dim]] if None, generate random\n                masks for training\n        \"\"\"\n        batch_size, batch_len = pitches.shape\n\n        self.checkpoint_path = None\n\n        # embed data to input vectors\n        inst_emb = self.instrument_emb(instruments) # batch, time, emb_size\n        pitch_emb = self.pitch_emb(pitches) # batch, time, emb_size\n        time_emb = self.time_emb(times) # batch, time, emb_size\n        vel_emb = self.vel_emb(velocities) # batch, time, emb_size\n\n        embs = (inst_emb, pitch_emb, time_emb, vel_emb)\n\n        # feed to RNN backbone\n        x = sum(embs)\n        ## broadcast initial state to batch size\n        initial_state = tuple(\n            t.expand(self.rnn.num_layers, x.shape[0], -1).contiguous() # 1 x batch x hidden\n            for t in self.initial_state)\n        h, _ = self.rnn(x, initial_state) #batch, time, hidden_size\n\n        # fit all event factorizations \n        # e.g. inst-&gt;pitch-&gt;time-&gt;vel vs vel-&gt;time-&gt;inst-&gt;pitch\n        trim_h = h[:,:-1]\n        # always include hidden state, never include same modality,\n        # other dependencies are random per time and position\n        n = self.note_dim\n        if ar_mask is None:\n            # random binary mask\n            ar_mask = torch.randint(2, (*trim_h.shape[:2],n,n), dtype=torch.bool, device=h.device)\n            # zero diagonal\n            ar_mask &amp;= ~torch.eye(n,n, dtype=torch.bool, device=h.device)\n        # include hidden state\n        ar_mask = torch.cat((ar_mask.new_ones(*ar_mask.shape[:-2],1,n), ar_mask), -2).float()\n\n        to_mask = torch.stack((\n            self.h_proj(trim_h),\n            *(emb[:,1:] for emb in embs)\n        ), -1)\n        # TODO: try without this tanh?\n        mode_hs = (to_mask @ ar_mask).tanh().unbind(-1)\n\n        # final projections to raw distribution parameters\n        inst_params, pitch_params, time_params, vel_params = [\n            proj(h) for proj,h in zip(self.projections, mode_hs)]\n\n        # get likelihood of data for each modality\n        inst_logits = F.log_softmax(inst_params, -1)\n        inst_targets = instruments[:,1:,None] #batch, time, 1\n        inst_log_probs = inst_logits.gather(-1, inst_targets)[...,0]\n\n        pitch_logits = F.log_softmax(pitch_params, -1)\n        pitch_targets = pitches[:,1:,None] #batch, time, 1\n        pitch_log_probs = pitch_logits.gather(-1, pitch_targets)[...,0]\n\n        time_targets = times[:,1:] # batch, time\n        time_result = self.time_dist(time_params, time_targets)\n        time_log_probs = time_result.pop('log_prob')\n\n        vel_targets = velocities[:,1:] # batch, time\n        vel_result = self.vel_dist(vel_params, vel_targets)\n        vel_log_probs = vel_result.pop('log_prob')\n\n        # end prediction\n        # skip the first position for convenience \n        # (so masking is the same for end as for note parts)\n        end_params = self.end_proj(h[:,1:])\n        end_logits = F.log_softmax(end_params, -1)\n        end_log_probs = end_logits.gather(-1, ends[:,1:,None])[...,0]\n\n        r = {\n            'end_log_probs': end_log_probs,\n            'instrument_log_probs': inst_log_probs,\n            'pitch_log_probs': pitch_log_probs,\n            'time_log_probs': time_log_probs,\n            'velocity_log_probs': vel_log_probs,\n            **{'time_'+k:v for k,v in time_result.items()},\n            **{'velocity_'+k:v for k,v in vel_result.items()}\n        }\n        # this just computes some extra diagnostics which are inconvenient to do in the\n        # training script. should be turned off during training for performance.\n        if validation:\n            with torch.no_grad():\n                r['time_acc_30ms'] = (\n                    self.time_dist.cdf(time_params, time_targets + 0.03)\n                    - torch.where(time_targets - 0.03 &gt;= 0,\n                        self.time_dist.cdf(time_params, time_targets - 0.03),\n                        time_targets.new_zeros([]))\n                )\n        return r\n\n    # TODO: add a constructor argument to specify which are drums\n    # hardcoded for now\n    # 0 - start token\n    # 1-128 - melodic\n    # 129-256 - drums\n    # 257-288 - anon melodic\n    # 289-320 - anon drums\n    def is_drum(self, inst):\n        return inst &gt; 128 and inst &lt; 257 or inst &gt; 288\n    def is_anon(self, inst):\n        return inst &gt; 256\n    def first_anon_like(self, inst):\n        # TODO: add a constructor argument to specify how many anon\n        # hardcoded for now\n        return 289 if self.is_drum(inst) else 257\n    def anon_like(self, i):\n        n_anon = (self.instrument_domain - 257)//2\n        i = self.first_anon_like(i)\n        return range(i, i+n_anon)\n\n    def feed(self, inst:int, pitch:int, time:Number, vel:Number, **kw):\n        \"\"\"consume an event and advance hidden state\n\n        Args:\n            inst: int. instrument of current note.\n                0 is start token\n                1-128 are General MIDI instruments\n                129-256 are drumkits (MIDI 1-128 on channel 13)\n                257-288 are 'anonymous' melodic instruments\n                289-320 are 'anonymous' drumkits\n            pitch: int. MIDI pitch of current note.\n                0-127 are MIDI pitches / drums\n                128 is start token\n            time: float. elapsed time in seconds since previous event.\n            vel: float. (possibly dequantized) MIDI velocity from 0-127 inclusive.\n                0 indicates a note-off event\n            **kw: ignored (allows doing e.g. noto.feed(**noto.query(...)))\n        \"\"\"\n        # print(f'FEED from {threading.get_ident()}') \n        # print('feed', inst, pitch, time, vel)\n\n        # track elapsed time and ongoing notes\n        key = (inst,pitch)\n        for k in self.held_notes:\n            self.held_notes[k] += time\n        self.current_time += time\n        self.step += 1\n\n        if vel &gt; 0:\n            self.held_notes[key] = 0\n        elif key in self.held_notes:\n            self.held_notes.pop(key)\n\n        # print(self.held_notes)\n\n        # update RNN state\n\n        with torch.inference_mode():\n            inst = torch.LongTensor([[inst]]) # 1x1 (batch, time)\n            pitch = torch.LongTensor([[pitch]]) # 1x1 (batch, time)\n            time = torch.FloatTensor([[time]]) # 1x1 (batch, time)\n            vel = torch.FloatTensor([[vel]]) # 1x1 (batch, time)\n\n            embs = [\n                self.instrument_emb(inst), # 1, 1, emb_size\n                self.pitch_emb(pitch), # 1, 1, emb_size\n                self.time_emb(time),# 1, 1, emb_size\n                self.vel_emb(vel)# 1, 1, emb_size\n            ]\n            x = sum(embs)\n\n            self.h, new_state = self.rnn(x, self.cell_state)\n            for t,new_t in zip(self.cell_state, new_state):\n                t[:] = new_t\n\n            self.h_query = None\n\n    def deep_query(self, query, predict_end=True):\n        \"\"\"flexible querying with nested Query objects.\n        see query_vtip for an example.\n\n        Args:\n            query: Query object\n        \"\"\"\n        with torch.inference_mode():\n            if self.h_query is None:\n                self.h_query = self.h_proj(self.h)\n            event = self._deep_query(\n                query, hidden=self.h_query[:,0], event={})\n\n            if predict_end:\n                # print('END')\n                # print(f'{self.h}')\n                end_params = self.end_proj(self.h)\n                event['end'] = end_params.softmax(-1)[...,1].item()\n                # event['end'] = D.Categorical(logits=end_params).sample().item()\n            else:\n                event['end'] = 0#torch.zeros(self.h.shape[:-1])\n\n        return event\n\n    def _deep_query(self, query, hidden, event):\n        if hasattr(query, '__call__'):\n            query = query(event)\n        m = query.modality\n        try:\n            idx = ('inst','pitch','time','vel').index(m)\n        except ValueError:\n            raise ValueError(f'unknown modality \"{m}\"')\n\n        project = self.projections[idx]\n        embed = self.embeddings[idx]\n\n        if query.value is not None:\n            result = torch.tensor(query.value)\n        else:\n            if m=='time':\n                dist = self.time_dist\n                sample = dist.sample\n            elif m=='vel':\n                dist = self.vel_dist\n                sample = dist.sample\n            else:\n                sample = categorical_sample\n\n            params = project(hidden.tanh())\n\n            if query.cases is None:\n                result = sample(params, **query.kw)\n                # print(f'{result=}, {query.kw=}, {event=}') ##DEBUG\n\n            elif m in ('inst', 'pitch'):\n                # weighted subsets case\n                assert all(isinstance(s, Subset) for s in query.cases), query.cases\n                if len(query.cases) &gt; 1:\n                    all_probs = params.softmax(-1)\n                    probs = [\n                        all_probs[...,s.values].sum(-1) * s.weight\n                        for s in query.cases]\n                    idx = categorical_sample(torch.tensor(probs).log())\n                else:\n                    idx = 0\n                # sample from subset\n                # TODO: handle case where user supplies cases and whitelist\n                s = query.cases[idx]\n                result = sample(\n                    params, whitelist=s.sample_values, **query.kw, **s.kw)\n            else:\n                # weiighted ranges case\n                assert all(isinstance(r, Range) for r in query.cases), query.cases\n                if len(query.cases) &gt; 1:\n                    probs = [\n                        (dist.cdf(params, r.hi) - dist.cdf(params, r.lo)\n                        ) * r.weight\n                        for r in query.cases]\n                    # print(f'deep_query {m} {probs=}')\n                    idx = categorical_sample(torch.tensor(probs).log())\n                else:\n                    idx = 0\n                r = query.cases[idx]\n                # sample from range\n                # TODO: handle case where user supplies cases and truncate\n                result = sample(\n                    params, truncate=(r.sample_lo, r.sample_hi), **query.kw, **r.kw)\n\n\n        if not result.isfinite().all():\n            print('WARNING: nonfinite value {result=} {m=}')\n            result.nan_to_num_(0)\n\n        try:\n            event[m] = result.item()\n        except Exception:\n            event[m] = result\n\n        # print(f'{result=}')\n        # embed, add to hidden, recurse into subquery\n        if isinstance(query.then, Query) or hasattr(query.then, '__call__'):\n            emb = embed(result)\n            hidden = hidden + emb\n            if (~hidden.isfinite()).any():\n                raise Exception(f'{m=} {result=} {emb=}')\n            return self._deep_query(query.then, hidden, event)\n        else:\n            event['path'] = query.then\n            return event\n\n    def query_tipv_onsets(self,\n        min_time=None, max_time=None, \n        include_inst=None,\n        include_pitch=None,\n        truncate_quantile_time=None,\n        truncate_quantile_pitch=None,\n        rhythm_temp=None, timing_temp=None,\n        min_vel=None, max_vel=None\n        ):\n        \"\"\"\n        for onset-only_models\n        \"\"\"\n        q = Query(\n            'time',\n            truncate=(min_time or -torch.inf, max_time or torch.inf), \n            truncate_quantile=truncate_quantile_time,\n            weight_top_p=rhythm_temp, component_temp=timing_temp,\n            then=Query(\n                'inst',\n                whitelist=include_inst,\n                then=Query(\n                    'pitch',\n                    whitelist=include_pitch,\n                    truncate_quantile=truncate_quantile_pitch,\n                    then=Query(\n                        'vel',\n                        truncate=(min_vel or 0.5, max_vel or torch.inf),\n                    )\n                )\n            )\n        )\n        return self.deep_query(q)\n\n    def query_itpv_onsets(self,\n        min_time=None, max_time=None, \n        include_inst=None,\n        include_pitch=None,\n        truncate_quantile_time=None,\n        truncate_quantile_pitch=None,\n        rhythm_temp=None, timing_temp=None,\n        min_vel=None, max_vel=None\n        ):\n        \"\"\"\n        for onset-only_models\n        \"\"\"\n        q = Query(\n            'inst',\n            whitelist=include_inst,\n            then=Query(\n                'time',\n                truncate=(min_time or -torch.inf, max_time or torch.inf), \n                truncate_quantile=truncate_quantile_time,\n                weight_top_p=rhythm_temp, component_temp=timing_temp,\n                then=Query(\n                    'pitch',\n                    whitelist=include_pitch,\n                    truncate_quantile=truncate_quantile_pitch,\n                    then=Query(\n                        'vel',\n                        truncate=(min_vel or 0.5, max_vel or torch.inf),\n                    )\n                )\n            )\n        )\n        return self.deep_query(q)\n\n    # TODO: should be possible to constrain duration per (i,p) pair,\n    # not just per instrument?\n    def query_vtip(self,\n        note_on_map:Dict[int,List[int]]|None=None, \n        note_off_map:Dict[int,List[int]]|None=None,\n        min_time:Number|None=None, max_time:Number|None=None,\n        min_vel:Number|None=None, max_vel:Number|None=None,\n        min_polyphony:Dict[int,int]|int|None=None, \n        max_polyphony:Dict[int,int]|int|None=None,\n        min_duration:Dict[int,Number]|Number|None=None, \n        max_duration:Dict[int,Number]|Number|None=None, \n        rhythm_temp:float=None, timing_temp:float=None,\n        truncate_quantile_time:Tuple[float,float]|None=None,\n        truncate_quantile_pitch:Tuple[float,float]|None=None,\n        truncate_quantile_vel:Tuple[float,float]|None=None,\n        steer_density:float=None,\n        inst_weights:Dict[int,Number]=None,\n        no_steer:List[int]=None,\n        ):\n        \"\"\"\n        Query in a fixed velocity-&gt;time-&gt;instrument-&gt;pitch order, sampling all\n        modalities. Because velocity is sampled first, this query method can \n        automatically prevent double NoteOn or NoteOff. It's also possible to\n        make some more detailed constraints per-instrument compared to `query`,\n        including note duration constraints which can eliminate stuck notes.\n\n        query_vipt is similar, but makes different compromises in applying \n        constraints. VTIP is likely to be better when setting min_time &gt; 0 \n        or otherwise heavily constraing time delta, while VIPT may be better\n        in other cases.\n\n        Args:\n            note_on_map: possible note-ons as {instrument: [pitch]} \n                defaults to allowing any note. Notes already playing on a given\n                instrument are always excluded.\n            note_off_map: possible note-offs as {instrument: [pitch]}\n                defaults to using only the instruments in note_on_map. \n                Notes not already playing on a given instrument are \n                automatically excluded.\n            min_time: global minimum interevent time (default 0)\n            max_time: global maximum interevent time (default no limit)\n            min_vel: global minimum velocity for NoteOn events (default 1)\n            max_vel: global maximum velocity for NoteOn events (default 127)\n            min_polyphony: minimum number of concurrent notes per instrument.\n                (default 0). Can be a dict mapping instrument to value,\n                or a single value for all instruments.\n                When an instrument has &lt;= min polyphony, exclude NoteOffs\n            max_polyphony: minimum number of concurrent notes per instrument.\n                (default no limit). Can be a dict mapping instrument to value,\n                or a single value for all instruments.\n                When an instrument has &gt;= max polyphony, exclude NoteOns.\n            min_duration: minimum note length per instrument (default 0). Can   \n                be a dict mapping instrument to value, or a single value for \n                all instruments.\n            max_duration: maximum note length per instrument (default 0). Can   \n                be a dict mapping instrument to value, or a single value for \n                all instruments.\n            rhythm_temp: if not None, apply top_p sampling to the weighting\n                of mixture components. this affects coarse rhythmic patterns;\n                0 is deterministic, 1 is 'natural' according to the model.\n            timing_temp: if not None, apply temperature sampling to the time\n                component. this affects fine timing; 0 is deterministic and \n                precise, 1 is 'natural' according to the model.\n            truncate_quantile_time: applied after min_time, max_time\n                truncate the remaining delta time distribution by quantile.\n                e.g. truncate_quantile_time=(0.25, 0.75)\n                excludes the shortest and longest 25% of probability mass.\n            truncate_quantile_pitch: truncate the pitch distribution by \n                quantile. e.g. truncate_quantile_pitch=(0.5, 1) always samples\n                above the median predicted pitch. Ignored for drums.\n            truncate_quantile_vel: truncate the velocity distribution by \n                quantile. e.g. truncate_quantile_vel=(0, 0.5) always\n                samples below the median predicted velocity. Affects only NoteOn.\n            steer_density: adjust relative weight of NoteOn and NoteOff.\n                values above 0.5 favor NoteOn, values below 0.5 favor NoteOff.\n            inst_weights: multiplicatively adjust instrument probabilities. \n                Any instrument not included has a weight of 1. 0 would exclude\n                an instrument completely (but better to do so via note_on_map)\n            no_steer: collection of instruments to exclude from effect of \n                truncate_quantile_pitch.\n        \"\"\"\n        # NOTE: have to add epsilon when comparing sampled times,\n        # or else rounding error can cause discrepancy \n        eps = 1e-5\n        min_time = min_time or 0\n        max_time = max_time or torch.inf\n\n        inst_weights = inst_weights or {}\n        no_steer = no_steer or set()\n\n        note_on_map, note_off_map = self.get_note_maps(\n            note_on_map, note_off_map, min_polyphony, max_polyphony\n        )\n\n        max_dur = get_from_scalar_or_dict(max_duration, torch.inf)\n        min_dur = get_from_scalar_or_dict(min_duration, 0)\n\n        # need to compute time constraints from polyphony and duration,\n        # given velocity but not inst/pitch\n        # polyphony should't affect time except via note op/off maps\n        # soonest_off can just be reduced for purposes of truncating time\n        # but then need to compute the allowed instruments, and then pitches,\n        # given the sampled time, based on duration constraints\n        # only needed in the noteoff case: then check if time &gt;= soonest_off\n        # 1. for any pitch in each instrument\n        # 2. which pitches for the sampled instrument\n\n        # duration does not constrain the soonest noteOn;\n        # the soonest possible noteOff is the next note which would end with \n        # minimal duration (but no sooner than the global min_time)\n        # compute that soonest noteOff time for each possible noteOff:\n        soonest_off = {\n            (i,p):max(min_time, min_dur(i) - self.held_notes[(i,p)]) \n            for i,ps in note_off_map.items()\n            for p in ps}\n        # print(f'{soonest_off=}')\n        soonest_off_any = min(soonest_off.values(), default=0)\n\n        # in case where only note off is allowed (likely due to max_polyphony)\n        # min_duration and max_time can be unsatisfiable\n        # break the max_time constraint in that case\n        no_on = all(len(ps)==0 for ps in note_on_map.values())\n        if no_on:\n            if soonest_off_any &gt; max_time:\n                max_time = soonest_off_any + eps\n                print(f'breaking max_time constraint -&gt; {max_time}s')\n\n        # latest possible event is minimum max remaining duration over all held notes (i.e. the soonest noteOff ending a max-duration note)\n        latest_event = max_time\n        for (i,p),t in self.held_notes.items():\n            latest_event = min(latest_event, max_dur(i) - t)\n        # slip to accomodate global constraint\n        latest_event = max(min_time, latest_event)\n\n        # print(f'pre {note_off_map=}') ###DEBUG\n\n        # remove impossible note offs\n        # (i.e. soonest possible note-off is after the latest possible event)\n        for i,ps in list(note_off_map.items()):\n            for p in list(ps):\n                if soonest_off[(i,p)] &gt; latest_event:\n                    ps.remove(p)\n            if not len(ps):\n                note_off_map.pop(i)\n                continue\n\n        # print(f'post {note_off_map=}') ###DEBUG\n        no_off = all(len(ps)==0 for ps in note_off_map.values())\n        # print(f'{no_on=} {no_off=}')\n\n        if no_on and no_off:\n            raise NoPossibleEvents(f\"\"\"\n                no possible notes {note_on_map=} {note_off_map=}\"\"\")\n\n        def insts(e):\n            if e['vel'] &gt; 0:\n                return note_on_map\n            else:\n                return {\n                    i for i,ps in note_off_map.items() if any(\n                        soonest_off[(i,p)] &lt;= e['time']+eps for p in ps\n                    )}\n\n        def pitches(e):\n            i = e['inst']\n            if e['vel'] &gt; 0:\n                return note_on_map[i]\n            else:\n                return {\n                    p for p in note_off_map[i] \n                    if soonest_off[(i,p)] &lt;= e['time']+eps}\n\n        w = 1 if steer_density is None else 2**(steer_density*2-1)\n\n        w_on = 0 if no_on else w\n        w_off = 0 if no_off else 1/w\n\n        min_vel = max(0.5, 0 if min_vel is None else min_vel)\n        max_vel = torch.inf if max_vel is None else max_vel\n\n        return self.deep_query(Query(\n            'vel', \n            cases=(\n                Range(-torch.inf,0.5,w_off), \n                Range(0.5,torch.inf,w_on,\n                    min_vel,max_vel,truncate_quantile=truncate_quantile_vel)),\n            then=lambda e: Query(\n                'time',       \n                truncate=(\n                    min_time if e['vel']&gt;0 else soonest_off_any,\n                    latest_event\n                ),\n                truncate_quantile=truncate_quantile_time,\n                weight_top_p=rhythm_temp, \n                component_temp=timing_temp,\n                then=lambda e: Query(\n                    'inst', \n                    whitelist={\n                        i:inst_weights.get(i,1) if e['vel'] &gt; 0 else 1 \n                        for i in insts(e)},\n                    then=lambda e: Query(\n                        'pitch', \n                        whitelist=pitches(e),\n                        truncate_quantile=(\n                            None if (\n                                e['vel']==0 \n                                or self.is_drum(e['inst']) \n                                or e['inst'] in no_steer)\n                            else truncate_quantile_pitch),\n        )))))\n\n    def query_vipt(self,\n        note_on_map:Dict[int,List[int]]|None=None, \n        note_off_map:Dict[int,List[int]]|None=None,\n        min_time:Number|None=None, max_time:Number|None=None,\n        min_vel:Number|None=None, max_vel:Number|None=None,\n        min_polyphony:Dict[int,int]|int|None=None, \n        max_polyphony:Dict[int,int]|int|None=None,\n        min_duration:Dict[int,Number]|Number|None=None, \n        max_duration:Dict[int,Number]|Number|None=None, \n        rhythm_temp:float=None, timing_temp:float=None,\n        truncate_quantile_time:Tuple[float,float]|None=None,\n        truncate_quantile_pitch:Tuple[float,float]|None=None,\n        truncate_quantile_vel:Tuple[float,float]|None=None,\n        steer_density:float=None,\n        inst_weights:Dict[int,Number]=None,\n        no_steer:List[int]=None,\n        ):\n        \"\"\"\n        Query in a fixed velocity-&gt;instrument-&gt;pitch-&gt;time order, sampling all\n        modalities. Because velocity is sampled first, this query method can \n        automatically prevent double noteOn or NoteOff. It's also possible to\n        make some more detailed constraints per-instrument compared to `query`,\n        including note duration constraints which can eliminate stuck notes.\n\n        query_vtip is similar, but makes different compromises in applying \n        constraints. VTIP is likely to be better when setting min_time &gt; 0 \n        or otherwise heavily constraing time delta, while VIPT may be better\n        in other cases.\n\n        Args:\n            note_on_map: possible note-ons as {instrument: [pitch]} \n                defaults to allowing any note. Notes already playing on a given\n                instrument are always excluded.\n            note_off_map: possible note-offs as {instrument: [pitch]}\n                defaults to using only the instruments in note_on_map. \n                Notes not already playing on a given instrument are \n                automatically excluded.\n            min_time: global minimum interevent time (default 0)\n            max_time: global maximum interevent time (default no limit)\n            min_vel: global minimum velocity for NoteOn events (default 1)\n            max_vel: global maximum velocity for NoteOn events (default 127)\n            min_polyphony: minimum number of concurrent notes per instrument.\n                (default 0). Can be a dict mapping instrument to value,\n                or a single value for all instruments.\n                When an instrument has &lt;= min polyphony, exclude NoteOffs\n            max_polyphony: minimum number of concurrent notes per instrument.\n                (default no limit). Can be a dict mapping instrument to value,\n                or a single value for all instruments.\n                When an instrument has &gt;= max polyphony, exclude NoteOns.\n            min_duration: minimum note length per instrument (default 0). Can   \n                be a dict mapping instrument to value, or a single value for \n                all instruments.\n            max_duration: maximum note length per instrument (default 0). Can   \n                be a dict mapping instrument to value, or a single value for \n                all instruments.\n            rhythm_temp: if not None, apply top_p sampling to the weighting\n                of mixture components. this affects coarse rhythmic patterns;\n                0 is deterministic, 1 is 'natural' according to the model.\n            timing_temp: if not None, apply temperature sampling to the time\n                component. this affects fine timing; 0 is deterministic and \n                precise, 1 is 'natural' according to the model.\n            truncate_quantile_time: applied after min_time, max_time\n                truncate the remaining delta time distribution by quantile.\n                e.g. truncate_quantile_time=(0.25, 0.75)\n                excludes the shortest 25% and longest 25% of interevent times.\n            truncate_quantile_pitch: truncate the pitch distribution by \n                quantile. e.g. truncate_quantile_pitch=(0.5, 1) always samples\n                above the median predicted pitch. Ignored for drums.\n            truncate_quantile_vel: truncate the velocity distribution by \n                quantile. e.g. truncate_quantile_vel=(0, 0.5) always\n                samples below the median predicted velocity. Affects only NoteOn.\n            steer_density: adjust relative weight of NoteOn and NoteOff.\n                values above 0.5 favor NoteOn, values below 0.5 favor NoteOff.\n            inst_weights: multiplicatively adjust instrument probabilities. \n                Any instrument not included has a weight of 1. 0 would exclude\n                an instrument completely (but better to do so via note_on_map)\n            no_steer: collection of instruments to exclude from effect of \n                truncate_quantile_pitch and truncate_quantile_time.\n        \"\"\"\n        eps = 1e-5\n        min_time = min_time or 0\n        max_time = max_time or torch.inf\n\n        inst_weights = inst_weights or {}\n        no_steer = no_steer or set()\n\n        note_on_map, note_off_map = self.get_note_maps(\n            note_on_map, note_off_map, min_polyphony, max_polyphony\n        )\n\n        max_dur = get_from_scalar_or_dict(max_duration, torch.inf)\n        min_dur = get_from_scalar_or_dict(min_duration, 0)\n\n        # duration does not constrain the soonest noteOn;\n        # the soonest possible noteOff is the next note which would end with \n        # minimal duration (but no sooner than the global min_time)\n        # compute that soonest noteOff time for each possible noteOff:\n        soonest_off = {\n            (i,p):max(min_time, min_dur(i) - self.held_notes[(i,p)]) \n            for i,ps in note_off_map.items()\n            for p in ps}\n        # print(f'{soonest_off=}')\n\n        # in case where only note off is allowed (likely due to max_polyphony)\n        # min_duration and max_time can be unsatisfiable\n        # break the max_time constraint in that case\n        no_on = all(len(ps)==0 for ps in note_on_map.values())\n        if no_on:\n            soonest_off_any = min(soonest_off.values(), default=0)\n            if soonest_off_any &gt; max_time:\n                max_time = soonest_off_any + eps\n                print(f'breaking max_time constraint -&gt; {max_time}s')\n\n        # latest possible event is minimum max remaining duration over all held notes (i.e. the soonest noteOff ending a max-duration note)\n        # or the global max interevent time, if shorter\n        latest_event = max_time\n        for (i,p),t in self.held_notes.items():\n            latest_event = min(latest_event, max_dur(i) - t)\n        # slip to accomodate global constraint\n        latest_event = max(min_time, latest_event)\n\n        # if latest_event is &lt;= min_time, probably means one of two things:\n        # 1. some notes are already over time and should be prioritized to end\n        # we don't want noteoffs which would prevent ending a different note on time -- except in the case where the soonest noteoff is already late according to global min_time; any such noteOff is valid\n        # since both latest_event and soonest_off are clipped to min_time --\n        # we can exclude noteOffs when soonest_off &gt; latest_event,\n        # but allow soonest_off==latest_event\n        # 2. polyphony+duration contraints contradict max_time\n        # i.e. solo monophonic instrument has min_duration = 5, \n        # but max_time is 3 -- nothing to do after 3 seconds\n        # ought to break the max_time constraint in this case;\n        # can set max_time = max(max_time, min(remaining min duration of held notes))\n\n        # remove impossible note offs\n        # (i.e. soonest possible note-off is after the latest possible event)\n        for i,ps in list(note_off_map.items()):\n            for p in list(ps):\n                if soonest_off[(i,p)] &gt; latest_event:\n                    ps.remove(p)\n            if not len(ps):\n                note_off_map.pop(i)\n                continue\n\n        no_off = all(len(ps)==0 for ps in note_off_map.values())\n        # print(f'{no_on=} {no_off=}')\n\n        if no_on and no_off:\n            # if len(soonest_off):\n            #     i_off,p_off = min(soonest_off, key=soonest_off.__getitem__)\n            #     note_off_map = {i_off:[p_off]}\n            #     print('breaking constraint to allow note off')\n            # else:\n            raise ValueError(f\"\"\"\n                no possible notes {note_on_map=} {note_off_map=}\"\"\")\n\n        def note_map(e):\n            try:\n                if e['vel'] &gt; 0:\n                    m = note_on_map\n                else:\n                    m = note_off_map\n                i = e.get('inst')\n                if i is not None:\n                    m = m[i]\n                return m\n            except Exception:\n                traceback.print_exc()\n                print(f'{e=} {note_off_map=} {note_on_map=}')\n                raise\n            # print(f'{m=}')\n\n        w = 1 if steer_density is None else 2**(steer_density*2-1)\n\n        w_on = 0 if no_on else w\n        w_off = 0 if no_off else 1/w\n\n        min_vel = max(0.5, 0 if min_vel is None else min_vel)\n        max_vel = torch.inf if max_vel is None else max_vel\n\n        # print(f'{truncate_quantile_vel=}')\n\n        return self.deep_query(Query(\n            'vel', \n            cases=(\n                Range(-torch.inf,0.5,w_off), \n                Range(0.5,torch.inf,w_on,\n                    min_vel,max_vel,truncate_quantile=truncate_quantile_vel)),\n            then=lambda e: Query(\n                'inst', \n                whitelist={\n                    i:inst_weights.get(i,1) if e['vel'] &gt; 0 else 1 \n                    for i in note_map(e)},\n                then=lambda e: Query(\n                    'pitch', \n                    whitelist=note_map(e),\n                    truncate_quantile=(\n                        None if (\n                            e['vel']==0 \n                            or self.is_drum(e['inst']) \n                            or e['inst'] in no_steer)\n                        else truncate_quantile_pitch),\n                    then=lambda e: Query(\n                        'time', #'note on' if e['vel']&gt;0 else 'note off',         \n                        truncate=(\n                            min_time if e['vel']&gt;0 \n                            else soonest_off[(e['inst'],e['pitch'])],\n                            latest_event\n                        ),\n                        truncate_quantile=(\n                            None if e['inst'] in no_steer\n                            else truncate_quantile_time),\n                        weight_top_p=rhythm_temp, \n                        component_temp=timing_temp\n        )))))\n\n    # def query_ipvt(self,\n    #     note_map, \n    #     min_time=-torch.inf, max_time=torch.inf, \n    #     min_vel=-torch.inf, max_vel=torch.inf,\n    #     truncate_quantile_time=None,\n    #     truncate_quantile_pitch=None,\n    #     ):\n    #     \"\"\"\n    #     \"\"\"\n\n    #     return self.deep_query(Query(\n    #         'inst', then=[(\n    #             Subset([i]), Query(\n    #                 'pitch', \n    #                 whitelist=list(ps), \n    #                 truncate_quantile=truncate_quantile_pitch,\n    #                 then=Query(\n    #                     'vel',\n    #                     truncate=(min_vel or -torch.inf, max_vel or torch.inf),\n    #                     then=Query(\n    #                         'time',         \n    #                         truncate=(min_time or -torch.inf, max_time or torch.inf), truncate_quantile=truncate_quantile_time\n    #                     )\n    #                 )\n    #             )\n    #         ) for i,ps in note_map.items() if len(ps)]\n    #     ))\n\n    # TODO: remove pitch_topk and sweep_time?\n    # TODO: rewrite this to build queries and dispatch to deep_query\n    def query(self,\n            next_inst:int=None, next_pitch:int=None, \n            next_time:float=None, next_vel:int=None,\n\n            allow_end:bool=False,\n\n            include_inst:List[int]=None, exclude_inst:List[int]=None,\n            allow_anon:bool=True, \n            instrument_temp:float=None, \n\n            include_pitch:List[int]=None, exclude_pitch:List[int]=None,\n            include_drum:List[int]=None,\n            truncate_quantile_pitch:Tuple[float,float]=None,\n            pitch_temp:float=None, \n            index_pitch:int=None,\n\n            min_time:float=None, max_time:float=None,\n            truncate_quantile_time:Tuple[float, float]=None,\n            rhythm_temp:float=None, timing_temp:float=None,\n\n            min_vel:int=None, max_vel:int=None,\n            velocity_temp:float=None,\n\n            pitch_topk:int=None, sweep_time:bool=False, \n\n            handle:str=None, return_params:bool=False\n            ) -&gt; dict:\n        \"\"\"\n        Sample a prediction for the next MIDI event.\n\n        various constraints on the the next event can be requested.\n\n        Args:\n            # hard constraints\n\n            next_inst: fix a particular instrument for the predicted event.\n                sampled values will always condition on fixed values, so passing\n                `next_inst=1`, for example, will make the event appropriate\n                for the Grand Piano (instrument 1) to play.\n            next_pitch: fix a particular MIDI number for the predicted event.\n                sampled values will always condition on fixed values, so passing\n                `next_pitch=60`, for example, will make the event a middle C\n                (for melodic instruments) or High Bongo (for drums)\n            next_time: fix a particular delta time for the predicted event.\n                sampled values will always condition on fixed values, so passing\n                `next_time=0`, for example, will make the event concurrent with\n                the previous event.\n            next_vel: fix a particular velocity for the predicted event.\n                sampled values will always condition on fixed values, so passing\n                `next_inst=0`, for example, will ensure the event is a noteOff.\n\n            # partial constraints\n\n            include_inst: instrument id(s) to include in sampling.\n                (if not None, all others will be excluded)\n            exclude_inst: instrument id(s) to exclude from sampling.\n            allow_anon: bool. if False, zero probability of anon instruments\n\n            include_pitch: pitch(es) to include in sampling.\n                (if not None, all others will be excluded)\n            exclude_pitch: pitch(es) to exclude from sampling.\n            include_drum: like `include_pitch`, but only in effect when \n                instrument is a drumkit\n\n            min_time: if not None, truncate the time distribution below\n            max_time: if not None, truncate the time distribution above\n\n            min_vel: if not None, truncate the velocity distribution below\n                e.g., `min_vel=1` prevents NoteOff events\n            max_vel: if not None, truncate the velocity distribution above\n\n            allow_end: if False, zero probability of sampling the end marker\n\n            # sampling strategies\n\n            instrument_temp: if not None, apply top_p sampling to instrument. 0 is\n                deterministic, 1 is 'natural' according to the model\n\n            pitch_temp: if not None, apply top_p sampling to pitch. 0 is\n                deterministic, 1 is 'natural' according to the model\n            truncate_quantile_pitch: applied after include_pitch, exclude_pitch\n                truncate the remaining pitch distribution by quantile.\n                e.g. truncate_quantile_pitch=(0.25, 0.75)\n                excludes the lowest and highest 25% of pitches\n            index_pitch: if not None, deterministically take the\n                nth most likely pitch instead of sampling.\n\n            timing_temp: if not None, apply temperature sampling to the time\n                component. this affects fine timing; 0 is deterministic and \n                precise, 1 is 'natural' according to the model.\n            rhythm_temp: if not None, apply top_p sampling to the weighting\n                of mixture components. this affects coarse rhythmic patterns;\n                0 is deterministic, 1 is 'natural' according to the model.\n            truncate_quantile_time: applied after min_time, max_time\n                truncate the remaining delta time distribution by quantile.\n                e.g. truncate_quantile_time=(0.25, 0.75)\n                excludes the shortest 25% and longest 25% of interevent times.\n\n            velocity_temp: if not None, apply temperature sampling to the \n                velocity component.\n\n            # multiple predictions\n\n            pitch_topk: Optional[int]. if not None, instead of sampling pitch, \n                stack the top k most likely pitches along the batch dimension\n            sweep_time: if True, instead of sampling time, choose a diverse set\n            of times and stack along the batch dimension\n\n            # other\n\n            handle: metadata to be included in the returned dict, if not None\n            return_params: if True, return tensors of distribution parameters\n                under the keys `inst_params`, `pitch_params`, `time_params`,\n                and `vel_params`.\n\n        Returns:\n            'inst': int. id of predicted instrument.\n                1-128 are General MIDI standard melodic instruments\n                129-256 are drumkits for MIDI programs 1-128\n                257-288 are 'anonymous' melodic instruments\n                289-320 are 'anonymous' drumkits\n            'pitch': int. predicted MIDI number of next note, 0-128.\n            'time': float. predicted time to next note in seconds.\n            'vel': float. unquantized predicted velocity of next note.\n                0-127; hard 0 indicates a note-off event.\n            'end': int. value of 1 indicates the *current* event (the one \n                passed as arguments to `predict`) was the last event, and the\n                predicted event should *not* be played. if `allow end` is false, \n                this will always be 0.\n            'step': int. number of steps since calling `reset`.\n            '*_params': tensor. distribution parameters for visualization\n                and debugging purposes. present if `return_params` is True.\n\n        NOTE: `instrument`, `pitch`, `time`, `velocity` may return lists,\n            when using `sweep_time` or `pitch_topk`. that part of the API \n            is very experimental and likely to break.\n        \"\"\"\n         # validate options:\n        if (index_pitch is not None) and (pitch_temp is not None):\n            print(\"warning: `index pitch` overrides `pitch_temp`\")\n\n        inst_intervention = any(p is not None for p in (\n            instrument_temp, include_inst, exclude_inst))\n\n        pitch_intervention = (pitch_topk or any(p is not None for p in (\n            pitch_temp, include_pitch, exclude_pitch, include_drum)))\n\n        time_intervention = any(p is not None for p in (\n            min_time, max_time, rhythm_temp, timing_temp))\n\n        vel_intervention = any(p is not None for p in (\n            min_vel, max_vel, velocity_temp))\n\n        exclude_inst = arg_to_set(exclude_inst)\n        if not allow_anon:\n            exclude_inst |= set(range(257, 321))\n        constrain_inst = list((\n            set(range(self.instrument_domain)) - {self.instrument_start_token}\n            if include_inst is None \n            else arg_to_set(include_inst)\n        ) - exclude_inst)\n        if len(constrain_inst)==0:\n            raise ValueError(\"\"\"\n            every instrument has been excluded. check values of \n            `include_inst` and `exclude_inst`\n            \"\"\")\n        # elif len(constrain_inst)==1:\n        #     print(\"\"\"\n        #     warning: recommended to use `next_inst`, not \n        #     `include_inst` to allow only one specific instrument\n        #     \"\"\")\n\n        constrain_pitch = list((\n            set(range(self.pitch_domain)) - {self.pitch_start_token}\n            if include_pitch is None \n            else arg_to_set(include_pitch)\n        ) - arg_to_set(exclude_pitch))\n        if len(constrain_pitch)==0:\n            raise ValueError(\"\"\"\n            every pitch has been excluded. check values of \n            `include_pitch` and `exclude_pitch`\n            \"\"\")\n        elif len(constrain_pitch)==1:\n            print(\"\"\"\n            warning: recommended to use `next_pitch`, not \n            `include_pitch` to allow only one specific pitch\n            \"\"\")\n\n        # TODO: this got really complicated to support include_drum...\n        # really want to edit the whole joint distribution of pitch,inst in \n        # cases where certain pitches or drums need to be excluded...\n        # would that be practical? if there are ~40000 inst x pitch combos?\n        # would need to run the instrument head for a whole batch of all\n        # allowable pitches or vice-versa...\n        def sample_instrument(x):\n            # if include_drum is supplied, make sure to exclude drum instruments\n            # when no pitch is in the allowed drums\n            if include_drum is not None:\n                pit = predicted_by_name('pitch')\n                pits = [pit] if pit is not None else constrain_pitch\n                if pits is not None and all(pit not in include_drum for pit in pits):\n                    nonlocal constrain_inst\n                    if constrain_inst is None:\n                        constrain_inst = range(1,self.instrument_domain)\n                    constrain_inst = [\n                        i for i in constrain_inst if not self.is_drum(i)]\n\n            # if constrain_inst is not None:\n            #     preserve_x = x[...,constrain_inst]\n            #     x = torch.full_like(x, -torch.inf)\n            #     x[...,constrain_inst] = preserve_x\n            # probs = x.softmax(-1)\n            # if instrument_temp is not None:\n            #     probs = reweight_top_p(probs, instrument_temp)\n            # return D.Categorical(probs).sample()\n\n            return categorical_sample(x, \n                whitelist=constrain_inst,\n                top_p=instrument_temp)\n\n        def sample_pitch(x):\n            # conditional constraint\n            if include_drum is not None:\n                # if this event is / must be a drum,\n                # use include_drum instead of constrain_inst\n                inst = predicted_by_name('instrument')\n                insts = [inst] if inst is not None else constrain_inst\n                if insts is not None and all(self.is_drum(i) for i in insts):\n                    nonlocal constrain_pitch\n                    constrain_pitch = include_drum\n\n            if pitch_topk is not None:\n                raise NotImplementedError\n\n            return categorical_sample(x,\n                whitelist=constrain_pitch, \n                index=index_pitch,\n                top_p=pitch_temp,\n                truncate_quantile=truncate_quantile_pitch\n                )\n            # if constrain_pitch is not None:\n            #     preserve_x = x[...,constrain_pitch]\n            #     x = torch.full_like(x, -torch.inf)\n            #     x[...,constrain_pitch] = preserve_x\n            # # x is modified logits\n\n            # if index_pitch is not None:\n            #     return x.argsort(-1, True)[...,index_pitch]\n            # elif pitch_topk is not None:\n            #     return x.argsort(-1, True)[...,:pitch_topk].transpose(0,-1)\n\n            # probs = x.softmax(-1)\n            # if pitch_temp is not None:\n            #     probs = reweight_top_p(probs, pitch_temp)\n\n            # if steer_pitch is not None:\n            #     return steer_categorical(probs, steer_pitch)\n            # else:\n            #     return D.Categorical(probs).sample()\n\n        def sample_time(x):\n            # TODO: respect trunc_time when sweep_time is True\n            if sweep_time:\n                if min_time is not None or max_time is not None:\n                    raise NotImplementedError(\"\"\"\n                    min_time/max_time with sweep_time needs implementation\n                    \"\"\")\n                assert x.shape[0]==1, \"batch size should be 1 here\"\n                log_pi, loc, s = self.time_dist.get_params(x)\n                idx = log_pi.squeeze().argsort()[:9]\n                loc = loc.squeeze()[idx].sort().values[...,None] \n                # multiple times in batch dim\n                # print(loc.shape)\n                return loc\n\n            trunc = (\n                -torch.inf if min_time is None else min_time,\n                torch.inf if max_time is None else max_time)\n\n            return self.time_dist.sample(x, \n                truncate=trunc,\n                component_temp=timing_temp, \n                weight_top_p=rhythm_temp,\n                truncate_quantile=truncate_quantile_time\n                )\n\n        def sample_velocity(x):\n            trunc = (\n                -torch.inf if min_vel is None else min_vel,\n                torch.inf if max_vel is None else max_vel)\n            return self.vel_dist.sample(\n                x, component_temp=velocity_temp, truncate=trunc,\n                # truncate_quantile=truncate_quantile_vel\n                )\n\n        with torch.inference_mode():\n            if self.h_query is None:\n                self.h_query = self.h_proj(self.h)\n\n            modalities = list(zip(\n                self.projections,\n                (sample_instrument, sample_pitch, sample_time, sample_velocity),\n                self.embeddings,\n                ))\n\n            context = [self.h_query] # embedded outputs for autoregressive prediction\n            predicted = [] # raw outputs\n            params = [] # distribution parameters for visualization\n\n            fix = [\n                None if item is None else torch.tensor([[item]], dtype=dtype)\n                for item, dtype in zip(\n                    [next_inst, next_pitch, next_time, next_vel],\n                    [torch.long, torch.long, torch.float, torch.float])]\n\n            # if any modalities are determined, embed them\n            # sort constrained modalities before unconstrained\n            # TODO: option to skip modalities\n            det_idx, cons_idx, uncons_idx = [], [], []\n            for i,(item, embed) in enumerate(zip(fix, self.embeddings)):\n                if item is None:\n                    if (\n                        i==0 and inst_intervention or\n                        i==1 and pitch_intervention or\n                        i==2 and time_intervention or\n                        i==3 and vel_intervention):\n                        cons_idx.append(i)\n                    else:\n                        uncons_idx.append(i)\n                else:\n                    det_idx.append(i)\n                    context.append(embed(item))\n                    predicted.append(item)\n                    params.append(None)\n            undet_idx = cons_idx + uncons_idx\n            perm = det_idx + undet_idx # permutation from the canonical order\n            iperm = argsort(perm) # inverse permutation back to canonical order\n\n            mode_names = ['instrument', 'pitch', 'time', 'velocity']\n            name_to_idx = {k:v for k,v in zip(mode_names, iperm)}\n            def predicted_by_name(name):\n                idx = name_to_idx[name]\n                if len(predicted) &gt; idx:\n                    return predicted[idx]\n                return None\n            # print('sampling order:', [mode_names[i] for i in perm])\n\n            # for each undetermined modality, \n            # sample a new value conditioned on already determined ones\n\n            running_ctx = sum(context)\n            # print(running_ctx)\n            # perm_h_tgt = [h_tgt[i] for i in perm]\n            while len(undet_idx):\n                # print(running_ctx.norm())\n                i = undet_idx.pop(0) # index of modality to determine\n                # j = len(det_idx) # number already determined\n                project, sample, embed = modalities[i]\n                # determine value for the next modality\n                hidden = running_ctx.tanh()\n                params.append(project(hidden))\n                pred = sample(params[-1])\n                predicted.append(pred)\n                # prepare for next iteration\n                if len(undet_idx):\n                    # context.append(embed(pred))\n                    running_ctx += embed(pred)\n                det_idx.append(i)\n\n            pred_inst = predicted_by_name('instrument')\n            pred_pitch = predicted_by_name('pitch')\n            pred_time = predicted_by_name('time')\n            pred_vel = predicted_by_name('velocity')\n\n            if allow_end:\n                end_params = self.end_proj(self.h)\n                # print(end_params)\n                end = D.Categorical(logits=end_params).sample()\n            else:\n                end = torch.zeros(self.h.shape[:-1])\n\n            if sweep_time or pitch_topk:\n                # return lists of predictions\n                pred_inst = [x.item() for x in pred_inst]\n                pred_pitch = [x.item() for x in pred_pitch]\n                pred_time = [x.item() for x in pred_time]\n                pred_vel = [x.item() for x in pred_vel]\n                end = [x.item() for x in end]\n                # print(pred_time, pred_pitch, pred_vel)\n            else:\n                # return single predictions\n                pred_inst = pred_inst.item()\n                pred_pitch = pred_pitch.item()\n                pred_time = pred_time.item()\n                pred_vel = pred_vel.item()\n                end = end.item()\n\n            r = {\n                'inst': pred_inst,\n                'pitch': pred_pitch, \n                'time': pred_time,\n                'vel': pred_vel,\n\n                'end': end,\n                'step': self.step,\n            }\n\n            if handle is not None:\n                r['handle'] = handle\n\n            if return_params:\n                r |= {\n                    'inst_params': params[iperm[0]],\n                    'pitch_params': params[iperm[1]],\n                    'time_params': params[iperm[2]],\n                    'vel_params': params[iperm[3]]\n                }\n\n            return r\n\n    def predict(self, inst, pitch, time, vel, **kw):\n        \"\"\"\n        DEPRECATED: alias for feed_query\n        \"\"\"\n        self.feed(inst, pitch, time, vel)\n        return self.query(**kw)\n\n    def feed_query(self, inst:int, pitch:int, time:Number, vel:Number, \n **kw):\n        \"\"\"\n        feed an event to the model, \n        then query for the next predicted event and return it.\n        \"\"\"\n        self.feed(inst, pitch, time, vel)\n        return self.query(**kw)\n\n    def query_feed(self, *a, **kw):\n        \"\"\"\n        query for the next predicted event and immediately feed it to the model,\n        also returning the predicted event.\n        \"\"\"\n        r = self.query(*a, **kw)\n        self.feed(r['inst'], r['pitch'], r['time'], r['vel'])\n        return r\n\n    def feed_query_feed(self, \n            inst:int, pitch:int, time:Number, vel:Number, \n            **kw):\n        \"\"\"\n        given an event, return the next predicted event, \n        feeding both to the model.\n        \"\"\" \n        self.feed(inst, pitch, time, vel)\n        return self.query_feed(**kw)\n\n    def reset(self, start=None, state=None):\n        \"\"\"\n        resets internal model state.\n        Args:\n            start: if True, send start tokens through the model\n                default behavior is True when state=None, False otherwise\n            state: set the state from a result of `get_state`,\n                instead of the initial state\n        \"\"\"\n        self.current_time = 0\n        self.held_notes.clear()\n        self.step = 0\n        if start is None:\n            start = state is None\n        if state is None: \n            named_states = zip(self.cell_state_names(), self.initial_state)\n        else:\n            named_states = state.items()\n        self.h_query = None\n        with torch.inference_mode():\n            for n,t in named_states:\n                getattr(self, n)[:] = t\n            if start:\n                self.feed(\n                    self.instrument_start_token, self.pitch_start_token, 0., 0.)\n        # for n,t in zip(self.cell_state_names(), self.initial_state):\n        #     getattr(self, n)[:] = t.detach()\n        # if start:\n        #     self.feed(\n        #         self.instrument_start_token, self.pitch_start_token, 0., 0.)\n\n    def get_state(self) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"return a dict of {str:Tensor} representing the model state\"\"\"\n        return {n:getattr(self, n).clone() for n in self.cell_state_names()}\n\n\n    @classmethod\n    def user_data_dir(cls):\n        return _user_data_dir()\n\n    @classmethod\n    def from_checkpoint(cls, path):\n        \"\"\"\n        create a Notochord from a checkpoint file containing \n        hyperparameters and model weights.\n\n        Args:\n            path: file path to Notochord model\n        \"\"\"\n        if path==\"notochord-latest.ckpt\":\n            url = 'https://github.com/Intelligent-Instruments-Lab/iil-python-tools/releases/download/notochord-v0.4.0/notochord_lakh_50G_deep.pt'\n        elif path==\"txala-latest.ckpt\":\n            url = 'https://github.com/Intelligent-Instruments-Lab/notochord/releases/download/notochord-v0.5.4/noto-txala-011-0020.ckpt'\n        else:\n            url = None\n\n        if url is not None:\n            d = Notochord.user_data_dir()\n            path = d / path\n            # maybe download\n            if not path.is_file():\n                while True:\n                    answer = input(\"Do you want to download a notochord model? (y/n)\")\n                    if answer.lower() in [\"y\",\"yes\"]:\n                        download_url(url, path)\n                        print(f'saved to {path}')\n                        break\n                    if answer.lower() in [\"n\",\"no\"]:\n                        break\n        # path = \n        checkpoint = torch.load(\n            path, map_location=torch.device('cpu'), weights_only=False)\n        model = cls(**checkpoint['kw']['model'])\n        model.load_state_dict(checkpoint['model_state'], strict=False)\n        model.checkpoint_path = path\n        model.reset()\n        model.eval()\n        return model\n\n    def prompt(self, midi_file, merge=False):\n        \"\"\"Read a MIDI file and feed events to this Notochord model.\n\n        When possible, the hidden states will be cached so re-using the same prompt will be fast.\n\n        Args:\n            midi_file: path of a midi file to read\n        Returns:\n            state: hidden state dict of the Notochord encoding the MIDI prompt\n            channel_inst: dict mapping MIDI channel (0-index) to Notochord instrument (1-256)\n        \"\"\"\n        return prompt(\n            self, Path(midi_file), \n            merge=merge, state_hash=hash_states(self.get_state()))\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.Notochord.__init__","title":"<code>__init__(emb_size=256, rnn_hidden=2048, rnn_layers=1, kind='gru', mlp_layers=0, dropout=0.1, norm=None, num_pitches=128, num_instruments=320, time_sines=128, vel_sines=128, time_bounds=(0, 10), time_components=32, time_res=0.01, vel_components=16)</code>","text":"Source code in <code>src/notochord/model.py</code> <pre><code>def __init__(self, \n        emb_size=256, \n        rnn_hidden=2048, rnn_layers=1, kind='gru', \n        mlp_layers=0,\n        dropout=0.1, norm=None,\n        num_pitches=128, \n        num_instruments=320,\n        time_sines=128, vel_sines=128,\n        time_bounds=(0,10), time_components=32, time_res=1e-2,\n        vel_components=16\n        ):\n    \"\"\"\n    \"\"\"\n    super().__init__()\n\n    self.step = 0\n    self.current_time = 0\n    self.held_notes = {}\n\n    self.note_dim = 4 # instrument, pitch, time, velocity\n\n    self.instrument_start_token = 0\n    self.instrument_domain = num_instruments+1\n\n    self.pitch_start_token = num_pitches\n    self.pitch_domain = num_pitches+1\n\n    self.max_dt = time_bounds[1]\n    self.time_dist = CensoredMixtureLogistic(\n        time_components, time_res, \n        sharp_bounds=(1e-4,2e3),\n        lo=time_bounds[0], hi=time_bounds[1], init='time')\n    self.vel_dist = CensoredMixtureLogistic(\n        vel_components, 1.0,\n        sharp_bounds=(1e-3,128),\n        lo=0, hi=127, init='velocity')\n\n    # embeddings for inputs\n    self.instrument_emb = nn.Embedding(self.instrument_domain, emb_size)\n    self.pitch_emb = nn.Embedding(self.pitch_domain, emb_size)\n    self.time_emb = (#torch.jit.script(\n        SineEmbedding(\n        time_sines, emb_size, 1e-3, 30, scale='log'))\n    # self.vel_emb = MixEmbedding(emb_size, (0, 127))\n    self.vel_emb = (#torch.jit.script(\n        SineEmbedding(\n        vel_sines, emb_size, 2, 512, scale='lin'))\n\n    # RNN backbone\n    self.rnn = GenericRNN(kind, \n        emb_size, rnn_hidden, \n        num_layers=rnn_layers, batch_first=True, dropout=dropout)\n\n    # learnable initial RNN state\n    self.initial_state = nn.ParameterList([\n         # layer x batch x hidden\n        nn.Parameter(torch.randn(rnn_layers,1,rnn_hidden)*rnn_hidden**-0.5)\n        for _ in range(2 if kind=='lstm' else 1)\n    ])\n\n    mlp_cls = GLUMLP#lambda *a: torch.jit.script(GLUMLP(*a))\n    # projection from RNN state to distribution parameters\n    self.h_proj = mlp_cls(\n            rnn_hidden, emb_size, emb_size, \n            mlp_layers, dropout, norm)\n    self.projections = nn.ModuleList([\n        mlp_cls(\n            emb_size, emb_size, self.instrument_domain, \n            mlp_layers, dropout, norm),\n        mlp_cls(\n            emb_size, emb_size, self.pitch_domain, \n            mlp_layers, dropout, norm),\n        mlp_cls(\n            emb_size, emb_size, self.time_dist.n_params,\n            mlp_layers, dropout, norm),\n        mlp_cls(\n            emb_size, emb_size, self.vel_dist.n_params, \n            mlp_layers, dropout, norm),\n    ])\n\n    self.end_proj = nn.Linear(rnn_hidden, 2)\n\n    with torch.no_grad():\n        for p in self.projections:\n            p.net[-1].weight.mul_(1e-2)\n        self.end_proj.weight.mul(1e-2)\n\n    # persistent RNN state for inference\n    for n,t in zip(self.cell_state_names(), self.initial_state):\n        self.register_buffer(n, t.clone())\n    self.step = 0\n\n    # volatile hidden states for caching purposes\n    self.h = None\n    self.h_query = None\n\n    self._default_note_map = {\n        i:range(128) for i in range(1,self.instrument_domain+1)}      \n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.Notochord.deep_query","title":"<code>deep_query(query, predict_end=True)</code>","text":"<p>flexible querying with nested Query objects. see query_vtip for an example.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>Query object</p> required Source code in <code>src/notochord/model.py</code> <pre><code>def deep_query(self, query, predict_end=True):\n    \"\"\"flexible querying with nested Query objects.\n    see query_vtip for an example.\n\n    Args:\n        query: Query object\n    \"\"\"\n    with torch.inference_mode():\n        if self.h_query is None:\n            self.h_query = self.h_proj(self.h)\n        event = self._deep_query(\n            query, hidden=self.h_query[:,0], event={})\n\n        if predict_end:\n            # print('END')\n            # print(f'{self.h}')\n            end_params = self.end_proj(self.h)\n            event['end'] = end_params.softmax(-1)[...,1].item()\n            # event['end'] = D.Categorical(logits=end_params).sample().item()\n        else:\n            event['end'] = 0#torch.zeros(self.h.shape[:-1])\n\n    return event\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.Notochord.feed","title":"<code>feed(inst, pitch, time, vel, **kw)</code>","text":"<p>consume an event and advance hidden state</p> <p>Parameters:</p> Name Type Description Default <code>inst</code> <code>int</code> <p>int. instrument of current note. 0 is start token 1-128 are General MIDI instruments 129-256 are drumkits (MIDI 1-128 on channel 13) 257-288 are 'anonymous' melodic instruments 289-320 are 'anonymous' drumkits</p> required <code>pitch</code> <code>int</code> <p>int. MIDI pitch of current note. 0-127 are MIDI pitches / drums 128 is start token</p> required <code>time</code> <code>Number</code> <p>float. elapsed time in seconds since previous event.</p> required <code>vel</code> <code>Number</code> <p>float. (possibly dequantized) MIDI velocity from 0-127 inclusive. 0 indicates a note-off event</p> required <code>**kw</code> <p>ignored (allows doing e.g. noto.feed(**noto.query(...)))</p> <code>{}</code> Source code in <code>src/notochord/model.py</code> <pre><code>def feed(self, inst:int, pitch:int, time:Number, vel:Number, **kw):\n    \"\"\"consume an event and advance hidden state\n\n    Args:\n        inst: int. instrument of current note.\n            0 is start token\n            1-128 are General MIDI instruments\n            129-256 are drumkits (MIDI 1-128 on channel 13)\n            257-288 are 'anonymous' melodic instruments\n            289-320 are 'anonymous' drumkits\n        pitch: int. MIDI pitch of current note.\n            0-127 are MIDI pitches / drums\n            128 is start token\n        time: float. elapsed time in seconds since previous event.\n        vel: float. (possibly dequantized) MIDI velocity from 0-127 inclusive.\n            0 indicates a note-off event\n        **kw: ignored (allows doing e.g. noto.feed(**noto.query(...)))\n    \"\"\"\n    # print(f'FEED from {threading.get_ident()}') \n    # print('feed', inst, pitch, time, vel)\n\n    # track elapsed time and ongoing notes\n    key = (inst,pitch)\n    for k in self.held_notes:\n        self.held_notes[k] += time\n    self.current_time += time\n    self.step += 1\n\n    if vel &gt; 0:\n        self.held_notes[key] = 0\n    elif key in self.held_notes:\n        self.held_notes.pop(key)\n\n    # print(self.held_notes)\n\n    # update RNN state\n\n    with torch.inference_mode():\n        inst = torch.LongTensor([[inst]]) # 1x1 (batch, time)\n        pitch = torch.LongTensor([[pitch]]) # 1x1 (batch, time)\n        time = torch.FloatTensor([[time]]) # 1x1 (batch, time)\n        vel = torch.FloatTensor([[vel]]) # 1x1 (batch, time)\n\n        embs = [\n            self.instrument_emb(inst), # 1, 1, emb_size\n            self.pitch_emb(pitch), # 1, 1, emb_size\n            self.time_emb(time),# 1, 1, emb_size\n            self.vel_emb(vel)# 1, 1, emb_size\n        ]\n        x = sum(embs)\n\n        self.h, new_state = self.rnn(x, self.cell_state)\n        for t,new_t in zip(self.cell_state, new_state):\n            t[:] = new_t\n\n        self.h_query = None\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.Notochord.feed_query","title":"<code>feed_query(inst, pitch, time, vel, **kw)</code>","text":"<p>feed an event to the model,  then query for the next predicted event and return it.</p> Source code in <code>src/notochord/model.py</code> <pre><code>   def feed_query(self, inst:int, pitch:int, time:Number, vel:Number, \n**kw):\n       \"\"\"\n       feed an event to the model, \n       then query for the next predicted event and return it.\n       \"\"\"\n       self.feed(inst, pitch, time, vel)\n       return self.query(**kw)\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.Notochord.feed_query_feed","title":"<code>feed_query_feed(inst, pitch, time, vel, **kw)</code>","text":"<p>given an event, return the next predicted event,  feeding both to the model.</p> Source code in <code>src/notochord/model.py</code> <pre><code>def feed_query_feed(self, \n        inst:int, pitch:int, time:Number, vel:Number, \n        **kw):\n    \"\"\"\n    given an event, return the next predicted event, \n    feeding both to the model.\n    \"\"\" \n    self.feed(inst, pitch, time, vel)\n    return self.query_feed(**kw)\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.Notochord.forward","title":"<code>forward(instruments, pitches, times, velocities, ends, validation=False, ar_mask=None)</code>","text":"<p>teacher-forced probabilistic loss and diagnostics for training.</p> <p>Parameters:</p> Name Type Description Default <code>instruments</code> <p>LongTensor[batch, time]</p> required <code>pitches</code> <p>LongTensor[batch, time]</p> required <code>times</code> <p>FloatTensor[batch, time]</p> required <code>velocities</code> <p>FloatTensor[batch, time]</p> required <code>ends</code> <p>LongTensor[batch, time]</p> required <code>validation</code> <p>bool (computes some extra diagnostics)</p> <code>False</code> <code>ar_mask</code> <p>Optional[Tensor[note_dim x note_dim]] if None, generate random masks for training</p> <code>None</code> Source code in <code>src/notochord/model.py</code> <pre><code>def forward(self, instruments, pitches, times, velocities, ends,\n        validation=False, ar_mask=None):\n    \"\"\"\n    teacher-forced probabilistic loss and diagnostics for training.\n\n    Args:\n        instruments: LongTensor[batch, time]\n        pitches: LongTensor[batch, time]\n        times: FloatTensor[batch, time]\n        velocities: FloatTensor[batch, time]\n        ends: LongTensor[batch, time]\n        validation: bool (computes some extra diagnostics)\n        ar_mask: Optional[Tensor[note_dim x note_dim]] if None, generate random\n            masks for training\n    \"\"\"\n    batch_size, batch_len = pitches.shape\n\n    self.checkpoint_path = None\n\n    # embed data to input vectors\n    inst_emb = self.instrument_emb(instruments) # batch, time, emb_size\n    pitch_emb = self.pitch_emb(pitches) # batch, time, emb_size\n    time_emb = self.time_emb(times) # batch, time, emb_size\n    vel_emb = self.vel_emb(velocities) # batch, time, emb_size\n\n    embs = (inst_emb, pitch_emb, time_emb, vel_emb)\n\n    # feed to RNN backbone\n    x = sum(embs)\n    ## broadcast initial state to batch size\n    initial_state = tuple(\n        t.expand(self.rnn.num_layers, x.shape[0], -1).contiguous() # 1 x batch x hidden\n        for t in self.initial_state)\n    h, _ = self.rnn(x, initial_state) #batch, time, hidden_size\n\n    # fit all event factorizations \n    # e.g. inst-&gt;pitch-&gt;time-&gt;vel vs vel-&gt;time-&gt;inst-&gt;pitch\n    trim_h = h[:,:-1]\n    # always include hidden state, never include same modality,\n    # other dependencies are random per time and position\n    n = self.note_dim\n    if ar_mask is None:\n        # random binary mask\n        ar_mask = torch.randint(2, (*trim_h.shape[:2],n,n), dtype=torch.bool, device=h.device)\n        # zero diagonal\n        ar_mask &amp;= ~torch.eye(n,n, dtype=torch.bool, device=h.device)\n    # include hidden state\n    ar_mask = torch.cat((ar_mask.new_ones(*ar_mask.shape[:-2],1,n), ar_mask), -2).float()\n\n    to_mask = torch.stack((\n        self.h_proj(trim_h),\n        *(emb[:,1:] for emb in embs)\n    ), -1)\n    # TODO: try without this tanh?\n    mode_hs = (to_mask @ ar_mask).tanh().unbind(-1)\n\n    # final projections to raw distribution parameters\n    inst_params, pitch_params, time_params, vel_params = [\n        proj(h) for proj,h in zip(self.projections, mode_hs)]\n\n    # get likelihood of data for each modality\n    inst_logits = F.log_softmax(inst_params, -1)\n    inst_targets = instruments[:,1:,None] #batch, time, 1\n    inst_log_probs = inst_logits.gather(-1, inst_targets)[...,0]\n\n    pitch_logits = F.log_softmax(pitch_params, -1)\n    pitch_targets = pitches[:,1:,None] #batch, time, 1\n    pitch_log_probs = pitch_logits.gather(-1, pitch_targets)[...,0]\n\n    time_targets = times[:,1:] # batch, time\n    time_result = self.time_dist(time_params, time_targets)\n    time_log_probs = time_result.pop('log_prob')\n\n    vel_targets = velocities[:,1:] # batch, time\n    vel_result = self.vel_dist(vel_params, vel_targets)\n    vel_log_probs = vel_result.pop('log_prob')\n\n    # end prediction\n    # skip the first position for convenience \n    # (so masking is the same for end as for note parts)\n    end_params = self.end_proj(h[:,1:])\n    end_logits = F.log_softmax(end_params, -1)\n    end_log_probs = end_logits.gather(-1, ends[:,1:,None])[...,0]\n\n    r = {\n        'end_log_probs': end_log_probs,\n        'instrument_log_probs': inst_log_probs,\n        'pitch_log_probs': pitch_log_probs,\n        'time_log_probs': time_log_probs,\n        'velocity_log_probs': vel_log_probs,\n        **{'time_'+k:v for k,v in time_result.items()},\n        **{'velocity_'+k:v for k,v in vel_result.items()}\n    }\n    # this just computes some extra diagnostics which are inconvenient to do in the\n    # training script. should be turned off during training for performance.\n    if validation:\n        with torch.no_grad():\n            r['time_acc_30ms'] = (\n                self.time_dist.cdf(time_params, time_targets + 0.03)\n                - torch.where(time_targets - 0.03 &gt;= 0,\n                    self.time_dist.cdf(time_params, time_targets - 0.03),\n                    time_targets.new_zeros([]))\n            )\n    return r\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.Notochord.from_checkpoint","title":"<code>from_checkpoint(path)</code>  <code>classmethod</code>","text":"<p>create a Notochord from a checkpoint file containing  hyperparameters and model weights.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>file path to Notochord model</p> required Source code in <code>src/notochord/model.py</code> <pre><code>@classmethod\ndef from_checkpoint(cls, path):\n    \"\"\"\n    create a Notochord from a checkpoint file containing \n    hyperparameters and model weights.\n\n    Args:\n        path: file path to Notochord model\n    \"\"\"\n    if path==\"notochord-latest.ckpt\":\n        url = 'https://github.com/Intelligent-Instruments-Lab/iil-python-tools/releases/download/notochord-v0.4.0/notochord_lakh_50G_deep.pt'\n    elif path==\"txala-latest.ckpt\":\n        url = 'https://github.com/Intelligent-Instruments-Lab/notochord/releases/download/notochord-v0.5.4/noto-txala-011-0020.ckpt'\n    else:\n        url = None\n\n    if url is not None:\n        d = Notochord.user_data_dir()\n        path = d / path\n        # maybe download\n        if not path.is_file():\n            while True:\n                answer = input(\"Do you want to download a notochord model? (y/n)\")\n                if answer.lower() in [\"y\",\"yes\"]:\n                    download_url(url, path)\n                    print(f'saved to {path}')\n                    break\n                if answer.lower() in [\"n\",\"no\"]:\n                    break\n    # path = \n    checkpoint = torch.load(\n        path, map_location=torch.device('cpu'), weights_only=False)\n    model = cls(**checkpoint['kw']['model'])\n    model.load_state_dict(checkpoint['model_state'], strict=False)\n    model.checkpoint_path = path\n    model.reset()\n    model.eval()\n    return model\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.Notochord.get_note_maps","title":"<code>get_note_maps(note_on_map=None, note_off_map=None, min_polyphony=None, max_polyphony=None)</code>","text":"<p>common logic for v-first sampling</p> Source code in <code>src/notochord/model.py</code> <pre><code>def get_note_maps(self, \n    note_on_map=None, note_off_map=None, \n    min_polyphony=None, max_polyphony=None):\n    \"\"\"common logic for v-first sampling\"\"\"\n    # convert {(i,p):t} to {i:[p]}\n    held_map = self.held_map()\n\n    # get default note_on_map (anything)\n    if note_on_map is None:\n        note_on_map = copy.copy(self._default_note_map)\n    else:\n        note_on_map = copy.deepcopy(note_on_map)\n\n    # note offs can be any from the note_on instruments by default\n    # but users can also supply this themselves\n    if note_off_map is None:\n        note_off_map = {\n            i: held_map[i] \n            for i in note_on_map\n            if i in held_map}\n    else:\n        note_on_map = copy.deepcopy(note_off_map)\n\n    # exclude held notes for note on\n    for i in held_map:\n        if i in note_on_map:\n            note_on_map[i] = set(note_on_map[i]) - held_map[i]\n\n    # exclude non-held notes for note off\n    note_off_map = {\n        i: set(note_off_map[i]) &amp; held_map[i]\n        for i in note_off_map\n        if i in held_map\n    }\n\n    # TODO: \n    # allow breaking polyphony constraint when it results in no options?\n    # may not work to check here as more constraints applied downstream\n    # could track number/degree of constraint violations instead of simply\n    # removing pitches -- later sample from the top stratum only\n\n    max_poly = get_from_scalar_or_dict(max_polyphony, torch.inf)\n    min_poly = get_from_scalar_or_dict(min_polyphony, 0)\n\n    # prevent note on if polyphony exceeded\n    for i in list(note_on_map):\n        if len(held_map.get(i, [])) &gt;= max_poly(i):\n            note_on_map.pop(i)\n\n    # prevent note off if below minimum polyphony\n    for i in list(note_off_map):\n        if len(held_map[i]) &lt;= min_poly(i):\n            note_off_map.pop(i)\n\n    return note_on_map, note_off_map\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.Notochord.get_state","title":"<code>get_state()</code>","text":"<p>return a dict of {str:Tensor} representing the model state</p> Source code in <code>src/notochord/model.py</code> <pre><code>def get_state(self) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"return a dict of {str:Tensor} representing the model state\"\"\"\n    return {n:getattr(self, n).clone() for n in self.cell_state_names()}\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.Notochord.held_map","title":"<code>held_map()</code>","text":"<p>currently held notes as a map from instrument to pitch set</p> Source code in <code>src/notochord/model.py</code> <pre><code>def held_map(self):\n    \"\"\"\n        currently held notes as a map from instrument to pitch set\n    \"\"\"\n    held_map = {}\n    for i,p in self.held_notes:\n        if i not in held_map:\n            held_map[i] = set()\n        held_map[i].add(p)\n    return held_map\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.Notochord.predict","title":"<code>predict(inst, pitch, time, vel, **kw)</code>","text":"<p>DEPRECATED: alias for feed_query</p> Source code in <code>src/notochord/model.py</code> <pre><code>def predict(self, inst, pitch, time, vel, **kw):\n    \"\"\"\n    DEPRECATED: alias for feed_query\n    \"\"\"\n    self.feed(inst, pitch, time, vel)\n    return self.query(**kw)\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.Notochord.prompt","title":"<code>prompt(midi_file, merge=False)</code>","text":"<p>Read a MIDI file and feed events to this Notochord model.</p> <p>When possible, the hidden states will be cached so re-using the same prompt will be fast.</p> <p>Parameters:</p> Name Type Description Default <code>midi_file</code> <p>path of a midi file to read</p> required <p>Returns:     state: hidden state dict of the Notochord encoding the MIDI prompt     channel_inst: dict mapping MIDI channel (0-index) to Notochord instrument (1-256)</p> Source code in <code>src/notochord/model.py</code> <pre><code>def prompt(self, midi_file, merge=False):\n    \"\"\"Read a MIDI file and feed events to this Notochord model.\n\n    When possible, the hidden states will be cached so re-using the same prompt will be fast.\n\n    Args:\n        midi_file: path of a midi file to read\n    Returns:\n        state: hidden state dict of the Notochord encoding the MIDI prompt\n        channel_inst: dict mapping MIDI channel (0-index) to Notochord instrument (1-256)\n    \"\"\"\n    return prompt(\n        self, Path(midi_file), \n        merge=merge, state_hash=hash_states(self.get_state()))\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.Notochord.query","title":"<code>query(next_inst=None, next_pitch=None, next_time=None, next_vel=None, allow_end=False, include_inst=None, exclude_inst=None, allow_anon=True, instrument_temp=None, include_pitch=None, exclude_pitch=None, include_drum=None, truncate_quantile_pitch=None, pitch_temp=None, index_pitch=None, min_time=None, max_time=None, truncate_quantile_time=None, rhythm_temp=None, timing_temp=None, min_vel=None, max_vel=None, velocity_temp=None, pitch_topk=None, sweep_time=False, handle=None, return_params=False)</code>","text":"<p>Sample a prediction for the next MIDI event.</p> <p>various constraints on the the next event can be requested.</p> <p>Parameters:</p> Name Type Description Default <code>next_inst</code> <code>int</code> <p>fix a particular instrument for the predicted event. sampled values will always condition on fixed values, so passing <code>next_inst=1</code>, for example, will make the event appropriate for the Grand Piano (instrument 1) to play.</p> <code>None</code> <code>next_pitch</code> <code>int</code> <p>fix a particular MIDI number for the predicted event. sampled values will always condition on fixed values, so passing <code>next_pitch=60</code>, for example, will make the event a middle C (for melodic instruments) or High Bongo (for drums)</p> <code>None</code> <code>next_time</code> <code>float</code> <p>fix a particular delta time for the predicted event. sampled values will always condition on fixed values, so passing <code>next_time=0</code>, for example, will make the event concurrent with the previous event.</p> <code>None</code> <code>next_vel</code> <code>int</code> <p>fix a particular velocity for the predicted event. sampled values will always condition on fixed values, so passing <code>next_inst=0</code>, for example, will ensure the event is a noteOff.</p> <code>None</code> <code>include_inst</code> <code>List[int]</code> <p>instrument id(s) to include in sampling. (if not None, all others will be excluded)</p> <code>None</code> <code>exclude_inst</code> <code>List[int]</code> <p>instrument id(s) to exclude from sampling.</p> <code>None</code> <code>allow_anon</code> <code>bool</code> <p>bool. if False, zero probability of anon instruments</p> <code>True</code> <code>include_pitch</code> <code>List[int]</code> <p>pitch(es) to include in sampling. (if not None, all others will be excluded)</p> <code>None</code> <code>exclude_pitch</code> <code>List[int]</code> <p>pitch(es) to exclude from sampling.</p> <code>None</code> <code>include_drum</code> <code>List[int]</code> <p>like <code>include_pitch</code>, but only in effect when  instrument is a drumkit</p> <code>None</code> <code>min_time</code> <code>float</code> <p>if not None, truncate the time distribution below</p> <code>None</code> <code>max_time</code> <code>float</code> <p>if not None, truncate the time distribution above</p> <code>None</code> <code>min_vel</code> <code>int</code> <p>if not None, truncate the velocity distribution below e.g., <code>min_vel=1</code> prevents NoteOff events</p> <code>None</code> <code>max_vel</code> <code>int</code> <p>if not None, truncate the velocity distribution above</p> <code>None</code> <code>allow_end</code> <code>bool</code> <p>if False, zero probability of sampling the end marker</p> <code>False</code> <code>instrument_temp</code> <code>float</code> <p>if not None, apply top_p sampling to instrument. 0 is deterministic, 1 is 'natural' according to the model</p> <code>None</code> <code>pitch_temp</code> <code>float</code> <p>if not None, apply top_p sampling to pitch. 0 is deterministic, 1 is 'natural' according to the model</p> <code>None</code> <code>truncate_quantile_pitch</code> <code>Tuple[float, float]</code> <p>applied after include_pitch, exclude_pitch truncate the remaining pitch distribution by quantile. e.g. truncate_quantile_pitch=(0.25, 0.75) excludes the lowest and highest 25% of pitches</p> <code>None</code> <code>index_pitch</code> <code>int</code> <p>if not None, deterministically take the nth most likely pitch instead of sampling.</p> <code>None</code> <code>timing_temp</code> <code>float</code> <p>if not None, apply temperature sampling to the time component. this affects fine timing; 0 is deterministic and  precise, 1 is 'natural' according to the model.</p> <code>None</code> <code>rhythm_temp</code> <code>float</code> <p>if not None, apply top_p sampling to the weighting of mixture components. this affects coarse rhythmic patterns; 0 is deterministic, 1 is 'natural' according to the model.</p> <code>None</code> <code>truncate_quantile_time</code> <code>Tuple[float, float]</code> <p>applied after min_time, max_time truncate the remaining delta time distribution by quantile. e.g. truncate_quantile_time=(0.25, 0.75) excludes the shortest 25% and longest 25% of interevent times.</p> <code>None</code> <code>velocity_temp</code> <code>float</code> <p>if not None, apply temperature sampling to the  velocity component.</p> <code>None</code> <code>pitch_topk</code> <code>int</code> <p>Optional[int]. if not None, instead of sampling pitch,  stack the top k most likely pitches along the batch dimension</p> <code>None</code> <code>sweep_time</code> <code>bool</code> <p>if True, instead of sampling time, choose a diverse set</p> <code>False</code> <code>handle</code> <code>str</code> <p>metadata to be included in the returned dict, if not None</p> <code>None</code> <code>return_params</code> <code>bool</code> <p>if True, return tensors of distribution parameters under the keys <code>inst_params</code>, <code>pitch_params</code>, <code>time_params</code>, and <code>vel_params</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>'inst': int. id of predicted instrument. 1-128 are General MIDI standard melodic instruments 129-256 are drumkits for MIDI programs 1-128 257-288 are 'anonymous' melodic instruments 289-320 are 'anonymous' drumkits</p> <code>dict</code> <p>'pitch': int. predicted MIDI number of next note, 0-128.</p> <code>dict</code> <p>'time': float. predicted time to next note in seconds.</p> <code>dict</code> <p>'vel': float. unquantized predicted velocity of next note. 0-127; hard 0 indicates a note-off event.</p> <code>dict</code> <p>'end': int. value of 1 indicates the current event (the one  passed as arguments to <code>predict</code>) was the last event, and the predicted event should not be played. if <code>allow end</code> is false,  this will always be 0.</p> <code>dict</code> <p>'step': int. number of steps since calling <code>reset</code>.</p> <code>dict</code> <p>'*_params': tensor. distribution parameters for visualization and debugging purposes. present if <code>return_params</code> is True.</p> <code>instrument</code>, <code>pitch</code>, <code>time</code>, <code>velocity</code> may return lists, <p>when using <code>sweep_time</code> or <code>pitch_topk</code>. that part of the API  is very experimental and likely to break.</p> Source code in <code>src/notochord/model.py</code> <pre><code>def query(self,\n        next_inst:int=None, next_pitch:int=None, \n        next_time:float=None, next_vel:int=None,\n\n        allow_end:bool=False,\n\n        include_inst:List[int]=None, exclude_inst:List[int]=None,\n        allow_anon:bool=True, \n        instrument_temp:float=None, \n\n        include_pitch:List[int]=None, exclude_pitch:List[int]=None,\n        include_drum:List[int]=None,\n        truncate_quantile_pitch:Tuple[float,float]=None,\n        pitch_temp:float=None, \n        index_pitch:int=None,\n\n        min_time:float=None, max_time:float=None,\n        truncate_quantile_time:Tuple[float, float]=None,\n        rhythm_temp:float=None, timing_temp:float=None,\n\n        min_vel:int=None, max_vel:int=None,\n        velocity_temp:float=None,\n\n        pitch_topk:int=None, sweep_time:bool=False, \n\n        handle:str=None, return_params:bool=False\n        ) -&gt; dict:\n    \"\"\"\n    Sample a prediction for the next MIDI event.\n\n    various constraints on the the next event can be requested.\n\n    Args:\n        # hard constraints\n\n        next_inst: fix a particular instrument for the predicted event.\n            sampled values will always condition on fixed values, so passing\n            `next_inst=1`, for example, will make the event appropriate\n            for the Grand Piano (instrument 1) to play.\n        next_pitch: fix a particular MIDI number for the predicted event.\n            sampled values will always condition on fixed values, so passing\n            `next_pitch=60`, for example, will make the event a middle C\n            (for melodic instruments) or High Bongo (for drums)\n        next_time: fix a particular delta time for the predicted event.\n            sampled values will always condition on fixed values, so passing\n            `next_time=0`, for example, will make the event concurrent with\n            the previous event.\n        next_vel: fix a particular velocity for the predicted event.\n            sampled values will always condition on fixed values, so passing\n            `next_inst=0`, for example, will ensure the event is a noteOff.\n\n        # partial constraints\n\n        include_inst: instrument id(s) to include in sampling.\n            (if not None, all others will be excluded)\n        exclude_inst: instrument id(s) to exclude from sampling.\n        allow_anon: bool. if False, zero probability of anon instruments\n\n        include_pitch: pitch(es) to include in sampling.\n            (if not None, all others will be excluded)\n        exclude_pitch: pitch(es) to exclude from sampling.\n        include_drum: like `include_pitch`, but only in effect when \n            instrument is a drumkit\n\n        min_time: if not None, truncate the time distribution below\n        max_time: if not None, truncate the time distribution above\n\n        min_vel: if not None, truncate the velocity distribution below\n            e.g., `min_vel=1` prevents NoteOff events\n        max_vel: if not None, truncate the velocity distribution above\n\n        allow_end: if False, zero probability of sampling the end marker\n\n        # sampling strategies\n\n        instrument_temp: if not None, apply top_p sampling to instrument. 0 is\n            deterministic, 1 is 'natural' according to the model\n\n        pitch_temp: if not None, apply top_p sampling to pitch. 0 is\n            deterministic, 1 is 'natural' according to the model\n        truncate_quantile_pitch: applied after include_pitch, exclude_pitch\n            truncate the remaining pitch distribution by quantile.\n            e.g. truncate_quantile_pitch=(0.25, 0.75)\n            excludes the lowest and highest 25% of pitches\n        index_pitch: if not None, deterministically take the\n            nth most likely pitch instead of sampling.\n\n        timing_temp: if not None, apply temperature sampling to the time\n            component. this affects fine timing; 0 is deterministic and \n            precise, 1 is 'natural' according to the model.\n        rhythm_temp: if not None, apply top_p sampling to the weighting\n            of mixture components. this affects coarse rhythmic patterns;\n            0 is deterministic, 1 is 'natural' according to the model.\n        truncate_quantile_time: applied after min_time, max_time\n            truncate the remaining delta time distribution by quantile.\n            e.g. truncate_quantile_time=(0.25, 0.75)\n            excludes the shortest 25% and longest 25% of interevent times.\n\n        velocity_temp: if not None, apply temperature sampling to the \n            velocity component.\n\n        # multiple predictions\n\n        pitch_topk: Optional[int]. if not None, instead of sampling pitch, \n            stack the top k most likely pitches along the batch dimension\n        sweep_time: if True, instead of sampling time, choose a diverse set\n        of times and stack along the batch dimension\n\n        # other\n\n        handle: metadata to be included in the returned dict, if not None\n        return_params: if True, return tensors of distribution parameters\n            under the keys `inst_params`, `pitch_params`, `time_params`,\n            and `vel_params`.\n\n    Returns:\n        'inst': int. id of predicted instrument.\n            1-128 are General MIDI standard melodic instruments\n            129-256 are drumkits for MIDI programs 1-128\n            257-288 are 'anonymous' melodic instruments\n            289-320 are 'anonymous' drumkits\n        'pitch': int. predicted MIDI number of next note, 0-128.\n        'time': float. predicted time to next note in seconds.\n        'vel': float. unquantized predicted velocity of next note.\n            0-127; hard 0 indicates a note-off event.\n        'end': int. value of 1 indicates the *current* event (the one \n            passed as arguments to `predict`) was the last event, and the\n            predicted event should *not* be played. if `allow end` is false, \n            this will always be 0.\n        'step': int. number of steps since calling `reset`.\n        '*_params': tensor. distribution parameters for visualization\n            and debugging purposes. present if `return_params` is True.\n\n    NOTE: `instrument`, `pitch`, `time`, `velocity` may return lists,\n        when using `sweep_time` or `pitch_topk`. that part of the API \n        is very experimental and likely to break.\n    \"\"\"\n     # validate options:\n    if (index_pitch is not None) and (pitch_temp is not None):\n        print(\"warning: `index pitch` overrides `pitch_temp`\")\n\n    inst_intervention = any(p is not None for p in (\n        instrument_temp, include_inst, exclude_inst))\n\n    pitch_intervention = (pitch_topk or any(p is not None for p in (\n        pitch_temp, include_pitch, exclude_pitch, include_drum)))\n\n    time_intervention = any(p is not None for p in (\n        min_time, max_time, rhythm_temp, timing_temp))\n\n    vel_intervention = any(p is not None for p in (\n        min_vel, max_vel, velocity_temp))\n\n    exclude_inst = arg_to_set(exclude_inst)\n    if not allow_anon:\n        exclude_inst |= set(range(257, 321))\n    constrain_inst = list((\n        set(range(self.instrument_domain)) - {self.instrument_start_token}\n        if include_inst is None \n        else arg_to_set(include_inst)\n    ) - exclude_inst)\n    if len(constrain_inst)==0:\n        raise ValueError(\"\"\"\n        every instrument has been excluded. check values of \n        `include_inst` and `exclude_inst`\n        \"\"\")\n    # elif len(constrain_inst)==1:\n    #     print(\"\"\"\n    #     warning: recommended to use `next_inst`, not \n    #     `include_inst` to allow only one specific instrument\n    #     \"\"\")\n\n    constrain_pitch = list((\n        set(range(self.pitch_domain)) - {self.pitch_start_token}\n        if include_pitch is None \n        else arg_to_set(include_pitch)\n    ) - arg_to_set(exclude_pitch))\n    if len(constrain_pitch)==0:\n        raise ValueError(\"\"\"\n        every pitch has been excluded. check values of \n        `include_pitch` and `exclude_pitch`\n        \"\"\")\n    elif len(constrain_pitch)==1:\n        print(\"\"\"\n        warning: recommended to use `next_pitch`, not \n        `include_pitch` to allow only one specific pitch\n        \"\"\")\n\n    # TODO: this got really complicated to support include_drum...\n    # really want to edit the whole joint distribution of pitch,inst in \n    # cases where certain pitches or drums need to be excluded...\n    # would that be practical? if there are ~40000 inst x pitch combos?\n    # would need to run the instrument head for a whole batch of all\n    # allowable pitches or vice-versa...\n    def sample_instrument(x):\n        # if include_drum is supplied, make sure to exclude drum instruments\n        # when no pitch is in the allowed drums\n        if include_drum is not None:\n            pit = predicted_by_name('pitch')\n            pits = [pit] if pit is not None else constrain_pitch\n            if pits is not None and all(pit not in include_drum for pit in pits):\n                nonlocal constrain_inst\n                if constrain_inst is None:\n                    constrain_inst = range(1,self.instrument_domain)\n                constrain_inst = [\n                    i for i in constrain_inst if not self.is_drum(i)]\n\n        # if constrain_inst is not None:\n        #     preserve_x = x[...,constrain_inst]\n        #     x = torch.full_like(x, -torch.inf)\n        #     x[...,constrain_inst] = preserve_x\n        # probs = x.softmax(-1)\n        # if instrument_temp is not None:\n        #     probs = reweight_top_p(probs, instrument_temp)\n        # return D.Categorical(probs).sample()\n\n        return categorical_sample(x, \n            whitelist=constrain_inst,\n            top_p=instrument_temp)\n\n    def sample_pitch(x):\n        # conditional constraint\n        if include_drum is not None:\n            # if this event is / must be a drum,\n            # use include_drum instead of constrain_inst\n            inst = predicted_by_name('instrument')\n            insts = [inst] if inst is not None else constrain_inst\n            if insts is not None and all(self.is_drum(i) for i in insts):\n                nonlocal constrain_pitch\n                constrain_pitch = include_drum\n\n        if pitch_topk is not None:\n            raise NotImplementedError\n\n        return categorical_sample(x,\n            whitelist=constrain_pitch, \n            index=index_pitch,\n            top_p=pitch_temp,\n            truncate_quantile=truncate_quantile_pitch\n            )\n        # if constrain_pitch is not None:\n        #     preserve_x = x[...,constrain_pitch]\n        #     x = torch.full_like(x, -torch.inf)\n        #     x[...,constrain_pitch] = preserve_x\n        # # x is modified logits\n\n        # if index_pitch is not None:\n        #     return x.argsort(-1, True)[...,index_pitch]\n        # elif pitch_topk is not None:\n        #     return x.argsort(-1, True)[...,:pitch_topk].transpose(0,-1)\n\n        # probs = x.softmax(-1)\n        # if pitch_temp is not None:\n        #     probs = reweight_top_p(probs, pitch_temp)\n\n        # if steer_pitch is not None:\n        #     return steer_categorical(probs, steer_pitch)\n        # else:\n        #     return D.Categorical(probs).sample()\n\n    def sample_time(x):\n        # TODO: respect trunc_time when sweep_time is True\n        if sweep_time:\n            if min_time is not None or max_time is not None:\n                raise NotImplementedError(\"\"\"\n                min_time/max_time with sweep_time needs implementation\n                \"\"\")\n            assert x.shape[0]==1, \"batch size should be 1 here\"\n            log_pi, loc, s = self.time_dist.get_params(x)\n            idx = log_pi.squeeze().argsort()[:9]\n            loc = loc.squeeze()[idx].sort().values[...,None] \n            # multiple times in batch dim\n            # print(loc.shape)\n            return loc\n\n        trunc = (\n            -torch.inf if min_time is None else min_time,\n            torch.inf if max_time is None else max_time)\n\n        return self.time_dist.sample(x, \n            truncate=trunc,\n            component_temp=timing_temp, \n            weight_top_p=rhythm_temp,\n            truncate_quantile=truncate_quantile_time\n            )\n\n    def sample_velocity(x):\n        trunc = (\n            -torch.inf if min_vel is None else min_vel,\n            torch.inf if max_vel is None else max_vel)\n        return self.vel_dist.sample(\n            x, component_temp=velocity_temp, truncate=trunc,\n            # truncate_quantile=truncate_quantile_vel\n            )\n\n    with torch.inference_mode():\n        if self.h_query is None:\n            self.h_query = self.h_proj(self.h)\n\n        modalities = list(zip(\n            self.projections,\n            (sample_instrument, sample_pitch, sample_time, sample_velocity),\n            self.embeddings,\n            ))\n\n        context = [self.h_query] # embedded outputs for autoregressive prediction\n        predicted = [] # raw outputs\n        params = [] # distribution parameters for visualization\n\n        fix = [\n            None if item is None else torch.tensor([[item]], dtype=dtype)\n            for item, dtype in zip(\n                [next_inst, next_pitch, next_time, next_vel],\n                [torch.long, torch.long, torch.float, torch.float])]\n\n        # if any modalities are determined, embed them\n        # sort constrained modalities before unconstrained\n        # TODO: option to skip modalities\n        det_idx, cons_idx, uncons_idx = [], [], []\n        for i,(item, embed) in enumerate(zip(fix, self.embeddings)):\n            if item is None:\n                if (\n                    i==0 and inst_intervention or\n                    i==1 and pitch_intervention or\n                    i==2 and time_intervention or\n                    i==3 and vel_intervention):\n                    cons_idx.append(i)\n                else:\n                    uncons_idx.append(i)\n            else:\n                det_idx.append(i)\n                context.append(embed(item))\n                predicted.append(item)\n                params.append(None)\n        undet_idx = cons_idx + uncons_idx\n        perm = det_idx + undet_idx # permutation from the canonical order\n        iperm = argsort(perm) # inverse permutation back to canonical order\n\n        mode_names = ['instrument', 'pitch', 'time', 'velocity']\n        name_to_idx = {k:v for k,v in zip(mode_names, iperm)}\n        def predicted_by_name(name):\n            idx = name_to_idx[name]\n            if len(predicted) &gt; idx:\n                return predicted[idx]\n            return None\n        # print('sampling order:', [mode_names[i] for i in perm])\n\n        # for each undetermined modality, \n        # sample a new value conditioned on already determined ones\n\n        running_ctx = sum(context)\n        # print(running_ctx)\n        # perm_h_tgt = [h_tgt[i] for i in perm]\n        while len(undet_idx):\n            # print(running_ctx.norm())\n            i = undet_idx.pop(0) # index of modality to determine\n            # j = len(det_idx) # number already determined\n            project, sample, embed = modalities[i]\n            # determine value for the next modality\n            hidden = running_ctx.tanh()\n            params.append(project(hidden))\n            pred = sample(params[-1])\n            predicted.append(pred)\n            # prepare for next iteration\n            if len(undet_idx):\n                # context.append(embed(pred))\n                running_ctx += embed(pred)\n            det_idx.append(i)\n\n        pred_inst = predicted_by_name('instrument')\n        pred_pitch = predicted_by_name('pitch')\n        pred_time = predicted_by_name('time')\n        pred_vel = predicted_by_name('velocity')\n\n        if allow_end:\n            end_params = self.end_proj(self.h)\n            # print(end_params)\n            end = D.Categorical(logits=end_params).sample()\n        else:\n            end = torch.zeros(self.h.shape[:-1])\n\n        if sweep_time or pitch_topk:\n            # return lists of predictions\n            pred_inst = [x.item() for x in pred_inst]\n            pred_pitch = [x.item() for x in pred_pitch]\n            pred_time = [x.item() for x in pred_time]\n            pred_vel = [x.item() for x in pred_vel]\n            end = [x.item() for x in end]\n            # print(pred_time, pred_pitch, pred_vel)\n        else:\n            # return single predictions\n            pred_inst = pred_inst.item()\n            pred_pitch = pred_pitch.item()\n            pred_time = pred_time.item()\n            pred_vel = pred_vel.item()\n            end = end.item()\n\n        r = {\n            'inst': pred_inst,\n            'pitch': pred_pitch, \n            'time': pred_time,\n            'vel': pred_vel,\n\n            'end': end,\n            'step': self.step,\n        }\n\n        if handle is not None:\n            r['handle'] = handle\n\n        if return_params:\n            r |= {\n                'inst_params': params[iperm[0]],\n                'pitch_params': params[iperm[1]],\n                'time_params': params[iperm[2]],\n                'vel_params': params[iperm[3]]\n            }\n\n        return r\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.Notochord.query_feed","title":"<code>query_feed(*a, **kw)</code>","text":"<p>query for the next predicted event and immediately feed it to the model, also returning the predicted event.</p> Source code in <code>src/notochord/model.py</code> <pre><code>def query_feed(self, *a, **kw):\n    \"\"\"\n    query for the next predicted event and immediately feed it to the model,\n    also returning the predicted event.\n    \"\"\"\n    r = self.query(*a, **kw)\n    self.feed(r['inst'], r['pitch'], r['time'], r['vel'])\n    return r\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.Notochord.query_itpv_onsets","title":"<code>query_itpv_onsets(min_time=None, max_time=None, include_inst=None, include_pitch=None, truncate_quantile_time=None, truncate_quantile_pitch=None, rhythm_temp=None, timing_temp=None, min_vel=None, max_vel=None)</code>","text":"<p>for onset-only_models</p> Source code in <code>src/notochord/model.py</code> <pre><code>def query_itpv_onsets(self,\n    min_time=None, max_time=None, \n    include_inst=None,\n    include_pitch=None,\n    truncate_quantile_time=None,\n    truncate_quantile_pitch=None,\n    rhythm_temp=None, timing_temp=None,\n    min_vel=None, max_vel=None\n    ):\n    \"\"\"\n    for onset-only_models\n    \"\"\"\n    q = Query(\n        'inst',\n        whitelist=include_inst,\n        then=Query(\n            'time',\n            truncate=(min_time or -torch.inf, max_time or torch.inf), \n            truncate_quantile=truncate_quantile_time,\n            weight_top_p=rhythm_temp, component_temp=timing_temp,\n            then=Query(\n                'pitch',\n                whitelist=include_pitch,\n                truncate_quantile=truncate_quantile_pitch,\n                then=Query(\n                    'vel',\n                    truncate=(min_vel or 0.5, max_vel or torch.inf),\n                )\n            )\n        )\n    )\n    return self.deep_query(q)\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.Notochord.query_tipv_onsets","title":"<code>query_tipv_onsets(min_time=None, max_time=None, include_inst=None, include_pitch=None, truncate_quantile_time=None, truncate_quantile_pitch=None, rhythm_temp=None, timing_temp=None, min_vel=None, max_vel=None)</code>","text":"<p>for onset-only_models</p> Source code in <code>src/notochord/model.py</code> <pre><code>def query_tipv_onsets(self,\n    min_time=None, max_time=None, \n    include_inst=None,\n    include_pitch=None,\n    truncate_quantile_time=None,\n    truncate_quantile_pitch=None,\n    rhythm_temp=None, timing_temp=None,\n    min_vel=None, max_vel=None\n    ):\n    \"\"\"\n    for onset-only_models\n    \"\"\"\n    q = Query(\n        'time',\n        truncate=(min_time or -torch.inf, max_time or torch.inf), \n        truncate_quantile=truncate_quantile_time,\n        weight_top_p=rhythm_temp, component_temp=timing_temp,\n        then=Query(\n            'inst',\n            whitelist=include_inst,\n            then=Query(\n                'pitch',\n                whitelist=include_pitch,\n                truncate_quantile=truncate_quantile_pitch,\n                then=Query(\n                    'vel',\n                    truncate=(min_vel or 0.5, max_vel or torch.inf),\n                )\n            )\n        )\n    )\n    return self.deep_query(q)\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.Notochord.query_vipt","title":"<code>query_vipt(note_on_map=None, note_off_map=None, min_time=None, max_time=None, min_vel=None, max_vel=None, min_polyphony=None, max_polyphony=None, min_duration=None, max_duration=None, rhythm_temp=None, timing_temp=None, truncate_quantile_time=None, truncate_quantile_pitch=None, truncate_quantile_vel=None, steer_density=None, inst_weights=None, no_steer=None)</code>","text":"<p>Query in a fixed velocity-&gt;instrument-&gt;pitch-&gt;time order, sampling all modalities. Because velocity is sampled first, this query method can  automatically prevent double noteOn or NoteOff. It's also possible to make some more detailed constraints per-instrument compared to <code>query</code>, including note duration constraints which can eliminate stuck notes.</p> <p>query_vtip is similar, but makes different compromises in applying  constraints. VTIP is likely to be better when setting min_time &gt; 0  or otherwise heavily constraing time delta, while VIPT may be better in other cases.</p> <p>Parameters:</p> Name Type Description Default <code>note_on_map</code> <code>Dict[int, List[int]] | None</code> <p>possible note-ons as {instrument: [pitch]}  defaults to allowing any note. Notes already playing on a given instrument are always excluded.</p> <code>None</code> <code>note_off_map</code> <code>Dict[int, List[int]] | None</code> <p>possible note-offs as {instrument: [pitch]} defaults to using only the instruments in note_on_map.  Notes not already playing on a given instrument are  automatically excluded.</p> <code>None</code> <code>min_time</code> <code>Number | None</code> <p>global minimum interevent time (default 0)</p> <code>None</code> <code>max_time</code> <code>Number | None</code> <p>global maximum interevent time (default no limit)</p> <code>None</code> <code>min_vel</code> <code>Number | None</code> <p>global minimum velocity for NoteOn events (default 1)</p> <code>None</code> <code>max_vel</code> <code>Number | None</code> <p>global maximum velocity for NoteOn events (default 127)</p> <code>None</code> <code>min_polyphony</code> <code>Dict[int, int] | int | None</code> <p>minimum number of concurrent notes per instrument. (default 0). Can be a dict mapping instrument to value, or a single value for all instruments. When an instrument has &lt;= min polyphony, exclude NoteOffs</p> <code>None</code> <code>max_polyphony</code> <code>Dict[int, int] | int | None</code> <p>minimum number of concurrent notes per instrument. (default no limit). Can be a dict mapping instrument to value, or a single value for all instruments. When an instrument has &gt;= max polyphony, exclude NoteOns.</p> <code>None</code> <code>min_duration</code> <code>Dict[int, Number] | Number | None</code> <p>minimum note length per instrument (default 0). Can  be a dict mapping instrument to value, or a single value for  all instruments.</p> <code>None</code> <code>max_duration</code> <code>Dict[int, Number] | Number | None</code> <p>maximum note length per instrument (default 0). Can  be a dict mapping instrument to value, or a single value for  all instruments.</p> <code>None</code> <code>rhythm_temp</code> <code>float</code> <p>if not None, apply top_p sampling to the weighting of mixture components. this affects coarse rhythmic patterns; 0 is deterministic, 1 is 'natural' according to the model.</p> <code>None</code> <code>timing_temp</code> <code>float</code> <p>if not None, apply temperature sampling to the time component. this affects fine timing; 0 is deterministic and  precise, 1 is 'natural' according to the model.</p> <code>None</code> <code>truncate_quantile_time</code> <code>Tuple[float, float] | None</code> <p>applied after min_time, max_time truncate the remaining delta time distribution by quantile. e.g. truncate_quantile_time=(0.25, 0.75) excludes the shortest 25% and longest 25% of interevent times.</p> <code>None</code> <code>truncate_quantile_pitch</code> <code>Tuple[float, float] | None</code> <p>truncate the pitch distribution by  quantile. e.g. truncate_quantile_pitch=(0.5, 1) always samples above the median predicted pitch. Ignored for drums.</p> <code>None</code> <code>truncate_quantile_vel</code> <code>Tuple[float, float] | None</code> <p>truncate the velocity distribution by  quantile. e.g. truncate_quantile_vel=(0, 0.5) always samples below the median predicted velocity. Affects only NoteOn.</p> <code>None</code> <code>steer_density</code> <code>float</code> <p>adjust relative weight of NoteOn and NoteOff. values above 0.5 favor NoteOn, values below 0.5 favor NoteOff.</p> <code>None</code> <code>inst_weights</code> <code>Dict[int, Number]</code> <p>multiplicatively adjust instrument probabilities.  Any instrument not included has a weight of 1. 0 would exclude an instrument completely (but better to do so via note_on_map)</p> <code>None</code> <code>no_steer</code> <code>List[int]</code> <p>collection of instruments to exclude from effect of  truncate_quantile_pitch and truncate_quantile_time.</p> <code>None</code> Source code in <code>src/notochord/model.py</code> <pre><code>def query_vipt(self,\n    note_on_map:Dict[int,List[int]]|None=None, \n    note_off_map:Dict[int,List[int]]|None=None,\n    min_time:Number|None=None, max_time:Number|None=None,\n    min_vel:Number|None=None, max_vel:Number|None=None,\n    min_polyphony:Dict[int,int]|int|None=None, \n    max_polyphony:Dict[int,int]|int|None=None,\n    min_duration:Dict[int,Number]|Number|None=None, \n    max_duration:Dict[int,Number]|Number|None=None, \n    rhythm_temp:float=None, timing_temp:float=None,\n    truncate_quantile_time:Tuple[float,float]|None=None,\n    truncate_quantile_pitch:Tuple[float,float]|None=None,\n    truncate_quantile_vel:Tuple[float,float]|None=None,\n    steer_density:float=None,\n    inst_weights:Dict[int,Number]=None,\n    no_steer:List[int]=None,\n    ):\n    \"\"\"\n    Query in a fixed velocity-&gt;instrument-&gt;pitch-&gt;time order, sampling all\n    modalities. Because velocity is sampled first, this query method can \n    automatically prevent double noteOn or NoteOff. It's also possible to\n    make some more detailed constraints per-instrument compared to `query`,\n    including note duration constraints which can eliminate stuck notes.\n\n    query_vtip is similar, but makes different compromises in applying \n    constraints. VTIP is likely to be better when setting min_time &gt; 0 \n    or otherwise heavily constraing time delta, while VIPT may be better\n    in other cases.\n\n    Args:\n        note_on_map: possible note-ons as {instrument: [pitch]} \n            defaults to allowing any note. Notes already playing on a given\n            instrument are always excluded.\n        note_off_map: possible note-offs as {instrument: [pitch]}\n            defaults to using only the instruments in note_on_map. \n            Notes not already playing on a given instrument are \n            automatically excluded.\n        min_time: global minimum interevent time (default 0)\n        max_time: global maximum interevent time (default no limit)\n        min_vel: global minimum velocity for NoteOn events (default 1)\n        max_vel: global maximum velocity for NoteOn events (default 127)\n        min_polyphony: minimum number of concurrent notes per instrument.\n            (default 0). Can be a dict mapping instrument to value,\n            or a single value for all instruments.\n            When an instrument has &lt;= min polyphony, exclude NoteOffs\n        max_polyphony: minimum number of concurrent notes per instrument.\n            (default no limit). Can be a dict mapping instrument to value,\n            or a single value for all instruments.\n            When an instrument has &gt;= max polyphony, exclude NoteOns.\n        min_duration: minimum note length per instrument (default 0). Can   \n            be a dict mapping instrument to value, or a single value for \n            all instruments.\n        max_duration: maximum note length per instrument (default 0). Can   \n            be a dict mapping instrument to value, or a single value for \n            all instruments.\n        rhythm_temp: if not None, apply top_p sampling to the weighting\n            of mixture components. this affects coarse rhythmic patterns;\n            0 is deterministic, 1 is 'natural' according to the model.\n        timing_temp: if not None, apply temperature sampling to the time\n            component. this affects fine timing; 0 is deterministic and \n            precise, 1 is 'natural' according to the model.\n        truncate_quantile_time: applied after min_time, max_time\n            truncate the remaining delta time distribution by quantile.\n            e.g. truncate_quantile_time=(0.25, 0.75)\n            excludes the shortest 25% and longest 25% of interevent times.\n        truncate_quantile_pitch: truncate the pitch distribution by \n            quantile. e.g. truncate_quantile_pitch=(0.5, 1) always samples\n            above the median predicted pitch. Ignored for drums.\n        truncate_quantile_vel: truncate the velocity distribution by \n            quantile. e.g. truncate_quantile_vel=(0, 0.5) always\n            samples below the median predicted velocity. Affects only NoteOn.\n        steer_density: adjust relative weight of NoteOn and NoteOff.\n            values above 0.5 favor NoteOn, values below 0.5 favor NoteOff.\n        inst_weights: multiplicatively adjust instrument probabilities. \n            Any instrument not included has a weight of 1. 0 would exclude\n            an instrument completely (but better to do so via note_on_map)\n        no_steer: collection of instruments to exclude from effect of \n            truncate_quantile_pitch and truncate_quantile_time.\n    \"\"\"\n    eps = 1e-5\n    min_time = min_time or 0\n    max_time = max_time or torch.inf\n\n    inst_weights = inst_weights or {}\n    no_steer = no_steer or set()\n\n    note_on_map, note_off_map = self.get_note_maps(\n        note_on_map, note_off_map, min_polyphony, max_polyphony\n    )\n\n    max_dur = get_from_scalar_or_dict(max_duration, torch.inf)\n    min_dur = get_from_scalar_or_dict(min_duration, 0)\n\n    # duration does not constrain the soonest noteOn;\n    # the soonest possible noteOff is the next note which would end with \n    # minimal duration (but no sooner than the global min_time)\n    # compute that soonest noteOff time for each possible noteOff:\n    soonest_off = {\n        (i,p):max(min_time, min_dur(i) - self.held_notes[(i,p)]) \n        for i,ps in note_off_map.items()\n        for p in ps}\n    # print(f'{soonest_off=}')\n\n    # in case where only note off is allowed (likely due to max_polyphony)\n    # min_duration and max_time can be unsatisfiable\n    # break the max_time constraint in that case\n    no_on = all(len(ps)==0 for ps in note_on_map.values())\n    if no_on:\n        soonest_off_any = min(soonest_off.values(), default=0)\n        if soonest_off_any &gt; max_time:\n            max_time = soonest_off_any + eps\n            print(f'breaking max_time constraint -&gt; {max_time}s')\n\n    # latest possible event is minimum max remaining duration over all held notes (i.e. the soonest noteOff ending a max-duration note)\n    # or the global max interevent time, if shorter\n    latest_event = max_time\n    for (i,p),t in self.held_notes.items():\n        latest_event = min(latest_event, max_dur(i) - t)\n    # slip to accomodate global constraint\n    latest_event = max(min_time, latest_event)\n\n    # if latest_event is &lt;= min_time, probably means one of two things:\n    # 1. some notes are already over time and should be prioritized to end\n    # we don't want noteoffs which would prevent ending a different note on time -- except in the case where the soonest noteoff is already late according to global min_time; any such noteOff is valid\n    # since both latest_event and soonest_off are clipped to min_time --\n    # we can exclude noteOffs when soonest_off &gt; latest_event,\n    # but allow soonest_off==latest_event\n    # 2. polyphony+duration contraints contradict max_time\n    # i.e. solo monophonic instrument has min_duration = 5, \n    # but max_time is 3 -- nothing to do after 3 seconds\n    # ought to break the max_time constraint in this case;\n    # can set max_time = max(max_time, min(remaining min duration of held notes))\n\n    # remove impossible note offs\n    # (i.e. soonest possible note-off is after the latest possible event)\n    for i,ps in list(note_off_map.items()):\n        for p in list(ps):\n            if soonest_off[(i,p)] &gt; latest_event:\n                ps.remove(p)\n        if not len(ps):\n            note_off_map.pop(i)\n            continue\n\n    no_off = all(len(ps)==0 for ps in note_off_map.values())\n    # print(f'{no_on=} {no_off=}')\n\n    if no_on and no_off:\n        # if len(soonest_off):\n        #     i_off,p_off = min(soonest_off, key=soonest_off.__getitem__)\n        #     note_off_map = {i_off:[p_off]}\n        #     print('breaking constraint to allow note off')\n        # else:\n        raise ValueError(f\"\"\"\n            no possible notes {note_on_map=} {note_off_map=}\"\"\")\n\n    def note_map(e):\n        try:\n            if e['vel'] &gt; 0:\n                m = note_on_map\n            else:\n                m = note_off_map\n            i = e.get('inst')\n            if i is not None:\n                m = m[i]\n            return m\n        except Exception:\n            traceback.print_exc()\n            print(f'{e=} {note_off_map=} {note_on_map=}')\n            raise\n        # print(f'{m=}')\n\n    w = 1 if steer_density is None else 2**(steer_density*2-1)\n\n    w_on = 0 if no_on else w\n    w_off = 0 if no_off else 1/w\n\n    min_vel = max(0.5, 0 if min_vel is None else min_vel)\n    max_vel = torch.inf if max_vel is None else max_vel\n\n    # print(f'{truncate_quantile_vel=}')\n\n    return self.deep_query(Query(\n        'vel', \n        cases=(\n            Range(-torch.inf,0.5,w_off), \n            Range(0.5,torch.inf,w_on,\n                min_vel,max_vel,truncate_quantile=truncate_quantile_vel)),\n        then=lambda e: Query(\n            'inst', \n            whitelist={\n                i:inst_weights.get(i,1) if e['vel'] &gt; 0 else 1 \n                for i in note_map(e)},\n            then=lambda e: Query(\n                'pitch', \n                whitelist=note_map(e),\n                truncate_quantile=(\n                    None if (\n                        e['vel']==0 \n                        or self.is_drum(e['inst']) \n                        or e['inst'] in no_steer)\n                    else truncate_quantile_pitch),\n                then=lambda e: Query(\n                    'time', #'note on' if e['vel']&gt;0 else 'note off',         \n                    truncate=(\n                        min_time if e['vel']&gt;0 \n                        else soonest_off[(e['inst'],e['pitch'])],\n                        latest_event\n                    ),\n                    truncate_quantile=(\n                        None if e['inst'] in no_steer\n                        else truncate_quantile_time),\n                    weight_top_p=rhythm_temp, \n                    component_temp=timing_temp\n    )))))\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.Notochord.query_vtip","title":"<code>query_vtip(note_on_map=None, note_off_map=None, min_time=None, max_time=None, min_vel=None, max_vel=None, min_polyphony=None, max_polyphony=None, min_duration=None, max_duration=None, rhythm_temp=None, timing_temp=None, truncate_quantile_time=None, truncate_quantile_pitch=None, truncate_quantile_vel=None, steer_density=None, inst_weights=None, no_steer=None)</code>","text":"<p>Query in a fixed velocity-&gt;time-&gt;instrument-&gt;pitch order, sampling all modalities. Because velocity is sampled first, this query method can  automatically prevent double NoteOn or NoteOff. It's also possible to make some more detailed constraints per-instrument compared to <code>query</code>, including note duration constraints which can eliminate stuck notes.</p> <p>query_vipt is similar, but makes different compromises in applying  constraints. VTIP is likely to be better when setting min_time &gt; 0  or otherwise heavily constraing time delta, while VIPT may be better in other cases.</p> <p>Parameters:</p> Name Type Description Default <code>note_on_map</code> <code>Dict[int, List[int]] | None</code> <p>possible note-ons as {instrument: [pitch]}  defaults to allowing any note. Notes already playing on a given instrument are always excluded.</p> <code>None</code> <code>note_off_map</code> <code>Dict[int, List[int]] | None</code> <p>possible note-offs as {instrument: [pitch]} defaults to using only the instruments in note_on_map.  Notes not already playing on a given instrument are  automatically excluded.</p> <code>None</code> <code>min_time</code> <code>Number | None</code> <p>global minimum interevent time (default 0)</p> <code>None</code> <code>max_time</code> <code>Number | None</code> <p>global maximum interevent time (default no limit)</p> <code>None</code> <code>min_vel</code> <code>Number | None</code> <p>global minimum velocity for NoteOn events (default 1)</p> <code>None</code> <code>max_vel</code> <code>Number | None</code> <p>global maximum velocity for NoteOn events (default 127)</p> <code>None</code> <code>min_polyphony</code> <code>Dict[int, int] | int | None</code> <p>minimum number of concurrent notes per instrument. (default 0). Can be a dict mapping instrument to value, or a single value for all instruments. When an instrument has &lt;= min polyphony, exclude NoteOffs</p> <code>None</code> <code>max_polyphony</code> <code>Dict[int, int] | int | None</code> <p>minimum number of concurrent notes per instrument. (default no limit). Can be a dict mapping instrument to value, or a single value for all instruments. When an instrument has &gt;= max polyphony, exclude NoteOns.</p> <code>None</code> <code>min_duration</code> <code>Dict[int, Number] | Number | None</code> <p>minimum note length per instrument (default 0). Can  be a dict mapping instrument to value, or a single value for  all instruments.</p> <code>None</code> <code>max_duration</code> <code>Dict[int, Number] | Number | None</code> <p>maximum note length per instrument (default 0). Can  be a dict mapping instrument to value, or a single value for  all instruments.</p> <code>None</code> <code>rhythm_temp</code> <code>float</code> <p>if not None, apply top_p sampling to the weighting of mixture components. this affects coarse rhythmic patterns; 0 is deterministic, 1 is 'natural' according to the model.</p> <code>None</code> <code>timing_temp</code> <code>float</code> <p>if not None, apply temperature sampling to the time component. this affects fine timing; 0 is deterministic and  precise, 1 is 'natural' according to the model.</p> <code>None</code> <code>truncate_quantile_time</code> <code>Tuple[float, float] | None</code> <p>applied after min_time, max_time truncate the remaining delta time distribution by quantile. e.g. truncate_quantile_time=(0.25, 0.75) excludes the shortest and longest 25% of probability mass.</p> <code>None</code> <code>truncate_quantile_pitch</code> <code>Tuple[float, float] | None</code> <p>truncate the pitch distribution by  quantile. e.g. truncate_quantile_pitch=(0.5, 1) always samples above the median predicted pitch. Ignored for drums.</p> <code>None</code> <code>truncate_quantile_vel</code> <code>Tuple[float, float] | None</code> <p>truncate the velocity distribution by  quantile. e.g. truncate_quantile_vel=(0, 0.5) always samples below the median predicted velocity. Affects only NoteOn.</p> <code>None</code> <code>steer_density</code> <code>float</code> <p>adjust relative weight of NoteOn and NoteOff. values above 0.5 favor NoteOn, values below 0.5 favor NoteOff.</p> <code>None</code> <code>inst_weights</code> <code>Dict[int, Number]</code> <p>multiplicatively adjust instrument probabilities.  Any instrument not included has a weight of 1. 0 would exclude an instrument completely (but better to do so via note_on_map)</p> <code>None</code> <code>no_steer</code> <code>List[int]</code> <p>collection of instruments to exclude from effect of  truncate_quantile_pitch.</p> <code>None</code> Source code in <code>src/notochord/model.py</code> <pre><code>def query_vtip(self,\n    note_on_map:Dict[int,List[int]]|None=None, \n    note_off_map:Dict[int,List[int]]|None=None,\n    min_time:Number|None=None, max_time:Number|None=None,\n    min_vel:Number|None=None, max_vel:Number|None=None,\n    min_polyphony:Dict[int,int]|int|None=None, \n    max_polyphony:Dict[int,int]|int|None=None,\n    min_duration:Dict[int,Number]|Number|None=None, \n    max_duration:Dict[int,Number]|Number|None=None, \n    rhythm_temp:float=None, timing_temp:float=None,\n    truncate_quantile_time:Tuple[float,float]|None=None,\n    truncate_quantile_pitch:Tuple[float,float]|None=None,\n    truncate_quantile_vel:Tuple[float,float]|None=None,\n    steer_density:float=None,\n    inst_weights:Dict[int,Number]=None,\n    no_steer:List[int]=None,\n    ):\n    \"\"\"\n    Query in a fixed velocity-&gt;time-&gt;instrument-&gt;pitch order, sampling all\n    modalities. Because velocity is sampled first, this query method can \n    automatically prevent double NoteOn or NoteOff. It's also possible to\n    make some more detailed constraints per-instrument compared to `query`,\n    including note duration constraints which can eliminate stuck notes.\n\n    query_vipt is similar, but makes different compromises in applying \n    constraints. VTIP is likely to be better when setting min_time &gt; 0 \n    or otherwise heavily constraing time delta, while VIPT may be better\n    in other cases.\n\n    Args:\n        note_on_map: possible note-ons as {instrument: [pitch]} \n            defaults to allowing any note. Notes already playing on a given\n            instrument are always excluded.\n        note_off_map: possible note-offs as {instrument: [pitch]}\n            defaults to using only the instruments in note_on_map. \n            Notes not already playing on a given instrument are \n            automatically excluded.\n        min_time: global minimum interevent time (default 0)\n        max_time: global maximum interevent time (default no limit)\n        min_vel: global minimum velocity for NoteOn events (default 1)\n        max_vel: global maximum velocity for NoteOn events (default 127)\n        min_polyphony: minimum number of concurrent notes per instrument.\n            (default 0). Can be a dict mapping instrument to value,\n            or a single value for all instruments.\n            When an instrument has &lt;= min polyphony, exclude NoteOffs\n        max_polyphony: minimum number of concurrent notes per instrument.\n            (default no limit). Can be a dict mapping instrument to value,\n            or a single value for all instruments.\n            When an instrument has &gt;= max polyphony, exclude NoteOns.\n        min_duration: minimum note length per instrument (default 0). Can   \n            be a dict mapping instrument to value, or a single value for \n            all instruments.\n        max_duration: maximum note length per instrument (default 0). Can   \n            be a dict mapping instrument to value, or a single value for \n            all instruments.\n        rhythm_temp: if not None, apply top_p sampling to the weighting\n            of mixture components. this affects coarse rhythmic patterns;\n            0 is deterministic, 1 is 'natural' according to the model.\n        timing_temp: if not None, apply temperature sampling to the time\n            component. this affects fine timing; 0 is deterministic and \n            precise, 1 is 'natural' according to the model.\n        truncate_quantile_time: applied after min_time, max_time\n            truncate the remaining delta time distribution by quantile.\n            e.g. truncate_quantile_time=(0.25, 0.75)\n            excludes the shortest and longest 25% of probability mass.\n        truncate_quantile_pitch: truncate the pitch distribution by \n            quantile. e.g. truncate_quantile_pitch=(0.5, 1) always samples\n            above the median predicted pitch. Ignored for drums.\n        truncate_quantile_vel: truncate the velocity distribution by \n            quantile. e.g. truncate_quantile_vel=(0, 0.5) always\n            samples below the median predicted velocity. Affects only NoteOn.\n        steer_density: adjust relative weight of NoteOn and NoteOff.\n            values above 0.5 favor NoteOn, values below 0.5 favor NoteOff.\n        inst_weights: multiplicatively adjust instrument probabilities. \n            Any instrument not included has a weight of 1. 0 would exclude\n            an instrument completely (but better to do so via note_on_map)\n        no_steer: collection of instruments to exclude from effect of \n            truncate_quantile_pitch.\n    \"\"\"\n    # NOTE: have to add epsilon when comparing sampled times,\n    # or else rounding error can cause discrepancy \n    eps = 1e-5\n    min_time = min_time or 0\n    max_time = max_time or torch.inf\n\n    inst_weights = inst_weights or {}\n    no_steer = no_steer or set()\n\n    note_on_map, note_off_map = self.get_note_maps(\n        note_on_map, note_off_map, min_polyphony, max_polyphony\n    )\n\n    max_dur = get_from_scalar_or_dict(max_duration, torch.inf)\n    min_dur = get_from_scalar_or_dict(min_duration, 0)\n\n    # need to compute time constraints from polyphony and duration,\n    # given velocity but not inst/pitch\n    # polyphony should't affect time except via note op/off maps\n    # soonest_off can just be reduced for purposes of truncating time\n    # but then need to compute the allowed instruments, and then pitches,\n    # given the sampled time, based on duration constraints\n    # only needed in the noteoff case: then check if time &gt;= soonest_off\n    # 1. for any pitch in each instrument\n    # 2. which pitches for the sampled instrument\n\n    # duration does not constrain the soonest noteOn;\n    # the soonest possible noteOff is the next note which would end with \n    # minimal duration (but no sooner than the global min_time)\n    # compute that soonest noteOff time for each possible noteOff:\n    soonest_off = {\n        (i,p):max(min_time, min_dur(i) - self.held_notes[(i,p)]) \n        for i,ps in note_off_map.items()\n        for p in ps}\n    # print(f'{soonest_off=}')\n    soonest_off_any = min(soonest_off.values(), default=0)\n\n    # in case where only note off is allowed (likely due to max_polyphony)\n    # min_duration and max_time can be unsatisfiable\n    # break the max_time constraint in that case\n    no_on = all(len(ps)==0 for ps in note_on_map.values())\n    if no_on:\n        if soonest_off_any &gt; max_time:\n            max_time = soonest_off_any + eps\n            print(f'breaking max_time constraint -&gt; {max_time}s')\n\n    # latest possible event is minimum max remaining duration over all held notes (i.e. the soonest noteOff ending a max-duration note)\n    latest_event = max_time\n    for (i,p),t in self.held_notes.items():\n        latest_event = min(latest_event, max_dur(i) - t)\n    # slip to accomodate global constraint\n    latest_event = max(min_time, latest_event)\n\n    # print(f'pre {note_off_map=}') ###DEBUG\n\n    # remove impossible note offs\n    # (i.e. soonest possible note-off is after the latest possible event)\n    for i,ps in list(note_off_map.items()):\n        for p in list(ps):\n            if soonest_off[(i,p)] &gt; latest_event:\n                ps.remove(p)\n        if not len(ps):\n            note_off_map.pop(i)\n            continue\n\n    # print(f'post {note_off_map=}') ###DEBUG\n    no_off = all(len(ps)==0 for ps in note_off_map.values())\n    # print(f'{no_on=} {no_off=}')\n\n    if no_on and no_off:\n        raise NoPossibleEvents(f\"\"\"\n            no possible notes {note_on_map=} {note_off_map=}\"\"\")\n\n    def insts(e):\n        if e['vel'] &gt; 0:\n            return note_on_map\n        else:\n            return {\n                i for i,ps in note_off_map.items() if any(\n                    soonest_off[(i,p)] &lt;= e['time']+eps for p in ps\n                )}\n\n    def pitches(e):\n        i = e['inst']\n        if e['vel'] &gt; 0:\n            return note_on_map[i]\n        else:\n            return {\n                p for p in note_off_map[i] \n                if soonest_off[(i,p)] &lt;= e['time']+eps}\n\n    w = 1 if steer_density is None else 2**(steer_density*2-1)\n\n    w_on = 0 if no_on else w\n    w_off = 0 if no_off else 1/w\n\n    min_vel = max(0.5, 0 if min_vel is None else min_vel)\n    max_vel = torch.inf if max_vel is None else max_vel\n\n    return self.deep_query(Query(\n        'vel', \n        cases=(\n            Range(-torch.inf,0.5,w_off), \n            Range(0.5,torch.inf,w_on,\n                min_vel,max_vel,truncate_quantile=truncate_quantile_vel)),\n        then=lambda e: Query(\n            'time',       \n            truncate=(\n                min_time if e['vel']&gt;0 else soonest_off_any,\n                latest_event\n            ),\n            truncate_quantile=truncate_quantile_time,\n            weight_top_p=rhythm_temp, \n            component_temp=timing_temp,\n            then=lambda e: Query(\n                'inst', \n                whitelist={\n                    i:inst_weights.get(i,1) if e['vel'] &gt; 0 else 1 \n                    for i in insts(e)},\n                then=lambda e: Query(\n                    'pitch', \n                    whitelist=pitches(e),\n                    truncate_quantile=(\n                        None if (\n                            e['vel']==0 \n                            or self.is_drum(e['inst']) \n                            or e['inst'] in no_steer)\n                        else truncate_quantile_pitch),\n    )))))\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.Notochord.reset","title":"<code>reset(start=None, state=None)</code>","text":"<p>resets internal model state. Args:     start: if True, send start tokens through the model         default behavior is True when state=None, False otherwise     state: set the state from a result of <code>get_state</code>,         instead of the initial state</p> Source code in <code>src/notochord/model.py</code> <pre><code>def reset(self, start=None, state=None):\n    \"\"\"\n    resets internal model state.\n    Args:\n        start: if True, send start tokens through the model\n            default behavior is True when state=None, False otherwise\n        state: set the state from a result of `get_state`,\n            instead of the initial state\n    \"\"\"\n    self.current_time = 0\n    self.held_notes.clear()\n    self.step = 0\n    if start is None:\n        start = state is None\n    if state is None: \n        named_states = zip(self.cell_state_names(), self.initial_state)\n    else:\n        named_states = state.items()\n    self.h_query = None\n    with torch.inference_mode():\n        for n,t in named_states:\n            getattr(self, n)[:] = t\n        if start:\n            self.feed(\n                self.instrument_start_token, self.pitch_start_token, 0., 0.)\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.Range","title":"<code>Range</code>","text":"Source code in <code>src/notochord/model.py</code> <pre><code>class Range:\n    def __init__(self, lo=-torch.inf, hi=torch.inf, weight=1, sample_lo=None, sample_hi=None, **kw):\n        \"\"\"use lo, hi when computing weights of each branch; but actually sample between sample_lo and sample_hi. for example, you could let lo,hi cover the full range to compute the true model on/off ratio, but sample from a narrow range of allowed velocities in the noteOn case.\n\n        **kw gets passed to sample once the case is selected\n        \"\"\"\n        self.lo = lo\n        self.hi = hi\n        self.weight = weight\n        self.sample_lo = lo if sample_lo is None else sample_lo\n        self.sample_hi = hi if sample_hi is None else sample_hi\n        self.kw = kw\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.Range.__init__","title":"<code>__init__(lo=-torch.inf, hi=torch.inf, weight=1, sample_lo=None, sample_hi=None, **kw)</code>","text":"<p>use lo, hi when computing weights of each branch; but actually sample between sample_lo and sample_hi. for example, you could let lo,hi cover the full range to compute the true model on/off ratio, but sample from a narrow range of allowed velocities in the noteOn case.</p> <p>**kw gets passed to sample once the case is selected</p> Source code in <code>src/notochord/model.py</code> <pre><code>def __init__(self, lo=-torch.inf, hi=torch.inf, weight=1, sample_lo=None, sample_hi=None, **kw):\n    \"\"\"use lo, hi when computing weights of each branch; but actually sample between sample_lo and sample_hi. for example, you could let lo,hi cover the full range to compute the true model on/off ratio, but sample from a narrow range of allowed velocities in the noteOn case.\n\n    **kw gets passed to sample once the case is selected\n    \"\"\"\n    self.lo = lo\n    self.hi = hi\n    self.weight = weight\n    self.sample_lo = lo if sample_lo is None else sample_lo\n    self.sample_hi = hi if sample_hi is None else sample_hi\n    self.kw = kw\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.SineEmbedding","title":"<code>SineEmbedding</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>src/notochord/model.py</code> <pre><code>class SineEmbedding(nn.Module):\n    def __init__(self, n, hidden, w0=1e-3, w1=10, scale='log'):\n        \"\"\"\n        Args:\n            n (int): number of sinusoids\n            hidden (int): embedding size\n            w0 (float): minimum wavelength\n            w1 (float): maximum wavelength\n            scale (str): if 'log', more wavelengths close to w0\n        \"\"\"\n        super().__init__()\n        if scale=='log':\n            w0 = math.log(w0)\n            w1 = math.log(w1)\n        ws = torch.linspace(w0, w1, n)\n        if scale=='log':\n            ws = ws.exp()\n        self.register_buffer('fs', 2 * math.pi / ws)\n        self.proj = nn.Linear(n,hidden)\n\n    def forward(self, x):\n        x = x[...,None] * self.fs\n        return self.proj(x.sin())\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.SineEmbedding.__init__","title":"<code>__init__(n, hidden, w0=0.001, w1=10, scale='log')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>number of sinusoids</p> required <code>hidden</code> <code>int</code> <p>embedding size</p> required <code>w0</code> <code>float</code> <p>minimum wavelength</p> <code>0.001</code> <code>w1</code> <code>float</code> <p>maximum wavelength</p> <code>10</code> <code>scale</code> <code>str</code> <p>if 'log', more wavelengths close to w0</p> <code>'log'</code> Source code in <code>src/notochord/model.py</code> <pre><code>def __init__(self, n, hidden, w0=1e-3, w1=10, scale='log'):\n    \"\"\"\n    Args:\n        n (int): number of sinusoids\n        hidden (int): embedding size\n        w0 (float): minimum wavelength\n        w1 (float): maximum wavelength\n        scale (str): if 'log', more wavelengths close to w0\n    \"\"\"\n    super().__init__()\n    if scale=='log':\n        w0 = math.log(w0)\n        w1 = math.log(w1)\n    ws = torch.linspace(w0, w1, n)\n    if scale=='log':\n        ws = ws.exp()\n    self.register_buffer('fs', 2 * math.pi / ws)\n    self.proj = nn.Linear(n,hidden)\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.Subset","title":"<code>Subset</code>","text":"Source code in <code>src/notochord/model.py</code> <pre><code>class Subset:\n    def __init__(self, values=None, weight=1, sample_values=None, **kw):\n        \"\"\"**kw gets passed to sample once the case is selected\"\"\"\n        self.values = values\n        self.weight = weight\n        self.sample_values = values if sample_values is None else sample_values\n        self.kw = kw\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.Subset.__init__","title":"<code>__init__(values=None, weight=1, sample_values=None, **kw)</code>","text":"<p>**kw gets passed to sample once the case is selected</p> Source code in <code>src/notochord/model.py</code> <pre><code>def __init__(self, values=None, weight=1, sample_values=None, **kw):\n    \"\"\"**kw gets passed to sample once the case is selected\"\"\"\n    self.values = values\n    self.weight = weight\n    self.sample_values = values if sample_values is None else sample_values\n    self.kw = kw\n</code></pre>"},{"location":"reference/notochord/model/#notochord.model.prompt","title":"<code>prompt(noto, midi_file, merge=False, insert_threshold=0.05, state_hash=None)</code>","text":"<p>Read a MIDI file and feed events to a Notochord model.</p> <p>Parameters:</p> Name Type Description Default <code>noto</code> <code>Notochord</code> <p>a Notochord</p> required <code>midi_file</code> <code>str | Path</code> <p>path of a midi file to read</p> required <code>merge</code> <code>bool</code> <p>whether to merge parts on the same instrument or use anonymous instrument IDs</p> <code>False</code> <code>insert_threshold</code> <code>Number</code> <p>minimum note duration, in seconds, to insert a noteOff rather than drop a noteOn when handling overlap</p> <code>0.05</code> <code>state_hash</code> <code>int | None</code> <p>representation of model hidden state to use for caching results</p> <code>None</code> <p>Returns:     state: hidden state dict of the Notochord encoding the MIDI prompt     inst_data: dict mapping Notochord instrument to metadata (see <code>InstrumentData</code> class)</p> Source code in <code>src/notochord/model.py</code> <pre><code>@mem.cache(ignore=('noto',))\ndef prompt(\n    noto:Notochord, midi_file:str|Path, \n    merge:bool=False, insert_threshold:Number=0.05, \n    state_hash:int|None=None): \n    # state_hash is used for disk cache only\n    \"\"\"Read a MIDI file and feed events to a Notochord model.\n\n    Args:\n        noto: a Notochord\n        midi_file: path of a midi file to read\n        merge: whether to merge parts on the same instrument or use anonymous\n            instrument IDs\n        insert_threshold: minimum note duration, in seconds, to insert a\n            noteOff rather than drop a noteOn when handling overlap\n        state_hash: representation of model hidden state to use for caching results\n    Returns:\n        state: hidden state dict of the Notochord encoding the MIDI prompt\n        inst_data: dict mapping Notochord instrument to metadata (see `InstrumentData` class)\n    \"\"\"\n    # TODO: deduplicate this code?\n    class AnonTracks:\n        def __init__(self):\n            self.n = 0\n            self.n_drum = 0\n        def __call__(self, drum=False):\n            if drum:\n                self.n_drum += 1\n                return noto.first_anon_like(129)+self.n_drum\n            else:\n                self.n += 1\n                return noto.first_anon_like(1)+self.n\n    next_anon = AnonTracks()\n    # track current instrument on each channel\n    noto_channel_inst = defaultdict(next_anon)\n    # tracks the original instrument when anon is used to disambiguate parts\n    # note the 'original' instrument is still anon when there is no PC on channel\n    orig_channel_inst = defaultdict(lambda c: noto_channel_inst[c])\n    # metadata for each instrument\n    inst_data = defaultdict(InstrumentData)\n\n    mid = mido.MidiFile(midi_file)\n    ticks_per_beat = mid.ticks_per_beat\n    us_per_beat = 500_000\n    time_seconds = 0\n    prev_time_seconds = 0\n    # event_count = defaultdict(int)\n    print(f'MIDI file: {ticks_per_beat} ticks, {us_per_beat} \u03bcs per beat')\n\n    dropped_notes = set()\n\n    def set_inst(chan, inst):\n        orig_inst = inst\n        # get anonymous instrument if already in use and not merging\n        inst_reused = any(\n            inst==i and chan!=c for c,i in noto_channel_inst.items())\n        if inst_reused and not merge:\n            inst = next_anon(drum=noto.is_drum(inst))\n\n        noto_channel_inst[chan] = inst\n        orig_channel_inst[chan] = orig_inst\n\n    for msg in tqdm(mid, desc='ingesting MIDI prompt'):\n        chan = msg.channel if hasattr(msg, 'channel') else None\n        # when iterating over a track this is ticks,\n        # when iterating the whole file it's seconds\n        time_seconds += msg.time\n\n        if msg.type=='program_change':\n            set_inst(chan, msg.program + 1 + 128*int(msg.channel==9))\n\n            # tqdm.write(str(msg))\n            tqdm.write(f'MIDI file: set program {msg.program} (channel {chan}) at {time_seconds} seconds')\n\n        elif msg.type=='set_tempo':\n            us_per_beat = msg.tempo\n            tqdm.write(f'MIDI file: set tempo {us_per_beat} \u03bcs/beat at {time_seconds} seconds')\n\n        elif msg.type in ('note_on', 'note_off'):\n            # make channel 10 with no PC anonymous drumkit\n            if chan not in noto_channel_inst and chan==9:\n                set_inst(chan, noto.first_anon_like(129))\n\n            orig_inst = orig_channel_inst[chan]\n            inst = noto_channel_inst[chan]\n            pitch = msg.note\n            dt = time_seconds - prev_time_seconds\n            vel = msg.velocity if msg.type=='note_on' else 0\n            # event_count[(chan, mid_channel_inst[chan])] += 1\n\n            d = inst_data[inst]\n            d.orig_inst = orig_inst\n            if vel &gt; 0:\n                # handle collision:\n                # if the ongoing note is past a given length,\n                # insert a noteOff\n                # otherwise drop the second one\n                if (inst, pitch) in noto.held_notes:\n                    # if inst==50: tqdm.write(f'collision: {(inst, pitch)=} {time_seconds=}')\n                    dur = noto.held_notes[(inst,pitch)] + dt\n                    dropped_notes.add((chan, pitch))\n                    if dur &gt; insert_threshold:\n                        # if inst==50: tqdm.write(f'insert {dur=}')\n                        noto.feed(inst, pitch, dt, 0)\n                        d.shortened += 1\n                        dt = 0\n                    else:\n                        # if inst==50: tqdm.write(f'drop {dur=}')\n                        d.dropped += 1\n                        continue\n                d.notes += 1\n                d.pitches.add(pitch)\n                d.velocities.add(vel)\n            else:\n                # use first noteOff (likely from a different channel)\n                # if (inst, pitch) not in noto.held_notes:\n                    # continue\n                # use corresponding noteOff\n                if (chan, pitch) in dropped_notes:\n                    # if inst==50: tqdm.write(f'dropped: {(chan, pitch)=} {time_seconds=}')\n                    dropped_notes.remove((chan, pitch))\n                    continue\n\n            # if inst==50: tqdm.write(f'event: {(chan, inst, pitch, vel)=} {time_seconds=}')\n\n            d.channels.add(chan)\n\n            # event_count[mid_channel_inst[msg.channel]] += 1\n            noto.feed(inst, pitch, dt, vel)\n            prev_time_seconds = time_seconds\n\n        else: continue\n\n    inst_data = {k:v for k,v in inst_data.items() if v.notes&gt;0}\n    print('MIDI file:', inst_data)\n\n    return noto.get_state(), inst_data\n</code></pre>"},{"location":"reference/notochord/perform/","title":"Perform","text":""},{"location":"reference/notochord/perform/#notochord.perform.MIDIConfig","title":"<code>MIDIConfig</code>","text":"<p>             Bases: <code>dict</code></p> <p>invertible map from MIDI channel: Notochord instrument</p> Source code in <code>src/notochord/perform.py</code> <pre><code>class MIDIConfig(dict):\n    \"\"\"\n    invertible map from MIDI channel: Notochord instrument\n    \"\"\"\n    def __init__(self, *a, **kw):\n        super().__init__(*a, **kw)\n        self.invertible = len(self.channels)==len(self.insts)\n\n    @property\n    def channels(self):\n        \"\"\"set of channels\"\"\"\n        return set(self)\n    @property\n    def insts(self):\n        \"\"\"set of instruments\"\"\"\n        return set(self.values())\n    def inv(self, inst):\n        \"\"\"map from Notochord instrument: MIDI channel\"\"\"\n        if not self.invertible:\n            print('WARNING: MIDIConfig is not invertible')\n        for chan,inst_ in self.items():\n            if inst_==inst:\n                return chan\n        raise KeyError(f\"\"\"\n            instrument {inst} has no channel\n            \"\"\")\n</code></pre>"},{"location":"reference/notochord/perform/#notochord.perform.MIDIConfig.channels","title":"<code>channels</code>  <code>property</code>","text":"<p>set of channels</p>"},{"location":"reference/notochord/perform/#notochord.perform.MIDIConfig.insts","title":"<code>insts</code>  <code>property</code>","text":"<p>set of instruments</p>"},{"location":"reference/notochord/perform/#notochord.perform.MIDIConfig.inv","title":"<code>inv(inst)</code>","text":"<p>map from Notochord instrument: MIDI channel</p> Source code in <code>src/notochord/perform.py</code> <pre><code>def inv(self, inst):\n    \"\"\"map from Notochord instrument: MIDI channel\"\"\"\n    if not self.invertible:\n        print('WARNING: MIDIConfig is not invertible')\n    for chan,inst_ in self.items():\n        if inst_==inst:\n            return chan\n    raise KeyError(f\"\"\"\n        instrument {inst} has no channel\n        \"\"\")\n</code></pre>"},{"location":"reference/notochord/perform/#notochord.perform.NotoPerformance","title":"<code>NotoPerformance</code>","text":"<p>track various quantities of a Notochord performance:</p> event history <ul> <li>wall time</li> <li>nominal dt</li> <li>pitch</li> <li>velocity (0 for noteoff)</li> <li>notochord instrument</li> </ul> query for <ul> <li>instruments present in the last N events</li> <li>number of note_ons by instrument in last N events</li> <li>currently playing notes with user data as {(inst, pitch): Any}</li> <li>currently playing notes as {inst: pitches}</li> </ul> Source code in <code>src/notochord/perform.py</code> <pre><code>class NotoPerformance:\n    \"\"\"\n    track various quantities of a Notochord performance:\n\n    event history:\n        * wall time\n        * nominal dt\n        * pitch\n        * velocity (0 for noteoff)\n        * notochord instrument\n\n    query for:\n        * instruments present in the last N events\n        * number of note_ons by instrument in last N events\n        * currently playing notes with user data as {(inst, pitch): Any}\n        * currently playing notes as {inst: pitches}\n    \"\"\"\n    def __init__(self):\n        self._notes:Dict[Note, Any] = {} \n        self.past_segments:List[pd.DataFrame] = []\n        self.init()\n\n    def init(self):\n        self.events = pd.DataFrame(np.array([],dtype=[\n            ('wall_time_ns',np.int64), # actual wall time played in ns\n            ('time',np.float32), # nominal notochord dt in seconds\n            ('inst',np.int16), # notochord instrument\n            ('pitch',np.int16), # MIDI pitch\n            ('vel',np.int8), # MIDI velocity\n            ('channel',np.int8), # MIDI channel\n            ]))\n        self._notes.clear()\n\n    def push(self):\n        \"\"\"push current events onto a list of past segments,\n            start a fresh history\n        \"\"\"\n        self.past_segments.append(self.events)\n        self.init()\n\n    def feed(self, held_note_data:Any=None, **event):\n        \"\"\"\n        Args:\n            held_note_data: any Python object to be attached to held notes\n                (ignored for note-offs)\n            ('wall_time_ns',np.int64), # actual wall time played in ns\n            ('time',np.float32), # nominal notochord dt in seconds\n            ('inst',np.int16), # notochord instrument\n            ('pitch',np.int16), # MIDI pitch\n            ('vel',np.int8), # MIDI velocity\n            ('channel',np.int8), # MIDI channel (1-16)\n        \"\"\"\n        if 'wall_time_ns' not in event:\n            event['wall_time_ns'] = time.time_ns()\n        if 'channel' not in event:\n            # use -1 for missing channel to avoid coercion to float\n            event['channel'] = -1 \n        cast_event = {}\n        for k,v in event.items():\n            if k in self.events.columns:\n                cast_event[k] = self.events.dtypes[k].type(v)\n        event = cast_event\n\n        self.events.loc[len(self.events)] = event\n\n        chan = event.get('channel', None)\n        # inst, pitch, vel = event['inst'], event['pitch'], event['vel']\n        # k = (chan, inst, pitch)\n        vel = event['vel']\n        k = Note(chan, event['inst'], event['pitch'])\n\n        if vel &gt; 0:\n            self._notes[k] = held_note_data\n        else:\n            self._notes.pop(k, None)\n\n    def inst_counts(self, n=0, insts=None):\n        \"\"\"instrument counts in last n (default all) note_ons\"\"\"\n        df = self.events\n        df = df.iloc[-min(128,n*16):] # in case of very long history\n        df = df.loc[df.vel &gt; 0]\n        df = df.iloc[-n:]\n        counts = df.inst.value_counts()\n        if insts is not None:\n            for inst in insts:\n                if inst not in counts.index:\n                    counts[inst] = 0\n        return counts\n\n    def held_inst_pitch_map(self, insts=None):\n        \"\"\"held notes as {inst:[pitch]} for given instruments\"\"\"\n        note_map = defaultdict(list)\n        for note in self._notes:\n            if insts is None or note.inst in insts:\n                note_map[note.inst].append(note.pitch)\n        return note_map\n\n    @property\n    def note_pairs(self):\n        \"\"\"\n        held notes as {(inst,pitch)}.\n        returns a new `set`; safe to modify history while iterating\n        \"\"\"\n        return {(note.inst, note.pitch) for note in self._notes}\n\n    @property\n    def note_triples(self):\n        \"\"\"\n        held notes as {(channel,inst,pitch)}.\n        returns a new `set`; safe to modify history while iterating\n        \"\"\"\n        return {(note.chan, note.inst, note.pitch) for note in self._notes}\n\n    @property\n    def notes(self):\n        \"\"\"\n        generic way to access notes, returns set of namedtuples \n        returns a new `set`; safe to modify history while iterating\n        \"\"\"\n        return set(self._notes)\n\n    @property\n    def note_data(self):\n        \"\"\"held notes as {(chan,inst,pitch):held_note_data}.\n        mutable.\n        \"\"\"\n        # NOTE: returned dictionary should be mutable\n        return self._notes\n</code></pre>"},{"location":"reference/notochord/perform/#notochord.perform.NotoPerformance.note_data","title":"<code>note_data</code>  <code>property</code>","text":"<p>held notes as {(chan,inst,pitch):held_note_data}. mutable.</p>"},{"location":"reference/notochord/perform/#notochord.perform.NotoPerformance.note_pairs","title":"<code>note_pairs</code>  <code>property</code>","text":"<p>held notes as {(inst,pitch)}. returns a new <code>set</code>; safe to modify history while iterating</p>"},{"location":"reference/notochord/perform/#notochord.perform.NotoPerformance.note_triples","title":"<code>note_triples</code>  <code>property</code>","text":"<p>held notes as {(channel,inst,pitch)}. returns a new <code>set</code>; safe to modify history while iterating</p>"},{"location":"reference/notochord/perform/#notochord.perform.NotoPerformance.notes","title":"<code>notes</code>  <code>property</code>","text":"<p>generic way to access notes, returns set of namedtuples  returns a new <code>set</code>; safe to modify history while iterating</p>"},{"location":"reference/notochord/perform/#notochord.perform.NotoPerformance.feed","title":"<code>feed(held_note_data=None, **event)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>held_note_data</code> <code>Any</code> <p>any Python object to be attached to held notes (ignored for note-offs)</p> <code>None</code> Source code in <code>src/notochord/perform.py</code> <pre><code>def feed(self, held_note_data:Any=None, **event):\n    \"\"\"\n    Args:\n        held_note_data: any Python object to be attached to held notes\n            (ignored for note-offs)\n        ('wall_time_ns',np.int64), # actual wall time played in ns\n        ('time',np.float32), # nominal notochord dt in seconds\n        ('inst',np.int16), # notochord instrument\n        ('pitch',np.int16), # MIDI pitch\n        ('vel',np.int8), # MIDI velocity\n        ('channel',np.int8), # MIDI channel (1-16)\n    \"\"\"\n    if 'wall_time_ns' not in event:\n        event['wall_time_ns'] = time.time_ns()\n    if 'channel' not in event:\n        # use -1 for missing channel to avoid coercion to float\n        event['channel'] = -1 \n    cast_event = {}\n    for k,v in event.items():\n        if k in self.events.columns:\n            cast_event[k] = self.events.dtypes[k].type(v)\n    event = cast_event\n\n    self.events.loc[len(self.events)] = event\n\n    chan = event.get('channel', None)\n    # inst, pitch, vel = event['inst'], event['pitch'], event['vel']\n    # k = (chan, inst, pitch)\n    vel = event['vel']\n    k = Note(chan, event['inst'], event['pitch'])\n\n    if vel &gt; 0:\n        self._notes[k] = held_note_data\n    else:\n        self._notes.pop(k, None)\n</code></pre>"},{"location":"reference/notochord/perform/#notochord.perform.NotoPerformance.held_inst_pitch_map","title":"<code>held_inst_pitch_map(insts=None)</code>","text":"<p>held notes as {inst:[pitch]} for given instruments</p> Source code in <code>src/notochord/perform.py</code> <pre><code>def held_inst_pitch_map(self, insts=None):\n    \"\"\"held notes as {inst:[pitch]} for given instruments\"\"\"\n    note_map = defaultdict(list)\n    for note in self._notes:\n        if insts is None or note.inst in insts:\n            note_map[note.inst].append(note.pitch)\n    return note_map\n</code></pre>"},{"location":"reference/notochord/perform/#notochord.perform.NotoPerformance.inst_counts","title":"<code>inst_counts(n=0, insts=None)</code>","text":"<p>instrument counts in last n (default all) note_ons</p> Source code in <code>src/notochord/perform.py</code> <pre><code>def inst_counts(self, n=0, insts=None):\n    \"\"\"instrument counts in last n (default all) note_ons\"\"\"\n    df = self.events\n    df = df.iloc[-min(128,n*16):] # in case of very long history\n    df = df.loc[df.vel &gt; 0]\n    df = df.iloc[-n:]\n    counts = df.inst.value_counts()\n    if insts is not None:\n        for inst in insts:\n            if inst not in counts.index:\n                counts[inst] = 0\n    return counts\n</code></pre>"},{"location":"reference/notochord/perform/#notochord.perform.NotoPerformance.push","title":"<code>push()</code>","text":"<p>push current events onto a list of past segments, start a fresh history</p> Source code in <code>src/notochord/perform.py</code> <pre><code>def push(self):\n    \"\"\"push current events onto a list of past segments,\n        start a fresh history\n    \"\"\"\n    self.past_segments.append(self.events)\n    self.init()\n</code></pre>"},{"location":"reference/notochord/rnn/","title":"Rnn","text":""},{"location":"reference/notochord/rnn/#notochord.rnn.GenericRNN","title":"<code>GenericRNN</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>src/notochord/rnn.py</code> <pre><code>class GenericRNN(nn.Module):\n    kind_cls = {\n        'gru':GRU,\n        'lstm':LSTM,\n        'elman':RNN,\n        'exprnn':ExpRNN\n        }\n    def __init__(self, kind, *a, **kw):\n        super().__init__()\n        if kw.get('bidirectional'): raise ValueError(\"\"\"\n            bidirectional GenericRNN not supported.\n            \"\"\")\n        cls = GenericRNN.kind_cls[kind]\n        self.kind = kind\n        self.rnn = cls(*a, **kw)\n\n    def __getattr__(self, a):\n        try:\n            return  super().__getattr__(a)\n        except AttributeError:\n            return getattr(self.rnn, a)\n\n    def forward(self, x, initial_state):\n        \"\"\"\n        Args:\n            x: Tensor[batch x time x channel] if batch_first else [time x batch x channel]\n            initial_state: List[Tensor[layers x batch x hidden]]], list of components \n            with 0 being hidden state (e.g. 1 is cell state for LSTM). \n        Returns:\n            hidden: hidden states of top layers Tensor[batch x time x hidden]\n                or [time x batch x hidden]\n            new_states: List[Tensor[layers x batch x hidden]]\n        \"\"\"\n        hidden, final_state = self.rnn.forward(x, initial_state)  #forward or __call__?\n        return hidden, final_state\n</code></pre>"},{"location":"reference/notochord/rnn/#notochord.rnn.GenericRNN.forward","title":"<code>forward(x, initial_state)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <p>Tensor[batch x time x channel] if batch_first else [time x batch x channel]</p> required <code>initial_state</code> <p>List[Tensor[layers x batch x hidden]]], list of components </p> required <p>Returns:     hidden: hidden states of top layers Tensor[batch x time x hidden]         or [time x batch x hidden]     new_states: List[Tensor[layers x batch x hidden]]</p> Source code in <code>src/notochord/rnn.py</code> <pre><code>def forward(self, x, initial_state):\n    \"\"\"\n    Args:\n        x: Tensor[batch x time x channel] if batch_first else [time x batch x channel]\n        initial_state: List[Tensor[layers x batch x hidden]]], list of components \n        with 0 being hidden state (e.g. 1 is cell state for LSTM). \n    Returns:\n        hidden: hidden states of top layers Tensor[batch x time x hidden]\n            or [time x batch x hidden]\n        new_states: List[Tensor[layers x batch x hidden]]\n    \"\"\"\n    hidden, final_state = self.rnn.forward(x, initial_state)  #forward or __call__?\n    return hidden, final_state\n</code></pre>"},{"location":"reference/notochord/rnn/#notochord.rnn.rnn_shim","title":"<code>rnn_shim(cls)</code>","text":"<p>LSTM API for GRU and RNN.</p> <p>hidden state is first element of state tuple</p> Source code in <code>src/notochord/rnn.py</code> <pre><code>def rnn_shim(cls):\n    \"\"\"LSTM API for GRU and RNN.\n\n    hidden state is first element of state tuple\"\"\"\n    class shim(cls):\n        def forward(self, input, states=(None,)):\n            assert len(states)==1\n            out, h = super().forward(input, *states)\n            return out, (h,)\n    return shim\n</code></pre>"},{"location":"reference/notochord/train/","title":"Train","text":""},{"location":"reference/notochord/train/#notochord.train.Resumable","title":"<code>Resumable</code>","text":"Source code in <code>src/notochord/train.py</code> <pre><code>class Resumable:\n    def __init__(self, checkpoint=None, resume=True, **kw):\n        \"\"\"\n        Args:\n            checkpoint: path to training checkpoint file\n            resume: if True, retore optimizer states etc\n                otherwise, restore only model weights (for transfer learning)\n        \"\"\"\n        if checkpoint is not None:\n            d = torch.load(checkpoint, map_location=torch.device('cpu'))\n            print(f'loaded checkpoint {checkpoint}')\n            if d['kw'].get('batch_len_schedule') is not None: print(\"\"\"\n            warning: checkpoints don't track `batch_len`. \n            be sure to manually set batch_len if resuming a run \n            using `batch_len_schedule`\n            \"\"\")\n            # merges sub dicts, e.g. model hyperparameters\n            deep_update(d['kw'], kw)\n            self._trainer = Trainer(**d['kw'])\n            self._trainer.load_state(d, resume=resume)\n        else:\n            self._trainer = Trainer(**kw)\n\n    def train(self):\n        self._trainer.train()\n\n    def test(self):\n        self._trainer.test()\n</code></pre>"},{"location":"reference/notochord/train/#notochord.train.Resumable.__init__","title":"<code>__init__(checkpoint=None, resume=True, **kw)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <p>path to training checkpoint file</p> <code>None</code> <code>resume</code> <p>if True, retore optimizer states etc otherwise, restore only model weights (for transfer learning)</p> <code>True</code> Source code in <code>src/notochord/train.py</code> <pre><code>def __init__(self, checkpoint=None, resume=True, **kw):\n    \"\"\"\n    Args:\n        checkpoint: path to training checkpoint file\n        resume: if True, retore optimizer states etc\n            otherwise, restore only model weights (for transfer learning)\n    \"\"\"\n    if checkpoint is not None:\n        d = torch.load(checkpoint, map_location=torch.device('cpu'))\n        print(f'loaded checkpoint {checkpoint}')\n        if d['kw'].get('batch_len_schedule') is not None: print(\"\"\"\n        warning: checkpoints don't track `batch_len`. \n        be sure to manually set batch_len if resuming a run \n        using `batch_len_schedule`\n        \"\"\")\n        # merges sub dicts, e.g. model hyperparameters\n        deep_update(d['kw'], kw)\n        self._trainer = Trainer(**d['kw'])\n        self._trainer.load_state(d, resume=resume)\n    else:\n        self._trainer = Trainer(**kw)\n</code></pre>"},{"location":"reference/notochord/train/#notochord.train.Trainer","title":"<code>Trainer</code>","text":"Source code in <code>src/notochord/train.py</code> <pre><code>class Trainer:\n    def __init__(self, \n        experiment, # experiment name\n        model_dir,\n        log_dir,\n        data_dir,\n        results_dir,\n        model = None, # dict of model constructor overrides\n        batch_size = 128,\n        batch_len = 64,\n        batch_len_schedule = None,\n        batch_len_max = 512,\n        lr = 3e-4,\n        adam_betas = (0.9, 0.999),\n        adam_eps = 1e-08, \n        weight_decay = 0.01,\n        grad_clip = 1.0,\n        seed = 0, # random seed\n        n_jobs = 1, # for dataloaders\n        device = 'cpu', # 'cuda:0'\n        epoch_size = None, # in iterations, None for whole dataset\n        txala = False,\n        txala_remap = False,\n        txala_permute = False,\n        min_valid = 8,\n        min_test = 8,\n        aug_speed = 0.1,\n        aug_transpose = 5,\n        aug_remap = True,\n        freeze_rnn = False,\n        ):\n        \"\"\"TODO: Trainer __init__ docstring\"\"\"\n        kw = locals(); kw.pop('self')\n\n        # store all hyperparams for checkpointing\n        self.kw = kw\n\n        # get model defaults from model class\n        model_cls = Notochord\n        if model is None: model = {}\n        assert isinstance(model, dict), \"\"\"\n            model keywords are not a dict. check shell/fire syntax\n            \"\"\"\n        kw['model'] = model = get_class_defaults(model_cls) | model\n        model['num_pitches'] = 128\n        model['num_instruments'] = 320\n        # model['time_bounds'] = clamp_time\n\n        # assign all arguments to self by default\n        self.__dict__.update(kw)\n        # mutate some arguments:\n        self.model_dir = Path(model_dir) / self.experiment\n        self.log_dir = Path(log_dir) / self.experiment\n        self.results_dir = Path(results_dir) / self.experiment\n        self.data_dir = Path(data_dir)\n        self.device = torch.device(device)\n\n        # filesystem\n        for d in (self.model_dir, self.log_dir, self.results_dir):\n            d.mkdir(parents=True, exist_ok=True)\n\n        # random states\n        self.seed_random()\n\n        # logging\n        self.writer = SummaryWriter(self.log_dir)\n\n        # Trainer state\n        self.iteration = 0\n        self.exposure = 0\n        self.epoch = 0\n\n        # construct model from arguments \n        self.model = model_cls(**model).to(self.device)\n        tqdm.write(repr(self.model))\n\n        if freeze_rnn:\n            for n,p in self.model.rnn.named_parameters():\n                print(f'freezing {n}')\n                p.requires_grad_(False)\n\n        # dataset\n        if txala:\n            self.dataset = TxalaDataset(data_dir, self.batch_len, \n                remap=txala_remap, permute=txala_permute)\n        else:\n            self.dataset = MIDIDataset(data_dir, self.batch_len,\n                speed=aug_speed, transpose=aug_transpose, remap_instruments=aug_remap)\n\n        valid_len = max(min_valid, int(len(self.dataset)*0.03))\n        test_len = max(min_test, int(len(self.dataset)*0.02))\n        train_len = len(self.dataset) - valid_len - test_len\n        print(f'{valid_len=} {test_len=} {train_len=}')\n        self.train_dataset, self.valid_dataset, self.test_dataset = torch.utils.data.random_split(\n            self.dataset, [train_len, valid_len, test_len], \n            generator=torch.Generator().manual_seed(0))\n\n        # params = {k:v for k,v in self.model.named_parameters()}\n        # ks = ['projections.3.net.1.weight', 'projections.2.net.1.weight']\n        # slow_params = {k:params.pop(k) for k in ks}\n        # self.opt = torch.optim.AdamW([\n        #     {'params':params.values()},\n        #     {'params':slow_params.values(), 'lr':self.lr*1e-1}], \n        #     self.lr, self.adam_betas, self.adam_eps, self.weight_decay)\n        self.opt = torch.optim.AdamW(self.model.parameters(),\n            self.lr, self.adam_betas, self.adam_eps, self.weight_decay)\n\n    @property\n    def gpu(self):\n        return self.device.type!='cpu'\n\n    def seed_random(self):\n        random.seed(self.seed)\n        np.random.seed(self.seed)\n        torch.manual_seed(self.seed)\n\n    def set_random_state(self, states):\n        # note: GPU rng state not handled\n        std_state, np_state, torch_state = states\n        random.setstate(std_state)\n        np.random.set_state(np_state)\n        torch.set_rng_state(torch_state)\n\n    def save(self, fname):\n        torch.save(dict(\n            kw=self.kw,\n            model_state=self.model.state_dict(),\n            optimizer_state=self.opt.state_dict(),\n            step=(self.exposure, self.iteration, self.epoch),\n            random_state=(random.getstate(), np.random.get_state(), torch.get_rng_state())\n        ), fname)\n\n    def load_state(self, d, resume):\n        d = d if hasattr(d, '__getitem__') else torch.load(d)\n        self.model.load_state_dict(d['model_state'], strict=resume)\n        if resume:\n            print('loading optimizer state, RNG state, step counts')\n            print(\"\"\"\n            warning: optimizer lr, beta etc are restored with optimizer state,\n            even if different values given on the command line, when resume=True\n            \"\"\")\n            self.opt.load_state_dict(d['optimizer_state'])\n            self.exposure, self.iteration, self.epoch = d['step']\n            self.set_random_state(d['random_state'])\n        else:\n            print('fresh run transferring only model weights')\n\n    def log(self, tag, d):\n        # self.writer.add_scalars(tag, d, self.exposure)\n        for k,v in d.items():\n            self.writer.add_scalar(f'{tag}/{k}', v, self.exposure)\n\n    def process_grad(self):\n        r = {}\n        if self.grad_clip is not None:\n            r['grad_l2'] = torch.nn.utils.clip_grad_norm_(\n                self.model.parameters(), self.grad_clip, error_if_nonfinite=True)\n        return r\n\n    def get_loss_components(self, result, mask):\n        def reduce(k):\n            return result[k].masked_select(mask).mean()\n        return {\n            'instrument_nll': -reduce('instrument_log_probs'),\n            'pitch_nll': -reduce('pitch_log_probs'),\n            'time_nll': -reduce('time_log_probs'),\n            'velocity_nll': -reduce('velocity_log_probs'),\n            'end_nll': -reduce('end_log_probs'),\n        }\n\n    def _validate(self, valid_loader, ar_mask=None, testing=False):\n        \"\"\"\"\"\"\n        pops = defaultdict(list)\n        self.model.eval()\n        if testing:\n            self.dataset.testing = True\n        for batch in tqdm(valid_loader, desc=f'validating epoch {self.epoch}'):\n            # print(batch['mask'].shape, batch['mask'].sum())\n            mask = batch['mask'].to(self.device, non_blocking=True)[...,1:]\n            end = batch['end'].to(self.device, non_blocking=True)\n            inst = batch['instrument'].to(self.device, non_blocking=True)\n            pitch = batch['pitch'].to(self.device, non_blocking=True)\n            time = batch['time'].to(self.device, non_blocking=True)\n            vel = batch['velocity'].to(self.device, non_blocking=True)\n            with torch.no_grad():\n                result = self.model(\n                    inst, pitch, time, vel, end, \n                    validation=True, ar_mask=ar_mask)\n                losses = {k:v.item() for k,v in self.get_loss_components(\n                    result, mask).items()}\n                for k,v in losses.items():\n                    pops[k].append(v)\n                pops['loss'].append(sum(losses.values()))\n                pops['instrument_acc'].append(result['instrument_log_probs']\n                    .masked_select(mask).exp().mean().item())\n                pops['pitch_acc'].append(result['pitch_log_probs']\n                    .masked_select(mask).exp().mean().item())\n                pops['time_acc_30ms'].append(result['time_acc_30ms']\n                    .masked_select(mask).mean().item())\n                pops['velocity_acc'].append(result['velocity_log_probs']\n                    .masked_select(mask).exp().mean().item())\n        return {\n            'logs':{k:np.mean(v) for k,v in pops.items()},\n            # 'bootstraps':{\n            #     k:scipy.stats.bootstrap((v,), np.mean).confidence_interval \n            #     for k,v in pops.items()},\n            'pops':pops\n        }\n\n\n    def test(self):\n        \"\"\"Entry point to testing\"\"\"\n        # TODO: should make a test split before doing serious\n        # model comparison.\n        # ds = torch.utils.data.Subset(self.valid_dataset, [0,1,2])\n        ds = self.test_dataset\n        loader = DataLoader(\n            ds, 1,#self.batch_size,\n            shuffle=False, num_workers=self.n_jobs if self.gpu else 0, pin_memory=self.gpu)\n\n        results = []\n        for perm, mask in gen_masks(self.model.note_dim):\n            # TODO: bootstrap CI. need to return all likelihoods, not mean, from _validate\n            r = self._validate(\n                loader, ar_mask=mask.to(self.device, non_blocking=True),\n                testing=True)\n            # print(r['bootstraps'])\n            perm = [['instrument', 'pitch', 'time', 'velocity'][i] for i in perm]\n            results.append((perm, r['pops']))\n        torch.save(results, self.results_dir / f'result-{self.epoch:04d}.pt')\n\n    def train(self):\n        \"\"\"Entry point to model training\"\"\"\n        self.save(self.model_dir / f'{self.epoch:04d}.ckpt')\n\n        train_loader = DataLoader(\n            self.train_dataset, self.batch_size,\n            shuffle=True, num_workers=self.n_jobs, pin_memory=self.gpu)\n\n        valid_loader = DataLoader(\n            self.valid_dataset, self.batch_size,#//4,\n            shuffle=False, num_workers=self.n_jobs, pin_memory=self.gpu,\n            sampler=RandomSampler(\n                self.valid_dataset, \n                num_samples=self.batch_size, replacement=True))\n\n        ##### validation loop\n        def run_validation():\n            self.dataset.batch_len = self.dataset.max_test_len\n            logs = self._validate(valid_loader, testing=False)['logs']\n            self.log('valid', logs)\n\n        epoch_size = self.epoch_size or len(train_loader)\n\n        # validate at initialization\n        run_validation()\n\n        while True:\n            self.epoch += 1\n\n            ##### training loop\n            self.model.train()\n            self.dataset.testing = False\n            self.dataset.batch_len = self.batch_len\n            for batch in tqdm(\n                # itertools incantation to support epoch_size larger than train set\n                it.islice(\n                    it.chain.from_iterable(it.repeat(train_loader)), epoch_size), \n                desc=f'training epoch {self.epoch}', total=epoch_size\n                ):\n                mask = batch['mask'].to(self.device, non_blocking=True)\n                end = batch['end'].to(self.device, non_blocking=True)\n                inst = batch['instrument'].to(self.device, non_blocking=True)\n                pitch = batch['pitch'].to(self.device, non_blocking=True)\n                time = batch['time'].to(self.device, non_blocking=True)\n                vel = batch['velocity'].to(self.device, non_blocking=True)\n\n                self.iteration += 1\n                # TODO: use mask instead of batch dims\n                self.exposure += self.batch_size * self.batch_len\n                logs = {}\n\n                ### forward+backward+optimizer step ###\n                self.opt.zero_grad()\n                result = self.model(inst, pitch, time, vel, end)\n                losses = self.get_loss_components(result, mask[...,1:])\n                loss = sum(losses.values())\n                loss.backward()\n                logs |= self.process_grad()\n                self.opt.step()\n                ########\n\n                # log loss components\n                logs |= {k:v.item() for k,v in losses.items()}\n                # log total loss\n                logs |= {'loss':loss.item()}\n                # log any other returned scalars\n                logs |= {k:v.item() for k,v in result.items() if v.numel()==1}\n                self.log('train', logs)\n\n            run_validation()\n\n            if self.batch_len_schedule is not None:\n                self.batch_len = min(\n                    self.batch_len_max, self.batch_len+self.batch_len_schedule)\n                self.dataset.batch_len = self.batch_len\n\n            self.save(self.model_dir / f'{self.epoch:04d}.ckpt')\n</code></pre>"},{"location":"reference/notochord/train/#notochord.train.Trainer.__init__","title":"<code>__init__(experiment, model_dir, log_dir, data_dir, results_dir, model=None, batch_size=128, batch_len=64, batch_len_schedule=None, batch_len_max=512, lr=0.0003, adam_betas=(0.9, 0.999), adam_eps=1e-08, weight_decay=0.01, grad_clip=1.0, seed=0, n_jobs=1, device='cpu', epoch_size=None, txala=False, txala_remap=False, txala_permute=False, min_valid=8, min_test=8, aug_speed=0.1, aug_transpose=5, aug_remap=True, freeze_rnn=False)</code>","text":"<p>TODO: Trainer init docstring</p> Source code in <code>src/notochord/train.py</code> <pre><code>def __init__(self, \n    experiment, # experiment name\n    model_dir,\n    log_dir,\n    data_dir,\n    results_dir,\n    model = None, # dict of model constructor overrides\n    batch_size = 128,\n    batch_len = 64,\n    batch_len_schedule = None,\n    batch_len_max = 512,\n    lr = 3e-4,\n    adam_betas = (0.9, 0.999),\n    adam_eps = 1e-08, \n    weight_decay = 0.01,\n    grad_clip = 1.0,\n    seed = 0, # random seed\n    n_jobs = 1, # for dataloaders\n    device = 'cpu', # 'cuda:0'\n    epoch_size = None, # in iterations, None for whole dataset\n    txala = False,\n    txala_remap = False,\n    txala_permute = False,\n    min_valid = 8,\n    min_test = 8,\n    aug_speed = 0.1,\n    aug_transpose = 5,\n    aug_remap = True,\n    freeze_rnn = False,\n    ):\n    \"\"\"TODO: Trainer __init__ docstring\"\"\"\n    kw = locals(); kw.pop('self')\n\n    # store all hyperparams for checkpointing\n    self.kw = kw\n\n    # get model defaults from model class\n    model_cls = Notochord\n    if model is None: model = {}\n    assert isinstance(model, dict), \"\"\"\n        model keywords are not a dict. check shell/fire syntax\n        \"\"\"\n    kw['model'] = model = get_class_defaults(model_cls) | model\n    model['num_pitches'] = 128\n    model['num_instruments'] = 320\n    # model['time_bounds'] = clamp_time\n\n    # assign all arguments to self by default\n    self.__dict__.update(kw)\n    # mutate some arguments:\n    self.model_dir = Path(model_dir) / self.experiment\n    self.log_dir = Path(log_dir) / self.experiment\n    self.results_dir = Path(results_dir) / self.experiment\n    self.data_dir = Path(data_dir)\n    self.device = torch.device(device)\n\n    # filesystem\n    for d in (self.model_dir, self.log_dir, self.results_dir):\n        d.mkdir(parents=True, exist_ok=True)\n\n    # random states\n    self.seed_random()\n\n    # logging\n    self.writer = SummaryWriter(self.log_dir)\n\n    # Trainer state\n    self.iteration = 0\n    self.exposure = 0\n    self.epoch = 0\n\n    # construct model from arguments \n    self.model = model_cls(**model).to(self.device)\n    tqdm.write(repr(self.model))\n\n    if freeze_rnn:\n        for n,p in self.model.rnn.named_parameters():\n            print(f'freezing {n}')\n            p.requires_grad_(False)\n\n    # dataset\n    if txala:\n        self.dataset = TxalaDataset(data_dir, self.batch_len, \n            remap=txala_remap, permute=txala_permute)\n    else:\n        self.dataset = MIDIDataset(data_dir, self.batch_len,\n            speed=aug_speed, transpose=aug_transpose, remap_instruments=aug_remap)\n\n    valid_len = max(min_valid, int(len(self.dataset)*0.03))\n    test_len = max(min_test, int(len(self.dataset)*0.02))\n    train_len = len(self.dataset) - valid_len - test_len\n    print(f'{valid_len=} {test_len=} {train_len=}')\n    self.train_dataset, self.valid_dataset, self.test_dataset = torch.utils.data.random_split(\n        self.dataset, [train_len, valid_len, test_len], \n        generator=torch.Generator().manual_seed(0))\n\n    # params = {k:v for k,v in self.model.named_parameters()}\n    # ks = ['projections.3.net.1.weight', 'projections.2.net.1.weight']\n    # slow_params = {k:params.pop(k) for k in ks}\n    # self.opt = torch.optim.AdamW([\n    #     {'params':params.values()},\n    #     {'params':slow_params.values(), 'lr':self.lr*1e-1}], \n    #     self.lr, self.adam_betas, self.adam_eps, self.weight_decay)\n    self.opt = torch.optim.AdamW(self.model.parameters(),\n        self.lr, self.adam_betas, self.adam_eps, self.weight_decay)\n</code></pre>"},{"location":"reference/notochord/train/#notochord.train.Trainer.test","title":"<code>test()</code>","text":"<p>Entry point to testing</p> Source code in <code>src/notochord/train.py</code> <pre><code>def test(self):\n    \"\"\"Entry point to testing\"\"\"\n    # TODO: should make a test split before doing serious\n    # model comparison.\n    # ds = torch.utils.data.Subset(self.valid_dataset, [0,1,2])\n    ds = self.test_dataset\n    loader = DataLoader(\n        ds, 1,#self.batch_size,\n        shuffle=False, num_workers=self.n_jobs if self.gpu else 0, pin_memory=self.gpu)\n\n    results = []\n    for perm, mask in gen_masks(self.model.note_dim):\n        # TODO: bootstrap CI. need to return all likelihoods, not mean, from _validate\n        r = self._validate(\n            loader, ar_mask=mask.to(self.device, non_blocking=True),\n            testing=True)\n        # print(r['bootstraps'])\n        perm = [['instrument', 'pitch', 'time', 'velocity'][i] for i in perm]\n        results.append((perm, r['pops']))\n    torch.save(results, self.results_dir / f'result-{self.epoch:04d}.pt')\n</code></pre>"},{"location":"reference/notochord/train/#notochord.train.Trainer.train","title":"<code>train()</code>","text":"<p>Entry point to model training</p> Source code in <code>src/notochord/train.py</code> <pre><code>def train(self):\n    \"\"\"Entry point to model training\"\"\"\n    self.save(self.model_dir / f'{self.epoch:04d}.ckpt')\n\n    train_loader = DataLoader(\n        self.train_dataset, self.batch_size,\n        shuffle=True, num_workers=self.n_jobs, pin_memory=self.gpu)\n\n    valid_loader = DataLoader(\n        self.valid_dataset, self.batch_size,#//4,\n        shuffle=False, num_workers=self.n_jobs, pin_memory=self.gpu,\n        sampler=RandomSampler(\n            self.valid_dataset, \n            num_samples=self.batch_size, replacement=True))\n\n    ##### validation loop\n    def run_validation():\n        self.dataset.batch_len = self.dataset.max_test_len\n        logs = self._validate(valid_loader, testing=False)['logs']\n        self.log('valid', logs)\n\n    epoch_size = self.epoch_size or len(train_loader)\n\n    # validate at initialization\n    run_validation()\n\n    while True:\n        self.epoch += 1\n\n        ##### training loop\n        self.model.train()\n        self.dataset.testing = False\n        self.dataset.batch_len = self.batch_len\n        for batch in tqdm(\n            # itertools incantation to support epoch_size larger than train set\n            it.islice(\n                it.chain.from_iterable(it.repeat(train_loader)), epoch_size), \n            desc=f'training epoch {self.epoch}', total=epoch_size\n            ):\n            mask = batch['mask'].to(self.device, non_blocking=True)\n            end = batch['end'].to(self.device, non_blocking=True)\n            inst = batch['instrument'].to(self.device, non_blocking=True)\n            pitch = batch['pitch'].to(self.device, non_blocking=True)\n            time = batch['time'].to(self.device, non_blocking=True)\n            vel = batch['velocity'].to(self.device, non_blocking=True)\n\n            self.iteration += 1\n            # TODO: use mask instead of batch dims\n            self.exposure += self.batch_size * self.batch_len\n            logs = {}\n\n            ### forward+backward+optimizer step ###\n            self.opt.zero_grad()\n            result = self.model(inst, pitch, time, vel, end)\n            losses = self.get_loss_components(result, mask[...,1:])\n            loss = sum(losses.values())\n            loss.backward()\n            logs |= self.process_grad()\n            self.opt.step()\n            ########\n\n            # log loss components\n            logs |= {k:v.item() for k,v in losses.items()}\n            # log total loss\n            logs |= {'loss':loss.item()}\n            # log any other returned scalars\n            logs |= {k:v.item() for k,v in result.items() if v.numel()==1}\n            self.log('train', logs)\n\n        run_validation()\n\n        if self.batch_len_schedule is not None:\n            self.batch_len = min(\n                self.batch_len_max, self.batch_len+self.batch_len_schedule)\n            self.dataset.batch_len = self.batch_len\n\n        self.save(self.model_dir / f'{self.epoch:04d}.ckpt')\n</code></pre>"},{"location":"reference/notochord/util/","title":"Util","text":""},{"location":"reference/notochord/util/#notochord.util.arg_to_set","title":"<code>arg_to_set(x)</code>","text":"<p>convert None to empty set, iterable to set, or scalar to set with one item</p> Source code in <code>src/notochord/util.py</code> <pre><code>def arg_to_set(x):\n    \"\"\"convert None to empty set, iterable to set, or scalar to set with one item\"\"\"\n    if x is None:\n        return set()\n    elif not hasattr(x, '__iter__'):\n        return {x}\n    else:\n        return set(x)\n</code></pre>"},{"location":"reference/notochord/util/#notochord.util.deep_update","title":"<code>deep_update(a, b)</code>","text":"<p>in-place update a with contents of b, recursively for nested Mapping objects.</p> Source code in <code>src/notochord/util.py</code> <pre><code>def deep_update(a, b):\n    \"\"\"\n    in-place update a with contents of b, recursively for nested Mapping objects.\n    \"\"\"\n    for k in b:\n        if k in a and isinstance(a[k], Mapping) and isinstance(b[k], Mapping):\n            deep_update(a[k], b[k])\n        else:\n            a[k] = b[k]\n</code></pre>"},{"location":"reference/notochord/util/#notochord.util.gen_masks","title":"<code>gen_masks(n, dtype=torch.float)</code>","text":"<p>yield the autoregressive mask matrices of all permuations of n items</p> Source code in <code>src/notochord/util.py</code> <pre><code>def gen_masks(n, dtype=torch.float):\n    \"\"\"yield the autoregressive mask matrices of all permuations of n items\"\"\"\n    for perm in gen_perms(list(range(n))):\n        m = torch.zeros(n,n,dtype=dtype)\n        for idx,i in enumerate(perm):\n            for j in perm[:idx]:\n                m[j,i] = 1\n        yield perm, m\n</code></pre>"},{"location":"reference/notochord/util/#notochord.util.gen_perms","title":"<code>gen_perms(a)</code>","text":"<p>yield all permutations of the given list</p> Source code in <code>src/notochord/util.py</code> <pre><code>def gen_perms(a):\n    \"\"\"yield all permutations of the given list\"\"\"\n    if len(a)==1:\n        yield a\n    else:\n        # for each position\n        for i in range(len(a)):\n            # for permuations of remaining positions\n            for p in gen_perms(a[:i]+a[i+1:]):  \n                yield a[i:i+1]+p \n</code></pre>"},{"location":"reference/notochord/util/#notochord.util.get_class_defaults","title":"<code>get_class_defaults(cls)</code>","text":"<p>get the default argument values of a class constructor</p> Source code in <code>src/notochord/util.py</code> <pre><code>def get_class_defaults(cls):\n    \"\"\"get the default argument values of a class constructor\"\"\"\n    d = get_function_defaults(getattr(cls, '__init__'))\n    # ignore `self` argument, insist on default values\n    try:\n        d.pop('self')\n    except KeyError:\n        raise ValueError(\"\"\"\n            no `self` argument found in class __init__\n        \"\"\")\n    assert [v is not inspect._empty for v in d.values()], \"\"\"\n            get_class_defaults should be used on constructors with keyword arguments only.\n        \"\"\"\n    return d\n</code></pre>"},{"location":"reference/notochord/util/#notochord.util.get_function_defaults","title":"<code>get_function_defaults(fn)</code>","text":"<p>get dict of name:default for a function's arguments</p> Source code in <code>src/notochord/util.py</code> <pre><code>def get_function_defaults(fn):\n    \"\"\"get dict of name:default for a function's arguments\"\"\"\n    s = inspect.signature(fn)\n    return {k:v.default for k,v in s.parameters.items()}\n</code></pre>"},{"location":"reference/notochord/app/__init__/","title":"init","text":""},{"location":"reference/notochord/app/harmonizer/","title":"Harmonizer","text":"<p>Notochord MIDI harmonizer server. Each note from the player produces a harmonizing note from Notochord.</p> Authors <p>Victor Shepardson Intelligent Instruments Lab 2023</p>"},{"location":"reference/notochord/app/harmonizer/#notochord.app.harmonizer.NotoTUI","title":"<code>NotoTUI</code>","text":"<p>             Bases: <code>TUI</code></p> Source code in <code>src/notochord/app/harmonizer.py</code> <pre><code>class NotoTUI(TUI):\n    CSS_PATH = 'harmonizer.css'\n\n    BINDINGS = [\n        (\"m\", \"mute\", \"Mute Notochord\"),\n        (\"r\", \"reset\", \"Reset Notochord\")]\n\n    def compose(self):\n        \"\"\"Create child widgets for the app.\"\"\"\n        yield Header()\n        yield self.std_log\n        yield NotoLog(id='note')\n        yield NotoControl()\n        yield Footer()\n</code></pre>"},{"location":"reference/notochord/app/harmonizer/#notochord.app.harmonizer.NotoTUI.compose","title":"<code>compose()</code>","text":"<p>Create child widgets for the app.</p> Source code in <code>src/notochord/app/harmonizer.py</code> <pre><code>def compose(self):\n    \"\"\"Create child widgets for the app.\"\"\"\n    yield Header()\n    yield self.std_log\n    yield NotoLog(id='note')\n    yield NotoControl()\n    yield Footer()\n</code></pre>"},{"location":"reference/notochord/app/harmonizer/#notochord.app.harmonizer.main","title":"<code>main(checkpoint='notochord-latest.ckpt', player_channel=1, player_inst=1, noto_config=None, noto_channel=2, noto_inst=1, below=False, above=True, midi_in=None, midi_out=None, thru=False, send_pc=False, use_tui=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <p>path to notochord model checkpoint.</p> <code>'notochord-latest.ckpt'</code> <code>player_channel</code> <p>MIDI channels for player input.</p> <code>1</code> <code>player_inst</code> <p>Notochord instrument for the player.</p> <code>1</code> <code>noto_config</code> <code>Optional[List[Tuple[int, int, int, int]]]</code> <p>list voices played by Notochord. Each voice is a tuple of: ( MIDI channel indexed from 1, General MIDI instrument from 1, minimum transpose from the performed pitch, maximum transpose ). For example, [(2,1,-12,0), (3,13,0,12)] would play the grand piano on channel 2 in the octave below, and the marimba on channel 3 in the octave above. see https://en.wikipedia.org/wiki/General_MIDI for instrument numbers.</p> <code>None</code> <code>noto_channel</code> <p>alternative to using noto_config for a single voice</p> <code>2</code> <code>noto_inst</code> <p>alternative to using noto_config for a single voice</p> <code>1</code> <code>below</code> <p>alternative to using noto_config for a single voice -- allows harmonizing notes below the performed pitch</p> <code>False</code> <code>above</code> <p>alternative to using noto_config for a single voice -- allows harmonizing notes above the performed pitch</p> <code>True</code> <code>midi_in</code> <code>Optional[str]</code> <p>MIDI ports for player input.  default is to use all input ports. can be comma-separated list of ports.</p> <code>None</code> <code>midi_out</code> <code>Optional[str]</code> <p>MIDI ports for Notochord output.  default is to use only virtual 'From iipyper' port. can be comma-separated list of ports.</p> <code>None</code> <code>thru</code> <p>if True, copy incoming MIDI to output ports. only makes sense if input and output ports are different.</p> <code>False</code> <code>send_pc</code> <p>if True, send MIDI program change messages to set the General MIDI instrument according to player_inst, player_channel and noto_config. useful when using a General MIDI synthesizer like fluidsynth.</p> <code>False</code> <code>use_tui</code> <p>run textual UI.</p> <code>True</code> Source code in <code>src/notochord/app/harmonizer.py</code> <pre><code>def main(\n    checkpoint=\"notochord-latest.ckpt\", # Notochord checkpoint\n\n    player_channel=1, # MIDI channel numbered from 0\n    player_inst=1, # General MIDI numbered from 1 (see Notochord.feed docstring)\n    noto_config:Optional[List[Tuple[int,int,int,int]]]=None, # list of tuples of (channel, instrument, min transpose, max transpose)\n    noto_channel=2, # channel for single notochord voice (overriden by noto_config)\n    noto_inst=1, # instrument for single notochord voice (overridden by noto_config)\n    below=False, # harmonize above (overridden by noto_config)\n    above=True, # harmonize below (overridden by noto_config)\n\n    midi_in:Optional[str]=None, # MIDI port(s) for player input\n    midi_out:Optional[str]=None, # MIDI port(s) for Notochord output\n    thru=False, # copy player input to output\n    send_pc=False, # send program change messages to match player and noto_config (useful if using a General MIDI synth like fluidsynth or hardware)\n\n    use_tui=True,\n    ):\n    \"\"\"\n    Args:\n        checkpoint: path to notochord model checkpoint.\n\n        player_channel: MIDI channels for player input.\n        player_inst: Notochord instrument for the player.\n        noto_config: list voices played by Notochord. Each voice is a tuple of: (\n            MIDI channel indexed from 1,\n            General MIDI instrument from 1,\n            minimum transpose from the performed pitch,\n            maximum transpose\n            ).\n            For example, [(2,1,-12,0), (3,13,0,12)] would play the grand piano on channel 2 in the octave below, and the marimba on channel 3 in the octave above.\n            see https://en.wikipedia.org/wiki/General_MIDI for instrument numbers.\n        noto_channel: alternative to using noto_config for a single voice\n        noto_inst: alternative to using noto_config for a single voice\n        below: alternative to using noto_config for a single voice -- allows\n            harmonizing notes below the performed pitch\n        above: alternative to using noto_config for a single voice -- allows\n            harmonizing notes above the performed pitch\n\n        midi_in: MIDI ports for player input. \n            default is to use all input ports.\n            can be comma-separated list of ports.\n        midi_out: MIDI ports for Notochord output. \n            default is to use only virtual 'From iipyper' port.\n            can be comma-separated list of ports.\n        thru: if True, copy incoming MIDI to output ports.\n            only makes sense if input and output ports are different.\n        send_pc: if True, send MIDI program change messages to set the General MIDI\n            instrument according to player_inst, player_channel and noto_config.\n            useful when using a General MIDI synthesizer like fluidsynth.\n\n        use_tui: run textual UI.\n    \"\"\"\n        # nominal_time: if True, feed Notochord with its own predicted times\n        #     instead of the actual elapsed time.\n        #     May make Notochord more likely to play chords.\n    midi = MIDI(midi_in, midi_out)\n\n    ### Textual UI\n    tui = NotoTUI()\n    print = tui.print\n\n    def display_event(tag, inst, pitch, vel, channel, **kw):\n        \"\"\"print an event to the terminal\"\"\"\n        if tag is None:\n            return\n        s = f'{tag}:\\t {inst=:4d}    {pitch=:4d}    {vel=:4d}    {channel=:3d}'\n        tui(note=s)\n    ###\n\n    if noto_config is None:\n        if not below and not above:\n            raise ValueError\n        noto_config = [[\n            noto_channel-1, noto_inst, -128 if below else 1, 128 if above else -1]]\n    # convert to 0-index\n    player_channel = player_channel-1\n\n    # TODO: per-channel absolute range config\n\n    def warn_inst(i):\n        if i &gt; 128:\n            if i &lt; 257:\n                print(f\"WARNING: drum instrument {i} selected, be sure to select a drum bank in your synthesizer\")\n            else:\n                print(f\"WARNING: instrument {i} is not General MIDI\")\n\n    if send_pc:\n        warn_inst(player_inst)\n        midi.program_change(channel=player_channel, program=(player_inst-1)%128)\n        for (c,i,_,_) in noto_config:\n            warn_inst(i)\n            midi.program_change(channel=c, program=(i-1)%128)\n\n\n    for (_,_,lo,hi) in noto_config:\n        assert lo &lt;= hi, \"\"\"min transpose should be less than max transpose\"\"\"\n\n    noto = Notochord.from_checkpoint(checkpoint)\n    noto.eval()\n\n    history = NotoPerformance()\n    stopwatch = Stopwatch()\n\n    class AppState():\n        def __init__(self):\n            self.muted = False\n    state = AppState()\n\n    def noto_mute():\n        state.muted = not state.muted\n        print('MUTE' if state.muted else 'UNMUTE')\n\n    def noto_reset():\n        \"\"\"reset Notochord and end all of its held notes\"\"\"\n        print('RESET')\n\n        # end Notochord held notes\n        # noto_pairs = {(c,i) for (c,i,_,_) in noto_config}\n        # for (c,i,p) in history.note_triples:\n        #     if (c,i) in noto_pairs:\n        #         midi.note_off(note=p, velocity=0, channel=c)\n\n        # reset stopwatch\n        stopwatch.punch()\n        # reset notochord state\n        noto.reset()\n        # # reset history\n        # history.push()\n\n    @midi.handle(type='control_change', control=0, channel=player_channel)\n    def _(msg):\n        \"\"\"\n        any CC0 message on player channel resets Notochord\n        \"\"\"\n        noto.reset()\n\n    @midi.handle(type=('note_on', 'note_off'), channel=player_channel)\n    def _(msg):\n        \"\"\"\n        MIDI NoteOn events from the player\n        \"\"\"\n\n        pitch = msg.note\n        vel = msg.velocity\n\n        if thru:\n            midi.send(msg)\n\n        noto_range = (0,127) # TODO: config this\n\n        # NoteOn\n        if msg.type=='note_on' and vel &gt; 0:\n            dt = stopwatch.punch()\n            # track\n            event = dict(\n                channel=player_channel,\n                inst=player_inst, pitch=pitch, vel=vel, time=dt)\n            history.feed(**event)\n            # feed in the performed note\n            noto.feed(**event)\n            display_event('PLAYER', **event)\n\n            if state.muted:\n                return\n\n            for noto_channel, noto_inst, min_x, max_x in noto_config:\n\n                already_playing = {p for i,p in history.note_pairs if noto_inst==i}\n\n                lo, hi = noto_range\n                pitch_range = range(max(lo,pitch+min_x), min(hi, pitch+max_x+1))\n                pitches = (\n                    set(pitch_range) - {pitch} - already_playing\n                )\n\n                if len(pitches)==0:\n                    # edge case: no possible pitch\n                    print(f'skipping {noto_channel=}, no pitches available')\n                    print(pitch_range, 'minus', {pitch}, 'minus', already_playing)\n                    continue\n                elif len(pitches)==1:\n                    # edge case: there is exactly one possible pitch\n                    h = dict(\n                        inst=noto_inst, pitch=list(pitches)[0], \n                        time=0, vel=vel)\n                else:\n                    # notochord chooses pitch\n                    h = noto.query(\n                        next_inst=noto_inst, next_time=0, next_vel=vel,\n                        include_pitch=pitches)\n\n                h_inst = h['inst'] # noto_inst\n                h_pitch = h['pitch']\n                h_time = h['time'] # 0\n                h_vel = round(h['vel'])\n\n                # send it\n                midi.note_on(note=h_pitch, velocity=h_vel, channel=noto_channel)\n                # track\n                event = dict(\n                    channel=noto_channel,\n                    inst=h_inst, pitch=h_pitch, time=h_time, vel=h_vel)\n                history.feed(\n                    held_note_data=(player_inst, pitch), \n                    **event)\n                # feed back\n                noto.feed(**event)\n                display_event('NOTO', **event)\n        # NoteOff\n        else:\n            dt = stopwatch.punch()\n            event = dict(\n                channel=player_channel, \n                inst=player_inst, pitch=pitch, time=dt, vel=0)\n            noto.feed(**event)\n            history.feed(**event)\n            display_event('PLAYER', **event)\n\n            dependents = [\n                noto_k\n                for noto_k,player_k \n                in history.note_data.items()\n                if player_k==(player_inst, pitch)\n            ]\n\n            for noto_channel, noto_inst, noto_pitch in dependents:\n                # send harmonizing note offs\n                midi.note_off(\n                    note=noto_pitch, velocity=vel, channel=noto_channel)\n\n                event = dict(\n                    channel=noto_channel, \n                    inst=noto_inst, pitch=noto_pitch, time=dt, vel=0)\n                # TODO: nominal time option?\n                noto.feed(**event)\n                history.feed(**event)\n                display_event('NOTO', **event)\n\n    @cleanup\n    def _():\n        \"\"\"end any remaining notes\"\"\"\n        for (c,_,p) in history.note_triples:\n            midi.note_off(note=p, velocity=0, channel=c)\n\n    @tui.set_action\n    def mute():\n        noto_mute()\n\n    @tui.set_action\n    def reset():\n        noto_reset()\n\n    if use_tui:\n        tui.run()\n</code></pre>"},{"location":"reference/notochord/app/homunculus/","title":"Homunculus","text":"<p>Notochord MIDI co-improviser server. Notochord plays different instruments along with the player.</p>"},{"location":"reference/notochord/app/homunculus/#notochord.app.homunculus.NotoTUI","title":"<code>NotoTUI</code>","text":"<p>             Bases: <code>TUI</code></p> Source code in <code>src/notochord/app/homunculus.py</code> <pre><code>class NotoTUI(TUI):\n    CSS_PATH = 'homunculus.css'\n\n    BINDINGS = [\n        (\"m\", \"mute\", \"Mute Notochord\"),\n        (\"s\", \"sustain\", \"Sustain\"),\n        (\"q\", \"query\", \"Re-query Notochord\"),\n        (\"x\", \"stop\", \"Stop Notochord\"),\n        (\"r\", \"reset\", \"Reset Notochord\"),\n        (\"1\", \"preset1\", \"\"),\n        (\"2\", \"preset2\", \"\"),\n        (\"3\", \"preset3\", \"\"),\n        (\"4\", \"preset4\", \"\"),\n        (\"5\", \"preset5\", \"\"),\n        (\"6\", \"preset6\", \"\"),\n        (\"7\", \"preset7\", \"\"),\n        (\"8\", \"preset8\", \"\"),\n        (\"9\", \"preset9\", \"\"),\n        (\"0\", \"preset10\", \"\"),\n        ]\n\n    def compose(self):\n        \"\"\"Create child widgets for the app.\"\"\"\n        yield Header()\n        yield self.std_log\n        yield NotoLog(id='note')\n        yield NotoPrediction(id='prediction')\n        yield Mixer()\n        yield NotoPresets()\n        yield NotoControl()\n        yield Footer()\n\n    def on_mount(self) -&gt; None:\n        self.query_one(NotoPrediction).tooltip = \"displays the next predicted event\"\n        # print(self.screen)\n\n    def set_preset(self, idx, name):\n        if idx &lt; NotoPresets.n_presets:\n            node = self.query_one('#'+preset_id(idx))\n            node.label = str(idx) if name is None else name\n        else:\n            self.write(f'warning: more than {NotoPresets.n_presets} presets\\n')\n        # node.label = name\n\n    def set_channel(self, chan, cfg):\n        # print(f'set_channel {cfg}')\n        inst_node = self.query_one('#'+inst_id(chan))\n        mode_node = self.query_one('#'+mode_id(chan))\n        mute_node = self.query_one('#'+mute_id(chan))\n\n        if cfg is None:\n            inst_node.variant = 'default'\n            mute_node.variant = 'default'\n            mode_node.variant = 'default'\n            return\n\n        mode = cfg['mode']\n        mute = cfg['mute']\n        inst = cfg['inst']\n\n        inst_node.label = inst_label(inst)\n\n        if mode=='auto':\n            mode_node.label = f\"{chan:02d}\"\n        elif mode=='input':\n            mode_node.label = f\"--&gt;{chan:02d}\"\n        elif mode=='follow':\n            mode_node.label = f\"{cfg['source']:02d}-&gt;{chan:02d}\"\n\n        if mute:\n            mode_node.variant = 'default'\n            inst_node.variant = 'default'\n            mute_node.label = 'UNMUTE'\n            mute_node.variant = 'error'\n        else:\n            mode_node.variant = 'primary'\n            inst_node.variant = 'warning'\n            mute_node.label = 'MUTE'\n            mute_node.variant = 'default'\n</code></pre>"},{"location":"reference/notochord/app/homunculus/#notochord.app.homunculus.NotoTUI.compose","title":"<code>compose()</code>","text":"<p>Create child widgets for the app.</p> Source code in <code>src/notochord/app/homunculus.py</code> <pre><code>def compose(self):\n    \"\"\"Create child widgets for the app.\"\"\"\n    yield Header()\n    yield self.std_log\n    yield NotoLog(id='note')\n    yield NotoPrediction(id='prediction')\n    yield Mixer()\n    yield NotoPresets()\n    yield NotoControl()\n    yield Footer()\n</code></pre>"},{"location":"reference/notochord/app/homunculus/#notochord.app.homunculus.main","title":"<code>main(checkpoint='notochord-latest.ckpt', config=None, preset=None, preset_file=None, midi_prompt=None, prompt_config=True, prompt_channel_order=None, prompt_merge=False, seed=0, initial_mute=False, initial_query=None, initial_stop=False, midi_in=None, midi_out=None, midi_control=None, thru=False, send_pc=False, dump_midi=False, suppress_midi_feedback=True, input_channel_map=None, stop_on_end=False, reset_on_end=False, end_exponent=1, min_end_time=8, balance_sample=False, n_recent=32, n_margin=8, max_note_len=11, max_time=None, nominal_time=True, min_vel=None, max_vel=None, osc_port=None, osc_host='', punch_in=False, punch_out_after=1.0, use_tui=True, predict_input=True, predict_follow=False, debug_query=False, testing=False, estimated_query_latency=0.01, estimated_feed_latency=0.01, lateness_margin=0.1, soft_lateness_margin=0.05, soundfont=None, limit_input=None, profiler=0, wipe_presets=False)</code>","text":"<p>This a terminal app for using Notochord interactively with MIDI controllers and synthesizers. It allows combining both 'harmonizer' and 'improviser' features.</p> <p>Arguments to main can be given on the command line as flags, for example:</p> <p><code>notochord homunculus --config \"{1:{mode:auto, inst:1, mute:False}}\" --initial-query --max-time 1</code></p> <p>This says: play the grand piano autonomously on channel 1, start automatically, allow no more than 1 second between MIDI events. A different example:</p> <p><code>notochord homunculus --config \"{1:{mode:input, inst:1, mute:False}, 2:{mode:follow, inst:12, mute:False}}\" --thru --send-pc</code></p> <p>This says, take grand piano input on channel 1, and harmonize it with vibraphone on channel 2. Pass input through to the output, and also send program change messages to set the instruments on the synthesizer.</p> <p>You may also need to use the --midi-in or --midi-out flags to get MIDI to the right place.</p>"},{"location":"reference/notochord/app/homunculus/#notochord.app.homunculus.main--midi-channels","title":"MIDI Channels","text":"<p>In homunculus, you have 16 MIDI channels. Each channel can be in one of three modes:</p> <pre><code>* input (appearing like \"--&gt;01\"), channel 1 comes from MIDI input channel 1.\n* follow (appearing like \"01-&gt;02\"), channel 2 plays whenever channel 1 plays.\n* auto (appearing like just \"03\"), channel 3 plays autonomously.\n</code></pre> <p>Click the top section of each channel strip to cycle the mode.</p> <p>Each channel is also assigned a General MIDI instrument. Each 'input' and 'auto' channel should have a unique General MIDI instrument, but 'follow' channels can be duplicates of others. If you try to assign duplicate instruments, they will automatically change to \"anonymous\" melodic or drum instruments, but still send the right program change messages when using --send-pc.</p> <p>Click the middle section of each channel strip to choose a new instrument.</p> <p>The bottom section of each channel strip allows muting individual voices.</p>"},{"location":"reference/notochord/app/homunculus/#notochord.app.homunculus.main--global-controls","title":"Global controls","text":"<p>Along the bottom, there are global query, sustain, mute and reset buttons. Query manually replaces the next pending note. Sustain stops all auto voices from playing without ending any open notes. Mute ends all open notes and stops auto voices. Reset ends all open notes, forgets all context and sets the Notochord model to its initial state.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <p>path to notochord model checkpoint.</p> <code>'notochord-latest.ckpt'</code> <code>config</code> <code>Dict[int, Dict[str, Any]]</code> <p>mapping from MIDI channels to voice specs. MIDI channels and General MIDI instruments are indexed from 1. see wikipedia.org/wiki/General_MIDI for values. There are 3 modes of voice, 'auto', 'follow' and 'input'. For example, <pre><code>{\n    1:{\n        'mode':'input', 'inst':1\n    }, # input grand piano on MIDI channel 1\n    2:{\n        'mode':'follow', 'source':1, 'inst':1, \n        'transpose':(-12,12)\n    }, # harmonize the channel within 1 octave 1 with more piano\n    3:{\n        'mode':'auto', 'inst':12, 'range':(36,72)\n    }, # autonomous vibraphone voice in the MIDI pitch 36-72 range\n    10:{\n        'mode':'auto', 'inst':129,\n    }, # autonomous drums voice\n    4:{\n        'mode':'follow', 'source':3, 'inst':10, 'range':(72,96)\n    }, # harmonize channel 3 within upper registers of the glockenspiel\n}\n</code></pre> Notes: when two channels use the same instrument, one will be converted to an 'anonymous' instrument number (but it will still respect  the chosen instrument when using --send-pc)</p> <code>None</code> <code>preset</code> <code>str</code> <p>preset name (in preset file) to load config from</p> <code>None</code> <code>preset_file</code> <code>Path</code> <p>path to a TOML file containing channel presets the default config file is <code>homunculus.toml</code>;  running <code>notochord files</code> will show its location.</p> <code>None</code> <code>midi_prompt</code> <code>Path</code> <p>path to a MIDI file to read in as a prompt. note that prompts can alternatively be associated with presets  in <code>homunculus.toml</code></p> <code>None</code> <code>prompt_config</code> <code>bool</code> <p>if True, set unmuted instruments to those in the prompt file. prompt config overrides presets, but config from the <code>--condfig flag</code> will override the prompt</p> <code>True</code> <code>prompt_channel_order</code> <code>str</code> <p>method to re-order the channels in a MIDI prompt 'channel' (default, leave as they are in file) 'instrument': sort by instrument ID      (keeping associated anonymous IDs together) 'notes': sort by most notes</p> <code>None</code> <code>prompt_merge</code> <code>bool</code> <p>if True, merge multiple channels with the same GM program into one Notochord voice (with overlapping notes dropped or shortened) if False, use anonymous instruments.</p> <code>False</code> <code>seed</code> <code>int</code> <p>global random seed (added to preset random seed)</p> <code>0</code> <code>initial_mute</code> <p>start 'auto' voices muted so it won't play with input.</p> <code>False</code> <code>initial_stop</code> <p>if True, begin in a stopped state,  so 'auto' voices don't begin playing without input.</p> <code>False</code> <code>midi_in</code> <code>Optional[str]</code> <p>MIDI ports for input.  default is to use all input ports. can be comma-separated list of ports.</p> <code>None</code> <code>midi_out</code> <code>Optional[str]</code> <p>MIDI ports for output.  default is to use only virtual 'From iipyper' port. can be comma-separated list of ports.</p> <code>None</code> <code>thru</code> <p>if True, copy input MIDI to output ports. only makes sense if input and output ports are different.</p> <code>False</code> <code>send_pc</code> <p>if True, send MIDI program change messages to set the General MIDI instrument on each channel according to player_config and noto_config. useful when using a General MIDI synthesizer like fluidsynth.</p> <code>False</code> <code>dump_midi</code> <p>if True, print all incoming MIDI for debugging purposes</p> <code>False</code> <code>suppress_midi_feedback</code> <p>attempt to allow use of the one loopback port for both input and output, by ignoring any MIDI input which is identical to an output within a few milliseconds.</p> <code>True</code> <code>balance_sample</code> <p>choose 'auto' voices which have played less recently, ensures that all configured instruments will play.</p> <code>False</code> <code>n_recent</code> <p>number of recent note-on events to consider for above</p> <code>32</code> <code>n_margin</code> <p>controls the amount of slack in the balance_sample calculation</p> <code>8</code> <code>max_note_len</code> <p>time in seconds after which to force-release sustained 'auto' notes.</p> <code>11</code> <code>max_time</code> <p>maximum seconds between predicted events for 'auto' voices. default is the Notochord model's maximum (usually 10 seconds).</p> <code>None</code> <code>lateness_margin</code> <p>when events are playing later than this (in seconds), slow down</p> <code>0.1</code> <code>min_vel</code> <p>mininum velocity (except for noteOffs where vel=0)</p> <code>None</code> <code>max_vel</code> <p>maximum velocity</p> <code>None</code> <code>osc_port</code> <p>optional. if supplied, listen for OSC to set controls</p> <code>None</code> <code>osc_host</code> <p>hostname or IP of OSC sender. leave this as empty string to get all traffic on the port</p> <code>''</code> <code>punch_in</code> <p>EXPERIMENTAL. this causes all channels to switch between auto and input mode when input is received or not</p> <code>False</code> <code>punch_out_after</code> <p>time in seconds from last input NoteOn to revert to  auto mode</p> <code>1.0</code> <code>use_tui</code> <p>run textual UI.</p> <code>True</code> <code>predict_input</code> <p>forecasted next events can be for 'input' voices. generally should be True for manual input; use balance_sample to force 'auto' voices to play.  you might want it False if you have a very busy input.</p> <code>True</code> <code>predict_follow</code> <p>ditto for 'follow' voices. less obvious what the correct setting is, but False will be more efficient</p> <code>False</code> <code>wipe_presets</code> <p>if True, replaces your homunulus.toml with the defaults (may be useful after updating notochord)</p> <code>False</code> <code>soundfont</code> <p>path to a soundfont file from which default instrument  ranges will be loaded</p> <code>None</code> <code>wipe_presets</code> <p>CAUTION. replaces your homunculus.toml file with the  default. may fix errors after upgrading. backup your config file if you've been changing it! (<code>notochord files</code> to find it)</p> <code>False</code> Source code in <code>src/notochord/app/homunculus.py</code> <pre><code>def main(\n    checkpoint=\"notochord-latest.ckpt\", # Notochord checkpoint\n    config:Dict[int,Dict[str,Any]]=None, # map MIDI channel : GM instrument\n    preset:str=None,\n    preset_file:Path=None,\n\n    midi_prompt:Path=None,\n    prompt_config:bool=True, # set channel config based on prompt\n    prompt_channel_order:str=None,\n    prompt_merge:bool=False,\n\n    seed:int=0,\n\n    initial_mute=False, # start with Notochord muted\n    initial_query=None, # DEPRECATED, now inverse of `initial_stop`\n    initial_stop=False, # if False, auto voices play immediately\n\n    midi_in:Optional[str]=None, # MIDI port for player input\n    midi_out:Optional[str]=None, # MIDI port for Notochord output\n    midi_control:Optional[str]=None, # MIDI port for note messages as controls\n    thru=False, # copy player input to output\n    send_pc=False, # send program change messages\n    dump_midi=False, # print all incoming MIDI\n    suppress_midi_feedback=True,\n    input_channel_map=None,\n\n    stop_on_end=False, # auto channels stop when end is sampled\n    reset_on_end=False, # reset notochord when end is sampled\n    end_exponent=1, # &lt; 1 makes end more likely, &gt;1 less likely\n    min_end_time=8, # prevent sampling end before this many seconds since reset\n\n    balance_sample=False, # choose instruments which have played less recently\n    n_recent=32, # number of recent note-on events to consider for above\n    n_margin=8, # amount of 'slack' in the balance_sample calculation\n\n    max_note_len=11, # in seconds, to auto-release stuck Notochord notes\n    max_time=None, # max time between events\n    nominal_time=True, #DEPRECATED\n\n    min_vel=None, #mininum velocity (besides noteOffs)\n    max_vel=None, # maximum velocity\n\n    osc_port=None, # if supplied, listen for OSC to set controls on this port\n    osc_host='', # leave this as empty string to get all traffic on the port\n\n    punch_in=False, # EXPERIMENTAL. this causes all channels to switch between auto and input mode when input is received or not\n    punch_out_after=1.0, # time in seconds from last input to revert to auto\n\n    use_tui=True, # run textual UI\n    predict_input=True, # forecasted next events can be for input (preserves model distribution, but can lead to Notochord deciding not to play)\n    predict_follow=False,\n    debug_query=False, # don't query notochord when there is no pending event.\n    testing=False,\n    estimated_query_latency=10e-3,\n    estimated_feed_latency=10e-3,\n    lateness_margin=100e-3, # when events are playing later than this, slow down\n    soft_lateness_margin=50e-3, # when events are playing later than this, slow down\n    soundfont=None,\n    limit_input=None,\n    # thru_vel_offset=None,\n    profiler=0,\n    wipe_presets=False,\n    ):\n    \"\"\"\n    This a terminal app for using Notochord interactively with MIDI controllers and synthesizers. It allows combining both 'harmonizer' and 'improviser' features.\n\n    Arguments to main can be given on the command line as flags, for example:\n\n    `notochord homunculus --config \"{1:{mode:auto, inst:1, mute:False}}\" --initial-query --max-time 1`\n\n    This says: play the grand piano autonomously on channel 1, start automatically, allow no more than 1 second between MIDI events. A different example:\n\n    `notochord homunculus --config \"{1:{mode:input, inst:1, mute:False}, 2:{mode:follow, inst:12, mute:False}}\" --thru --send-pc`\n\n    This says, take grand piano input on channel 1, and harmonize it with vibraphone on channel 2. Pass input through to the output, and also send program change messages to set the instruments on the synthesizer.\n\n    You may also need to use the --midi-in or --midi-out flags to get MIDI to the right place.\n\n    # MIDI Channels\n\n    In homunculus, you have 16 MIDI channels. Each channel can be in one of three modes:\n\n        * input (appearing like \"--&gt;01\"), channel 1 comes from MIDI input channel 1.\n        * follow (appearing like \"01-&gt;02\"), channel 2 plays whenever channel 1 plays.\n        * auto (appearing like just \"03\"), channel 3 plays autonomously.\n\n    Click the top section of each channel strip to cycle the mode.\n\n    Each channel is also assigned a [General MIDI instrument](https://en.wikipedia.org/wiki/General_MIDI#Program_change_events). Each 'input' and 'auto' channel should have a unique General MIDI instrument, but 'follow' channels can be duplicates of others. If you try to assign duplicate instruments, they will automatically change to \"anonymous\" melodic or drum instruments, but still send the right program change messages when using --send-pc.\n\n    Click the middle section of each channel strip to choose a new instrument.\n\n    The bottom section of each channel strip allows muting individual voices.\n\n    # Global controls\n\n    Along the bottom, there are global query, sustain, mute and reset buttons. Query manually replaces the next pending note. Sustain stops all auto voices from playing without ending any open notes. Mute ends all open notes and stops auto voices. Reset ends all open notes, forgets all context and sets the Notochord model to its initial state.\n\n    Args:\n        checkpoint: path to notochord model checkpoint.\n\n        config: \n            mapping from MIDI channels to voice specs.\n            MIDI channels and General MIDI instruments are indexed from 1.\n            see wikipedia.org/wiki/General_MIDI for values.\n            There are 3 modes of voice, 'auto', 'follow' and 'input'. For example,\n            ```\n            {\n                1:{\n                    'mode':'input', 'inst':1\n                }, # input grand piano on MIDI channel 1\n                2:{\n                    'mode':'follow', 'source':1, 'inst':1, \n                    'transpose':(-12,12)\n                }, # harmonize the channel within 1 octave 1 with more piano\n                3:{\n                    'mode':'auto', 'inst':12, 'range':(36,72)\n                }, # autonomous vibraphone voice in the MIDI pitch 36-72 range\n                10:{\n                    'mode':'auto', 'inst':129,\n                }, # autonomous drums voice\n                4:{\n                    'mode':'follow', 'source':3, 'inst':10, 'range':(72,96)\n                }, # harmonize channel 3 within upper registers of the glockenspiel\n            }\n            ```\n            Notes:\n            when two channels use the same instrument, one will be converted\n            to an 'anonymous' instrument number (but it will still respect \n            the chosen instrument when using --send-pc)\n\n        preset: \n            preset name (in preset file) to load config from\n        preset_file: \n            path to a TOML file containing channel presets\n            the default config file is `homunculus.toml`; \n            running `notochord files` will show its location.\n\n        midi_prompt:\n            path to a MIDI file to read in as a prompt.\n            note that prompts can alternatively be associated with presets \n            in `homunculus.toml`\n        prompt_config:\n            if True, set unmuted instruments to those in the prompt file.\n            prompt config overrides presets,\n            but config from the `--condfig flag` will override the prompt\n        prompt_channel_order:\n            method to re-order the channels in a MIDI prompt\n            'channel' (default, leave as they are in file)\n            'instrument': sort by instrument ID \n                (keeping associated anonymous IDs together)\n            'notes': sort by most notes\n        prompt_merge:\n            if True, merge multiple channels with the same GM program into one\n            Notochord voice (with overlapping notes dropped or shortened)\n            if False, use anonymous instruments.\n\n        seed:\n            global random seed (added to preset random seed)\n\n        initial_mute: \n            start 'auto' voices muted so it won't play with input.\n        initial_stop: \n            if True, begin in a stopped state, \n            so 'auto' voices don't begin playing without input.\n\n        midi_in: \n            MIDI ports for input. \n            default is to use all input ports.\n            can be comma-separated list of ports.\n        midi_out: \n            MIDI ports for output. \n            default is to use only virtual 'From iipyper' port.\n            can be comma-separated list of ports.\n        thru: \n            if True, copy input MIDI to output ports.\n            only makes sense if input and output ports are different.\n        send_pc: \n            if True, send MIDI program change messages to set the General MIDI\n            instrument on each channel according to player_config and noto_config.\n            useful when using a General MIDI synthesizer like fluidsynth.\n        dump_midi: \n            if True, print all incoming MIDI for debugging purposes\n        suppress_midi_feedback:\n            attempt to allow use of the one loopback port for both input and\n            output, by ignoring any MIDI input which is identical to an output\n            within a few milliseconds.\n\n        balance_sample:\n            choose 'auto' voices which have played less recently,\n            ensures that all configured instruments will play.\n        n_recent: \n            number of recent note-on events to consider for above\n        n_margin: \n            controls the amount of slack in the balance_sample calculation\n\n        max_note_len: \n            time in seconds after which to force-release sustained 'auto' notes.\n        max_time: \n            maximum seconds between predicted events for 'auto' voices.\n            default is the Notochord model's maximum (usually 10 seconds).\n        lateness_margin:\n            when events are playing later than this (in seconds), slow down\n\n        min_vel: mininum velocity (except for noteOffs where vel=0)\n        max_vel: maximum velocity\n\n        osc_port: \n            optional. if supplied, listen for OSC to set controls\n        osc_host: \n            hostname or IP of OSC sender.\n            leave this as empty string to get all traffic on the port\n\n        punch_in: EXPERIMENTAL. this causes all channels to switch between\n            auto and input mode when input is received or not\n        punch_out_after: time in seconds from last input NoteOn to revert to \n            auto mode\n\n        use_tui: \n            run textual UI.\n        predict_input: \n            forecasted next events can be for 'input' voices.\n            generally should be True for manual input;\n            use balance_sample to force 'auto' voices to play. \n            you might want it False if you have a very busy input.\n        predict_follow:\n            ditto for 'follow' voices. less obvious what the correct setting\n            is, but False will be more efficient\n\n        wipe_presets:\n            if True, replaces your homunulus.toml with the defaults\n            (may be useful after updating notochord)\n\n        soundfont: path to a soundfont file from which default instrument \n            ranges will be loaded\n\n        wipe_presets: CAUTION. replaces your homunculus.toml file with the \n            default. may fix errors after upgrading. backup your config file\n            if you've been changing it! (`notochord files` to find it)\n    \"\"\"\n    if osc_port is not None:\n        osc = OSC(osc_host, osc_port)\n    if midi_in is None and midi_control is not None:\n        midi_in = set(mido.get_input_names()) - set(midi_control.split(','))\n    midi = MIDI(midi_in, midi_out, suppress_feedback=suppress_midi_feedback)\n    if midi_control is not None:\n        midi_control = MIDI(\n            midi_control, midi_control, \n            virtual_in_ports=0, virtual_out_ports=0)\n\n    # backwards compat\n    if initial_query is not None:\n        initial_stop = not initial_query\n\n    estimated_latency = estimated_feed_latency + estimated_query_latency\n\n    if input_channel_map is None: input_channel_map = {}\n\n    ### Textual UI\n    tui = NotoTUI()\n    print = notochord.print = iipyper.print = tui.print\n    ###\n\n    if soundfont is None:\n        sf_inst_ranges = {}\n    else:\n        # attempt to get instrument ranges from the soundfont\n        # assumes first bank is used\n        # not sure if entirely correct\n        from sf2utils.sf2parse import Sf2File\n        from sf2utils.generator import Sf2Gen\n\n        with open(soundfont, 'rb') as file:\n            soundfont = Sf2File(file)\n        sf_presets = {\n            p.preset:p\n            for p in soundfont.presets \n            if hasattr(p,'bank') and p.bank==0}\n        sf_drum_presets = {\n            p.preset:p\n            for p in soundfont.presets \n            if hasattr(p,'bank') and p.bank==128}\n\n        def _get_range(i):\n            if i&gt;128:\n                if i-128 in sf_drum_presets:\n                    p = sf_drum_presets[i-128]\n                else:\n                    p = sf_drum_presets[0]\n            else:\n                if i-1 in sf_presets:\n                    p = sf_presets[i-1]\n                else:\n                    p = sf_presets[0]\n            # union of bags in preset\n            # bag: intersection of range with union of bags in instrument\n            preset_range = set()\n            for b in p.bags:\n                bag_range = set()\n                if Sf2Gen.OPER_INSTRUMENT in b.gens: \n                    inst = soundfont.instruments[b[Sf2Gen.OPER_INSTRUMENT].amount]\n                    for bi in inst.bags:\n                        if Sf2Gen.OPER_SAMPLE_ID in bi.gens:\n                            if bi.key_range is not None:\n                                l,h = bi.key_range\n                                bag_range |= set(range(l, h+1))\n                if b.key_range is not None: \n                    # print(b.key_range)\n                    l,h = b.key_range\n                    bag_range &amp;= set(range(l, h+1))\n                preset_range |= bag_range\n            # print(f'{preset_range=}')\n            if len(preset_range):\n                return min(preset_range), max(preset_range)\n            return 0, 127\n        sf_inst_ranges = {i:_get_range(i) for i in range(1,257)}\n    def get_range(i):\n        return sf_inst_ranges.get(i, (0,127))\n\n    # print(f'{sf_inst_ranges=}')\n\n    if not nominal_time:\n        print('nominal_time is deprecated; nominal_time=False now sets lateness_margin to 0')\n        lateness_margin = 0\n\n    # make preset file if it doesn't exist\n    cfg_dir = Notochord.user_data_dir()\n    default_preset_file = cfg_dir / 'homunculus.toml'\n    src_preset_file = Path(__file__).parent / 'homunculus.toml'\n    if wipe_presets or not default_preset_file.exists():\n        shutil.copy(src_preset_file, default_preset_file)\n\n    ### presets and config\n    try:    \n        if preset_file is None:\n            global_config = toml_file.Config(str(default_preset_file))\n        else:\n            global_config = toml_file.Config(str(preset_file))\n        presets = global_config.get('preset', {})\n        control_meta = global_config.get('control', {})\n        action_meta = global_config.get('action', {})\n\n    except Exception:\n        print('WARNING: failed to load presets from file')\n        print(traceback.print_exc(file=tui))\n        global_config = {}\n        presets = {}\n        control_meta = []\n        action_meta = []\n\n    # print(f'{global_config=}')\n    # print(f'{presets=}')\n\n    # store control values\n    controls = {ctrl['name']:ctrl.get('value', None) for ctrl in control_meta}\n\n    # load notochord model\n    try:\n        noto = Notochord.from_checkpoint(checkpoint)\n        noto.eval()\n        noto.feed(0,0,0,0) # smoke test\n        noto.query()\n        noto.reset()\n    except Exception:\n        print(\"\"\"error loading notochord model\"\"\")\n        raise\n\n    # defaults\n    def default_config_channel(i):\n        \"\"\"default values for presets\n        all fields should appear here, even if default is None\n        \"\"\"\n        d = global_config.get('default', {})\n        # toml_file's get method also sets the value for some reason??\n        # and it refuses to set a None value...\n        def get(k, default):\n            return d[k] if k in d else default\n        return {\n            'mode': get('mode', 'auto'), \n            'inst': noto.first_anon_like(1),\n            'mute': get('mute', True), \n            'source': max(1,i-1), \n            'note_shift': get('note_shift', 0),\n            'duration': get('duration', None),\n            'poly': get('poly', None),\n            'range': get('range', None),\n            'transpose': get('transpose', None),\n            'cc': get('cc', toml_file.Config())\n            }\n\n    # convert MIDI channels to int\n    # print(presets)\n    for p in presets:\n        if 'channel' not in p:\n            p['channel'] = {}\n        else:\n            p['channel'] = {int(k):v for k,v in p['channel'].items()}   \n\n    ### this feeds all events from the prompt file to notochord\n    def do_prompt(prompt_file, channel_order=None, merge_channels=False, seed=None):\n        prompt_random = random.Random(seed)\n        prompt_file = Path(prompt_file).expanduser()\n        if prompt_file.is_dir():\n            prompt_file = prompt_random.choice([\n                p for p in \n                prompt_file.glob('**/*')\n                if p.is_file()\n                ])\n        noto.reset()\n        initial_state, inst_data = noto.prompt(\n            prompt_file, merge=merge_channels)\n        # first get the 16 most active parts\n        insts = sorted(inst_data, key=lambda x: -inst_data[x].notes)[:16]\n        # then order them onto channels\n        def make_cfg(i):\n            return {\n                'inst':i, 'mute':False, \n                'program_inst':inst_data[i].orig_inst}\n        if channel_order=='instrument':\n            # sort by (original_inst, noto_inst)\n            config = {\n                c+1:make_cfg(i)\n                for c,i in enumerate(sorted(\n                    insts, key=lambda k: (inst_data[k].orig_inst, k)))}\n        elif channel_order=='notes':\n            config = {c+1:make_cfg(i) for c,i in enumerate(insts)}\n        elif channel_order=='channel' or channel_order is None:\n            # NOTE this takes the minimum channel if an instrument\n            # appears on multiple channels\n            # the instrument with most notes will take precedence\n            # if multiple instruments appear\n            # but could be better to avoid collisions in some cases...\n            config = {\n                min(inst_data[i].channels)+1:make_cfg(i) \n                for i in reversed(insts)}\n        else:\n            raise ValueError(f\"\"\"\n                unknown {channel_order=} \n                (options are 'instrument', 'notes', or 'channel')\"\"\")\n        return initial_state, config\n\n    global_initial_state = None\n    config_ingest = None\n    if midi_prompt is not None:\n        global_initial_state, config_ingest = do_prompt(\n            midi_prompt, \n            merge_channels=prompt_merge, channel_order=prompt_channel_order)\n        if not prompt_config:\n            config_ingest = None\n    initial_state = global_initial_state\n\n    # process prompts in each preset\n    for p in presets:\n        if 'prompt' in p:\n            print(f'prompting \"{p[\"prompt\"]}\" for preset \"{p[\"name\"]}\"')\n            prompt_seed = p.get('seed')\n            if seed is None and p.get('seed') is None:\n                prompt_seed = None\n            else:\n                prompt_seed = (seed or 0) + p.get('seed', 0)\n            p['initial_state'], prompt_cfg = do_prompt(\n                p['prompt'], \n                merge_channels=p.get('prompt_merge_channels'),\n                channel_order=p.get('prompt_channel_order'),\n                seed=prompt_seed\n                )\n            # print(f'{prompt_cfg=}')\n            preset_cfg = p['channel']\n            for k,chan_cfg in prompt_cfg.items():\n                chan_cfg.update(preset_cfg.get(k, {}))\n                preset_cfg[k] = chan_cfg\n            print(f\"preset channel config: {preset_cfg}\")\n\n    # def validate_config():\n    #     assert all(\n    #         v['source'] in config for v in config.values() if v['mode']=='follow'\n    #         ), 'ERROR: no source given for follow voice'\n    #     # TODO: check for follow cycles\n    # validate_config()\n\n    def mode_insts(t, allow_muted=True):\n        if isinstance(t, str):\n            t = t,\n        # set of instruments with given mode(s)\n        return {\n            v['inst'] for v in config.values() \n            if v['mode'] in t and (allow_muted or not v['mute'])\n            }\n    def mode_chans(t):\n        if isinstance(t, str):\n            t = t,\n        # list of channels with given mode\n        return [k for k,v in config.items() if v['mode'] in t]\n    def channel_inst(c):\n        return config[c]['inst']\n    # def channel_insts():\n        # list of channel,instrument pairs\n        # return [(c,channel_inst(c)) for c in config]\n    def inst_ranges(insts):\n        # instruments to sets of allowed MIDI numbers\n        r = {}\n        for v in config.values():\n            i = v['inst']\n            if i in insts:\n                s = set(range(*(v.get('range') or get_range(i))))\n                if i in r:\n                    r[i] |= s\n                else:\n                    r[i] = s\n        return r\n    def auto_inst_channel(i):\n        for k,v in config.items():\n            if v['mode']=='auto' and v['inst']==i:\n                return k\n        return None\n    def channel_followers(chan):\n        # print(f'channel_followers {chan=} {config=}')\n        # return channel of all 'follow' voices with given source\n        return [\n            k for k,v in config.items() \n            if v['mode']=='follow' \n            and v.get('source', None)==chan]\n\n    def get_free_anon_like(i):\n        s = set(noto.anon_like(i)) - {d['inst'] for d in config.values()}\n        return next(iter(s))\n\n    def dedup_inst(c, i):\n        if any(\n            c_other!=c\n            and d['inst']==i \n            and d['mode']!='follow'\n            for c_other, d in config.items()\n            ):\n            i = get_free_anon_like(i)\n        return i\n\n    def do_send_pc(c, i):\n        # warn_inst(i)\n        # assuming fluidsynth -o synth.midi-bank-select=mma\n        if noto.is_drum(i):\n            midi.control_change(channel=c-1, control=0, value=1)\n        else:\n            midi.control_change(channel=c-1, control=32, value=0)\n            midi.control_change(channel=c-1, control=0, value=0)\n        if noto.is_anon(i):\n            program = 0\n        else:\n            # convert to 0-index\n            program = (i-1) % 128\n        midi.program_change(channel=c-1, program=program)\n\n    # simple class to hold pending event prediction\n    class Prediction:\n        def __init__(self):\n            self.gate = not initial_mute\n            self.reset()\n        def reset(self):\n            self.stopped = False\n            self.last_event_time = None\n            self.lateness = 0\n            self.cum_end_prob = 0\n            self.clear()\n        def clear(self):\n            self.event = None\n            self.next_event_time = None\n            # tui.defer(prediction=None)\n        def occurred(self):\n            self.last_event_time = self.next_event_time\n            self.clear()\n        def set(self, event):\n            if event.get('vel',0) &gt; 0:\n                self.stopped = False\n            if event.get('time') is None:\n                event['time'] = self.time_since()\n            self.event = event\n            self.next_event_time = event['time'] + (\n                self.last_event_time or now())\n        def time_since(self):\n            \"\"\"current actual time since nominal time of last played event\"\"\"\n            if self.last_event_time is None:\n                return 0\n            else:\n                return now() - self.last_event_time\n        def time_until(self):\n            if self.next_event_time is None: \n                return float('inf')\n            else:\n                return self.next_event_time - now()\n        def is_auto(self):\n            if self.event is None:\n                return False\n            return self.event['inst'] in mode_insts('auto')\n        def sample_end(self):\n            if self.event is None:\n                return False\n            end_prob = self.event.get('end', 0) ** end_exponent\n            # cumulative end probability\n            self.cum_end_prob = 1 - (1-self.cum_end_prob)*(1-end_prob)\n            # print(f'{self.cum_end_prob=}')\n            return random.random() &lt; end_prob\n    pending = Prediction()    \n\n    # tracks held notes, recently played instruments, etc\n    # NOTE: Notochord now tracks held notes,\n    # but NotoPerformance also tracks channels,\n    # lets you attach user data to notes,\n    # remembers past events, and contains other utilities\n    history = NotoPerformance()\n\n    status = {'reset_time':now()}\n\n    action_queue = []\n\n    def display_event(tag, memo, inst, pitch, vel, channel, **kw):\n        \"\"\"print an event to the terminal\"\"\"\n        if tag is None:\n            return\n        now = str(datetime.now())[:-2]\n        # tstr = \"O\" if kw[\"time\"]&lt;=1e-3 else \"\"\n        s = f'{now}   inst {inst:3d}   pitch {pitch:3d}   vel {vel:3d}   ch {channel:2d} {tag}'\n        # s = f'{tag}:\\t{inst=:4d}   {pitch=:4d}   {vel=:4d}   {channel=:3d}'\n        if memo is not None:\n            s += f' ({memo})'\n        tui.defer(note=s)\n\n    # @profile(print=print, enable=profiler)\n    def send_midi(note, velocity, channel):\n        kind = 'note_on' if velocity &gt; 0 else 'note_off'\n        cfg = config[channel]\n        # get channel note map\n        note_map = cfg.get('note_map', {})\n        if note in note_map:\n            note = note_map[note]\n        elif 'note_shift' in cfg:\n            note = note - cfg['note_shift']\n\n        if note &lt; 0:\n            print(f'WARNING: dropped note &lt; 0 ({note}, {channel=})')\n            return\n        if note &gt;= 128:\n            print(f'WARNING: dropped note &gt;= 128 ({note}, {channel=})')\n            return\n\n        port = cfg.get('port', None)\n\n        midi.send(kind, note=note, velocity=velocity, channel=channel-1, port=port)\n\n    @profile(print=print, enable=profiler)\n    def play_event(\n            channel, \n            parent=None, # parent note as (channel, inst, pitch)\n            feed=True, \n            send=True, \n            tag=None, memo=None):\n        \"\"\"realize an event as MIDI, terminal display, and Notochord update\"\"\"\n        with profile('rest', print=print, enable=profiler):\n            event = pending.event\n            if event is None:\n                print(\"WARNING: play_event on null event\")\n                return\n            time_until = pending.time_until()\n            pending.lateness = -time_until # set at the time of playing\n            # if time_until &lt; 10e-3:\n                # print(f'late {-time_until} at {now()=}')\n            pending.occurred()\n            # normalize values\n            vel = event['vel'] = math.ceil(event['vel'])\n\n            # send out as MIDI\n            if send:\n                send_midi(event['pitch'], vel, channel)\n\n            # print\n            display_event(tag, memo=memo, channel=channel, **event)\n\n        if feed:\n            # feed to NotoPerformance\n            # put a stopwatch in the held_note_data field for tracking note length\n            with profile('history.feed', print=print, enable=profiler&gt;1):\n                history.feed(held_note_data={\n                    'duration':Stopwatch(),\n                    'parent':parent\n                    }, channel=channel, **event)\n            # feed to model\n            with profile('noto.feed', print=print, enable=profiler):\n                noto.feed(**event)\n\n        follow_event(event, channel, feed=feed)\n\n    @profile(print=print, enable=profiler)\n    def follow_event(source_event, source_channel, feed=True):\n        source_vel = source_event['vel']\n        source_pitch = source_event['pitch']\n        source_inst = source_event['inst']\n        source_k = (source_channel, source_inst, source_pitch)\n\n        dt = 0\n\n        if source_vel &gt; 0:\n\n            # NoteOn\n            for noto_channel in channel_followers(source_channel):\n                cfg = config[noto_channel]\n\n                if cfg.get('mute', False): continue\n\n                noto_inst = cfg['inst']\n                min_x, max_x = cfg.get('transpose') or (-128,128)\n                lo, hi = cfg.get('range') or (0,127)\n\n                already_playing = {\n                    p for i,p in noto.held_notes if noto_inst==i}\n                # already_playing = {\n                #     note.pitch for note in history.notes if noto_inst==note.inst}\n                # print(f'{already_playing=}')\n\n                pitch_range = range(\n                    max(lo,source_pitch+min_x), min(hi, source_pitch+max_x+1))\n                pitches = (\n                    set(pitch_range) - {source_pitch} - already_playing\n                )\n\n                if len(pitches)==0:\n                    # edge case: no possible pitch\n                    print(f'skipping follow {noto_channel=}, no pitches available')\n                    print(f'{pitch_range} minus {source_pitch} minus {already_playing}')\n                    print(f'{lo=} {min_x=} {hi=} {max_x=} {source_pitch=}')\n                    continue\n                elif len(pitches)==1:\n                    # edge case: there is exactly one possible pitch\n                    pending.set(dict(\n                        inst=noto_inst, pitch=list(pitches)[0], \n                        time=dt, vel=source_vel))\n                else:\n                    # notochord chooses pitch\n                    with profile('noto.query', print=print, enable=profiler):\n                        pending.set(noto.query(\n                            next_inst=noto_inst, next_time=dt, next_vel=source_vel,\n                            include_pitch=pitches))\n\n                play_event(\n                    noto_channel, feed=feed,\n                    parent=source_k, tag='NOTO', memo='follow')\n        # NoteOff\n        else:\n            # print(f'{history.note_data=}')\n            dependents = [\n                noto_k # chan, inst, pitch\n                for noto_k, note_data\n                in history.note_data.items()\n                if note_data['parent']==source_k\n            ]\n\n            for noto_channel, noto_inst, noto_pitch in dependents:\n                pending.set(dict(\n                    inst=noto_inst, pitch=noto_pitch, time=dt, vel=0))\n                play_event(noto_channel, feed=feed, tag='NOTO', memo='follow')\n\n    # @lock\n    @profile(print=print, enable=profiler)\n    def noto_reset():\n        \"\"\"reset Notochord and end all of its held notes\"\"\"\n        print('RESET')\n\n        status['reset_time'] = now()\n\n        # cancel pending predictions\n        pending.reset()\n\n        # end Notochord held notes\n        # skip feeding for speed, since notochord gets reset anyway\n        end_held(feed=False, memo='reset')\n\n        # reset notochord state\n        noto.reset(state=initial_state)\n        # reset history\n        history.push()\n\n        # TODO: feed note-ons from any held input/follower notes?\n\n    # @lock\n    @profile(print=print, enable=profiler)\n    def noto_mute(sustain=False):\n        tui.query_one('#mute').label = 'UNMUTE' if pending.gate else 'MUTE'\n        # if sustain:\n        tui.query_one('#sustain').label = 'END SUSTAIN' if pending.gate else 'SUSTAIN'\n\n        pending.gate = not pending.gate\n\n        if sustain:\n            print('END SUSTAIN' if pending.gate else 'SUSTAIN')\n        else:\n            print('UNMUTE' if pending.gate else 'MUTE')\n\n        # if unmuting, we're done\n        if not pending.gate:\n            # cancel pending predictions\n            if not sustain:\n                end_held(memo='mute')\n            pending.clear()\n\n    def noto_stop():\n        print('STOP')\n        end_held(memo='stop on end')\n        pending.stopped = True\n\n    @profile(print=print, enable=profiler)    \n    def end_held(feed=True, channel=None, memo=None):\n        # end+feed all held notes\n        channels = mode_chans('auto') if channel is None else [channel]\n        ended_any = False\n        for note in history.notes:\n            if note.chan in channels:\n                pending.set(dict(inst=note.inst, pitch=note.pitch, vel=0))\n                play_event(\n                    channel=note.chan, feed=feed, tag='NOTO', memo=memo)\n                ended_any = True\n        return ended_any\n\n    # query Notochord for a new next event\n    # @lock\n    @profile(print=print, enable=profiler)\n    def auto_query(\n            predict_input=predict_input, \n            predict_follow=predict_follow,\n            immediate=False):\n\n        # NOTE: replaced this with duration constraints;\n        # should test more before deleting\n        # check for stuck notes\n        # and prioritize ending those\n        # for (_, inst, pitch), note_data in history.note_data.items():\n        #     dur = note_data['duration'].read()\n        #     if (\n        #         inst in mode_insts('auto') \n        #         and dur &gt; max_note_len*(.1+controls.get('steer_duration', 1))\n        #         ):\n        #         # query for the end of a note with flexible timing\n        #         # with profile('query', print=print, enable=profiler):\n        #         t = pending.time_since()\n        #         mt = max(t, min(max_time or np.inf, t+0.2))\n        #         pending.set(noto.query(\n        #             next_inst=inst, next_pitch=pitch,\n        #             next_vel=0, min_time=t, max_time=mt))\n        #         print(f'END STUCK NOTE {inst=},{pitch=},{dur=}')\n        #         return\n\n        if immediate:\n            # sampling immediately after realizing an event\n            min_time = 0\n        else:\n            if pending.event is not None and not pending.is_auto():\n                # re-sampling ahead after sampling an input or follow voice\n                min_time = pending.event['time']\n            else:\n                # otherwise sampling after some delay\n                min_time = pending.time_since()\n            min_time = min_time-estimated_feed_latency\n        # print(f'{immediate=} {min_time=}')\n\n        # print(f'{pending.lateness=} {pending.time_since()=}')\n\n        lateness = pending.lateness if immediate else -pending.time_until()\n        if lateness &gt; lateness_margin:\n            backoff = lateness + estimated_latency\n            if backoff &gt; min_time:\n                min_time = backoff\n                print(f'set {min_time=} due to {lateness=}')\n\n        # if max_time &lt; min_time, exclude input instruments\n        input_late = max_time is not None and max_time &lt; min_time\n\n        inst_modes = ['auto']\n        if predict_follow:\n            inst_modes.append('follow')\n        if predict_input and not input_late:\n            inst_modes.append('input')\n        allowed_insts = mode_insts(inst_modes, allow_muted=False)\n\n        # held_notes = history.held_inst_pitch_map()\n        # print(f'{held_notes=}')\n        steer_time = 1-controls.get('steer_rate', 0.5)\n        steer_pitch = controls.get('steer_pitch', 0.5)\n        steer_density = controls.get('steer_density', 0.5)\n        steer_velocity = controls.get('steer_velocity', 0.5)\n\n        if lateness &gt; soft_lateness_margin and lateness &lt;= lateness_margin:\n            steer_time = 1\n            print(f'set {steer_time=} due to {lateness=}')\n\n        rhythm_temp = controls.get('rhythm_temp', 1)\n        timing_temp = controls.get('timing_temp', 1)\n\n        tqt = (max(0,steer_time-0.5), min(1, steer_time+0.5))\n        tqp = (max(0,steer_pitch-0.5), min(1, steer_pitch+0.5))\n        tqv = (max(0,steer_velocity-0.5), min(1, steer_velocity+0.5))\n\n        # idea: maintain an 'instrument presence' quantity\n        # incorporating time since / number of notes playing\n        # ideally this would distinguish sustained from percussive instruments too\n\n        # balance_sample: note-ons only from instruments which have played less\n        inst_weights = None\n        if balance_sample:\n            counts = defaultdict(int)\n            for i,c in history.inst_counts(n=n_recent).items():\n                counts[i] = c\n            # print(f'{counts=}')\n            inst_weights = {}\n            mc = max(counts.values()) if len(counts) else 0\n            for i in allowed_insts:\n                # don't upweight player controlled instruments though\n                if i in mode_insts('auto'):\n                    inst_weights[i] = math.exp(max(0, mc - counts[i] - n_margin))\n                else:\n                    inst_weights[i] = 1.\n        # print(f'{inst_weights=}')\n\n        # VTIP is better for time interventions,\n        # VIPT is better for instrument interventions\n        if min_time &gt; estimated_latency or abs(steer_time-0.5) &gt; abs(steer_pitch-0.5):\n            query_method = noto.query_vtip\n            # print('VTIP')\n        else:\n            query_method = noto.query_vipt\n            # print('VIPT')\n        # query_method = noto.query_vipt ### DEBUG\n        # query_method = noto.query_vtip ### DEBUG\n        query_method = profile(print=print, enable=profiler)(query_method)\n\n        # print(f'considering {insts} for note_on')\n        # use only currently selected instruments\n        inst_pitch_map = inst_ranges(allowed_insts)\n        note_on_map = {\n            i: set(inst_pitch_map[i])#-set(held_notes[i]) # exclude held notes\n            for i in allowed_insts#allowed_insts\n            if i in inst_pitch_map\n        }\n\n        min_polyphony = {}\n        max_polyphony = {}\n        min_duration = {}\n        max_duration = {}\n        for i in note_on_map:\n            c = auto_inst_channel(i)\n            if c is None: continue\n            cfg = config[c]\n            if cfg is not None and cfg.get('poly') is not None:\n                min_polyphony[i], max_polyphony[i] = cfg['poly']\n            if cfg is not None and cfg.get('duration') is not None:\n                min_duration[i], max_duration[i] = cfg['duration']\n            elif max_note_len is not None:\n                max_duration[i] = max_note_len\n\n        # print(note_on_map, note_off_map)\n\n        max_t = None if max_time is None else max(max_time, min_time+0.2)\n\n        # print(f'{note_on_map=}')\n        try:\n            pending.set(query_method(\n                note_on_map, #note_off_map,\n                min_polyphony=min_polyphony, max_polyphony=max_polyphony,\n                min_duration=min_duration, max_duration=max_duration,\n                min_time=min_time, max_time=max_t,\n                min_vel=min_vel, max_vel=max_vel,\n                truncate_quantile_time=tqt,\n                truncate_quantile_pitch=tqp,\n                truncate_quantile_vel=tqv,\n                rhythm_temp=rhythm_temp,\n                timing_temp=timing_temp,\n                steer_density=steer_density,\n                inst_weights=inst_weights,\n                no_steer=mode_insts(('input','follow'), allow_muted=False),\n            ))\n        except NoPossibleEvents:\n            pass\n            # print(f'stopping; no possible events')\n            # pending.stopped = True\n            # pending.clear()\n        except Exception:\n            print(f'WARNING: query failed. {allowed_insts=} {note_on_map=}')\n            print(f'{noto.held_notes=}')\n            print(f'{config=}')\n            traceback.print_exc(file=tui)\n            # pending.clear()\n\n    #### MIDI handling\n\n    # print all incoming MIDI for debugging\n    if dump_midi:\n        @midi.handle\n        def _(msg):\n            print(msg)\n\n    @midi.handle(type='program_change')\n    def _(msg):\n        \"\"\"\n        TODO:Program change events set GM instruments on the corresponding channel\n        \"\"\"\n        # c = msg.channel+1\n        # c = input_channel_map.get(c, c)\n        # i = msg.program+1\n        # action_queue.append(ft.partial(set_inst(c,i)))\n        # if thru:\n        raise NotImplementedError\n\n    @midi.handle(type='pitchwheel')\n    def _(msg):\n        \"\"\"\n        pitchwheel affects steer_pitch\n        \"\"\"\n        if thru:\n            midi.send(msg)\n        # print(msg)\n        controls['steer_pitch'] = (msg.pitch+8192)/16384\n        # print(controls)\n        # print(f'{controls[\"steer_pitch\"]=}')\n\n    # very basic CC handling for controls\n    control_cc = {}\n    control_osc = {}\n    for ctrl in control_meta:\n        name = ctrl['name']\n        ccs = ctrl.get('control_change', [])\n        if isinstance(ccs, Number) or isinstance(ccs, str):\n            ccs = (ccs,)\n        for cc in ccs:\n            control_cc[cc] = ctrl\n        control_osc[f'/notochord/homunculus/{name}'] = ctrl\n    action_cc = {}\n    action_note = {}\n    action_osc = {}\n    for act in action_meta:\n        name = act['name']\n        ccs = act.get('control_change', [])\n        notes = act.get('note', [])\n        if isinstance(ccs, Number) or isinstance(ccs, str):\n            ccs = (ccs,)\n        if isinstance(notes, Number) or isinstance(notes, str):\n            notes = (notes,)\n        for cc in ccs:\n            action_cc[cc] = act\n        for note in notes:\n            action_note[note] = act\n        action_osc[f'/notochord/homunculus/{name}'] = act\n\n    def dispatch_action(k, v=None):   \n        if k=='reset':\n            action_queue.append(noto_reset)\n        elif k=='query':\n            action_queue.append(auto_query)\n        elif k=='mute':\n            action_queue.append(noto_mute)\n        elif k=='preset':\n            action_queue.append(ft.partial(set_preset, v or 0))\n        elif k=='preset_reset':\n            action_queue.append(ft.partial(set_preset, v or 0))\n            action_queue.append(noto_reset)\n        elif k=='stop':\n            action_queue.append(noto_stop)\n        else:\n            print(f'WARNING: action \"{k}\" not recognized')\n\n    @midi.handle(type='control_change')\n    def _(msg):\n        \"\"\"\n        these are global controls listening on all channels: \n        CC 01: steer pitch (&gt;64 higher pitches, &lt;64 lower)\n        CC 02: steer density (&gt;64 more simultaneous notes, &lt;64 fewer)\n        CC 03: steer rate (&gt;64 more events, &lt;64 fewer)\n        \"\"\"\n        if msg.control in control_cc:\n            ctrl = control_cc[msg.control]\n            name = ctrl['name']\n            lo, hi = ctrl.get('range', (0,1))\n            controls[name] = msg.value/127 * (hi-lo) + lo\n            print(f\"{name}={controls[name]}\")\n\n        if msg.control in action_cc:\n            k = action_cc[msg.control]['name']\n            v = action_cc[msg.control].get('value')\n            dispatch_action(k, v)\n\n    if midi_control is not None:\n        preset_keys = range(112,120)\n        momentary_keys = [120]\n        for note in preset_keys:\n            midi_control.note_on(\n                channel=0, note=note, velocity=0)\n\n        @midi_control.handle(type=('note_on'))\n        def _(msg, port):\n            print('control note event', msg)\n\n            if msg.velocity and msg.note in action_note:\n                k = action_note[msg.note]['name']\n                v = action_note[msg.note].get('value')\n                dispatch_action(k, v)\n\n            # MIDI feedback (lights)\n            if msg.note in preset_keys:\n                # if msg.velocity==0: return\n                if msg.velocity&gt;0:\n                    for note in preset_keys:\n                        midi_control.note_on(\n                            channel=0, note=note, velocity=0, port=port)\n                midi_control.note_on(\n                    channel=0, note=msg.note, velocity=70 if msg.velocity else 127, port=port)\n            if msg.note in momentary_keys:\n                midi_control.note_on(\n                    channel=0, note=msg.note, velocity=msg.velocity, port=port)\n\n    if osc_port is not None:\n        @osc.handle('/notochord/homunculus/*')\n        def _(route, *a):\n            # print('OSC:', route, *a)\n            if route in control_osc:\n                ctrl = control_osc[route]\n                name = ctrl['name']\n                assert len(a)==1\n                arg = a[0]\n                assert isinstance(arg, Number)\n                lo, hi = ctrl.get('range', (0,1))   \n                controls[name] = min(hi, max(lo, arg))\n                print(f\"{name}={controls[name]}\")\n\n            if route in action_osc:\n                action = action_osc[route]\n                k = action['name']\n                dispatch_action(k, v=a[0] if len(a) else action.get('value'))\n\n    input_sw = Stopwatch()\n    dropped = set()# (channel, pitch)\n    input_dts = []\n    @midi.handle(type=('note_on', 'note_off'))\n    def _(msg):\n        \"\"\"\n        MIDI NoteOn and NoteOff events affect input channels\n        e.g. a channel displaying --&gt;01 will listen to note events on channel 1\n        a channel displaying 02-&gt;03 will follow note events on channel 2\n        \"\"\"\n        channel = msg.channel + 1\n        # convert from 0-index\n        channel = input_channel_map.get(channel, channel)\n\n        if punch_in:\n            # set_mode(channel, 'input')\n            action_queue.append(ft.partial(set_mode, channel, 'input'))\n\n        cfg = config[channel]\n\n        if not (punch_in or channel in mode_chans('input')):\n            print(f\"{channel=} {mode_chans('input')=}\")\n            print(f'WARNING: ignoring MIDI {msg} on non-input channel {channel}')\n            return\n\n        if cfg['mute']:\n            print(f'WARNING: ignoring MIDI {msg} on muted channel {channel}')\n            return\n\n        inst = channel_inst(channel)\n        pitch = msg.note + cfg.get('note_shift', 0)\n        vel = msg.velocity if msg.type=='note_on' else 0\n\n        dt = input_sw.punch()\n        # print(f'EVENT {dt=} {msg}')\n        if len(input_dts) &gt;= 10:\n            input_dts.pop(0)\n        input_dts.append(dt)\n        input_dens = len(input_dts) / sum(input_dts)\n        # TODO: \n        # want to drop input when event density is high,\n        # not just dt is short\n        k = (channel,pitch)\n        if vel==0 and k in dropped:\n            dropped.remove(k)\n            print(f'WARNING: ignoring rate-limited input')\n            return\n        if vel&gt;0 and limit_input and input_dens&gt;limit_input:\n            print(f'WARNING: ignoring rate-limited input {input_dens=}')\n            dropped.add(k)\n            return \n\n        action_queue.append(ft.partial(\n            do_note_input, channel, inst, pitch, vel))\n\n    def do_note_input(channel, inst, pitch, vel):\n        # feed event to Notochord\n        pending.set({'inst':inst, 'pitch':pitch, 'vel':vel})\n        play_event(channel=channel, send=thru, tag='PLAYER')\n\n    @profile(print=print, enable=profiler)\n    def auto_event():\n        # print('auto_event')\n        # 'auto' event happens:\n        event = pending.event\n        inst, pitch, vel = event['inst'], event['pitch'], math.ceil(event['vel'])\n        chan = auto_inst_channel(inst)\n        if chan is None:\n            raise ValueError(f\"channel not found for instrument {inst}\")\n\n        # shouldn't happen, but prevent\n        # note on which is already playing or note off which is not\n        if (vel&gt;0) == ((inst, pitch) in noto.held_notes): \n            print(f'WARNING: re-query for invalid {vel=}, {inst=}, {pitch=}')\n            auto_query()\n            return\n\n        do_stop = False\n        do_reset = False\n        if (\n            (stop_on_end or reset_on_end) \n            and (now()-status['reset_time'] &gt; min_end_time) \n            and pending.sample_end()\n            ):\n            print('END')\n            do_stop = stop_on_end\n            do_reset = reset_on_end # needs to happen after final event plays\n\n        play_event(channel=chan, tag='NOTO')\n\n        if do_reset:\n            noto_reset()\n        elif do_stop:\n            noto_stop()\n\n    # @profile(print=print, enable=profiler)\n    def maybe_punch_out():\n        none_held = set(mode_chans('input'))\n        for c,_,_ in history.notes:\n            if c in none_held:\n                none_held.remove(c)\n\n        evt = history.events\n        recent_events = evt[\n            (evt.vel &gt; 0) &amp;\n            (evt.wall_time_ns &gt; time.time_ns() - punch_out_after*1e9)\n            ]\n        for c in none_held:\n            # print(recent_events.channel)\n            # print(f'{c=} {(c not in recent_events.channel)=}')\n            if c not in recent_events.channel.values:\n                set_mode(c, 'auto')\n\n    @cleanup\n    def _():\n        \"\"\"end any remaining notes\"\"\"\n        print(f'cleanup: {noto.held_notes=}')\n        # TODO: this should run in repeat thread..?\n        end_held(feed=False, memo='cleanup')\n\n    ### update_* keeps the UI in sync with the state\n\n    def update_config():\n        # pass\n        # print(config)\n        for c,v in config.items():\n            # tui.set_channel(c, v)\n            tui.call_from_anywhere(tui.set_channel, c, v)\n\n    def update_presets():\n        for k,p in enumerate(presets):\n            # tui.set_preset(k, p.get('name'))\n            tui.call_from_anywhere(tui.set_preset, k, p.get('name'))\n\n    ### set_* does whatever necessary to change channel properties\n    ### calls update_config() to keep the UI in sync\n\n    def set_mode(c, m, update=True):\n        if c in config:\n            prev_m = config[c]['mode']\n        else:\n            prev_m = None\n        if m==prev_m:\n            return\n\n        if m=='follow':\n            if 'source' not in config[c]:\n                print('WARNING: follower without a source, setting to 1')\n                config[c]['source'] = 1\n\n        config[c]['mode'] = m\n        print(f'set channel {c} from {prev_m} to {m} mode')\n\n        ### NOTE changing a channel with followers to input causes stuck notes\n\n        if prev_m=='follow':\n            # emancipate held notes\n            for (dep_c,_,_), note_data in history.note_data.items():\n                if dep_c==c:\n                    note_data['parent'] = None\n\n        if prev_m=='auto':\n            # release held notes\n            end_held(channel=c, memo='mode change')\n\n        if m=='auto':\n            # immediately refresh prediction\n            pending.clear()\n\n        if update:\n            update_config()\n\n    def set_inst(c, i, program=None, update=True, allow_pc=True):\n        # print(f'set channel {c} instrument {i}')\n        req_i = i\n        if c in config:\n            prev_i = config[c]['inst']\n        else:\n            prev_i = None\n\n        # don't steal from lower channels\n        lower_insts = {d['inst'] for c2, d in config.items() if c2 &lt; c}\n        if i in lower_insts:\n            # first anon not used by a lower channel\n            i = min(set(noto.anon_like(i)) - lower_insts)\n\n        # end held notes on old instrument\n        if prev_i!=i and config[c]['mode']!='input':\n            end_held(channel=c, memo='instrument change')\n            pending.clear()\n\n        # then set config\n        config[c]['inst'] = i\n\n        # steal from higher channels\n        for c2 in range(c+1,17):\n            i2 = config[c2]['inst']\n            if i2==i or i2 in lower_insts:\n                set_inst(\n                    c2, get_free_anon_like(i), \n                    update=False, allow_pc=False)\n\n        # send pc if appropriate\n        if send_pc and allow_pc:\n            do_send_pc(c, req_i if program is None else program)\n\n        print(f'set channel {c} to instrument {i} (was {prev_i}, requested {req_i})')\n\n        if update:\n            update_config()\n\n    # @lock\n    def set_mute(c, b, update=True):\n        config[c]['mute'] = b\n        if b:\n            print(f'mute channel {c}')\n            # release held notes\n            if config[c]['mode']!='input':\n                end_held(channel=c, memo='mute channel')\n                pending.clear()\n        else:\n            print(f'unmute channel {c}')\n\n        if update:\n            update_config()\n\n    def set_cc(c, ccs, update=True):\n        # print(f'{c=}, {ccs=}')\n        for k,v in ccs.items():\n            k = int(k)\n            midi.send('control_change', channel=c-1, control=k, value=v)\n            # print(f'{(k,v)=}')\n\n    # @lock\n    def set_preset(p, update=True):\n        nonlocal initial_state\n        if p &gt;= len(presets): return\n        print(f'load preset: {p+1}')\n        preset = presets[p]\n\n        overlay = preset.get('overlay', False)\n\n        state = preset.get('initial_state')\n        if state is None:\n            if overlay:\n                print(f'leaving current initial state')\n            else:\n                print(f'using global initial state')\n                initial_state = global_initial_state\n        else:\n            print(f'using initial state from preset prompt')\n            initial_state = state\n\n        if update:\n            for i in range(NotoPresets.n_presets):\n                tui.query_one('#'+preset_id(i)).variant = (\n                    'warning' if i==p else 'default')\n\n        set_config(preset['channel'], overlay=overlay, update=update)\n\n    def set_config(cfg, overlay=False, update=True):\n        # NOTE: config should *not* be updated before calling set_* \n        if overlay:\n            # just apply settings which are in the preset\n            for c in sorted(cfg):\n                v = {**cfg.get(c, {})}\n                if 'mode' in v:\n                    set_mode(c, v.pop('mode'), update=False)\n                if 'inst' in v:\n                    set_inst(\n                        c, v.pop('inst'), \n                        program=v.pop('program_inst', None), \n                        update=False)\n                if 'mute' in v:\n                    set_mute(c, v.pop('mute'), update=False)\n                if 'cc' in v:\n                    set_cc(c, v.pop('cc'), update=False)\n                config[c].update(v)\n        else:\n            # deterministically apply preset on top of defaults\n            # for c in range(16,0,-1):    \n            for c in range(1,17):    \n                v = default_config_channel(c)\n                # v.update(cfg.get(c, {}))\n                deep_update(v, cfg.get(c, {}))\n                print(f'{v=}')\n                set_mode(c, v.pop('mode'), update=False)\n                set_inst(\n                    c, v.pop('inst'), \n                    program=v.pop('program_inst', None), \n                    update=False)\n                set_mute(c, v.pop('mute'), update=False)\n                set_cc(c, v.pop('cc'), update=False)\n                config[c].update(v)\n\n        if update:\n            update_config()\n\n    ### action_* runs on key/button press;\n    ### invokes cycler / picker logic and schedules set_*\n\n    # this is pretty awful\n    # need a better way to reconcile iipyper and textual here\n    def action_mode(c):\n        if c not in config: return\n        # TODO: mode picker\n        if config[c]['mode'] == 'auto':\n            mode = 'input'\n        elif config[c]['mode'] == 'input' and config[c]['source']!=c:\n            # TODO: source picker for follow\n            mode = 'follow'\n        else:\n            mode = 'auto'\n        action_queue.append(ft.partial(set_mode, c, mode))\n\n    def action_inst(c):\n        print(f'inst channel {c}')\n        tui.push_screen(InstrumentGroupSelect(c))\n\n    def action_mute(c):\n        if i not in config: return\n        # set_mute(c, not config[c].get('mute', False))\n        action_queue.append(ft.partial(\n            set_mute, c, not config[c].get('mute', False)))\n\n    def action_preset(p):\n        if p &gt;= len(presets): \n            return\n        action_queue.append(ft.partial(set_preset, p))\n\n    ### set actions which have an index argument\n    ### TODO move this logic into @tui.set_action\n\n    for i in range(1,17):\n        setattr(tui, f'action_mode_{i}', ft.partial(action_mode, i))\n        setattr(tui, f'action_inst_{i}', ft.partial(action_inst, i))\n        setattr(tui, f'action_mute_{i}', ft.partial(action_mute, i))\n\n    for i in range(NotoPresets.n_presets):\n        setattr(tui, f'action_preset_{i}', ft.partial(action_preset, i))\n\n    ### additional key/button actions\n\n    @tui.set_action\n    def mute():\n        action_queue.append(noto_mute)\n\n    @tui.set_action\n    def sustain():\n        action_queue.append(ft.partial(noto_mute, sustain=True))\n\n    @tui.set_action\n    def reset():\n        action_queue.append(noto_reset)\n\n    @tui.set_action\n    def query():\n        action_queue.append(auto_query)\n\n    @tui.set_action\n    def stop():\n        action_queue.append(noto_stop)\n\n    @tui.set_action\n    def preset1():\n        action_queue.append(ft.partial(set_preset, 0))\n    @tui.set_action\n    def preset2():\n        action_queue.append(ft.partial(set_preset, 1))\n    @tui.set_action\n    def preset3():\n        action_queue.append(ft.partial(set_preset, 2))\n    @tui.set_action\n    def preset4():\n        action_queue.append(ft.partial(set_preset, 3))\n    @tui.set_action\n    def preset5():\n        action_queue.append(ft.partial(set_preset, 4))\n    @tui.set_action\n    def preset6():\n        action_queue.append(ft.partial(set_preset, 5))\n    @tui.set_action\n    def preset7():\n        action_queue.append(ft.partial(set_preset, 6))\n    @tui.set_action\n    def preset8():\n        action_queue.append(ft.partial(set_preset, 7))\n    @tui.set_action\n    def preset9():\n        action_queue.append(ft.partial(set_preset, 8))\n    @tui.set_action\n    def preset10():\n        action_queue.append(ft.partial(set_preset, 9))\n\n    ### TUI classes which close over variables defined in main\n\n    class Instrument(Button):\n        \"\"\"button which picks an instrument\"\"\"\n        def __init__(self, c, i):\n            super().__init__(inst_label(i))\n            self.channel = c\n            self.inst = i\n        def on_button_pressed(self, event: Button.Pressed):\n            self.app.pop_screen()\n            self.app.pop_screen()\n            # set_inst(self.channel, self.inst)\n            action_queue.append(ft.partial(\n                set_inst, self.channel, self.inst))\n\n    class InstrumentGroup(Button):\n        \"\"\"button which picks an instrument group\"\"\"\n        def __init__(self, text, c, g):\n            super().__init__(text)\n            self.channel = c\n            self.group = g\n        def on_button_pressed(self, event: Button.Pressed):\n            # show inst buttons\n            tui.push_screen(InstrumentSelect(self.channel, self.group))\n\n    class InstrumentSelect(ModalScreen):\n        \"\"\"Screen with instruments\"\"\"\n        def __init__(self, c, g):\n            super().__init__()\n            self.channel = c\n            self.group = g\n\n        def compose(self):\n            yield Grid(\n                *(\n                    Instrument(self.channel, i)\n                    for i in gm_groups[self.group][1]\n                ), id=\"dialog\",\n            )\n\n    class InstrumentGroupSelect(ModalScreen):\n        \"\"\"Screen with instrument groups\"\"\"\n        # TODO: add other features to this screen -- transpose, range, etc?\n        def __init__(self, c):\n            super().__init__()\n            self.channel = c\n\n        def compose(self):\n            yield Grid(\n                *(\n                    InstrumentGroup(s, self.channel, g)\n                    for g,(s,_) in enumerate(gm_groups)\n                ), id=\"dialog\",\n            )\n\n    initial_preset = 0\n    if isinstance(preset, str):\n        for n,p in enumerate(presets):\n            if preset == p['name']:\n                initial_preset = n\n                break\n\n    config_cli = config or {}\n    config = {i:default_config_channel(i) for i in range(1,17)}\n    set_preset(initial_preset, update=False)\n    if config_ingest is not None:\n        set_config(config_ingest, overlay=False, update=False)\n    set_config(config_cli, overlay=True, update=False)\n\n    prediction_displayed = [None]\n    @repeat(lock=True, err_file=tui)\n    # @profile(print=print, enable=profiler)\n    def _():\n        \"\"\"Loop, process enqueued actions and check if predicted next event happens\"\"\"\n        # print(f'repeat {time.time()}')\n        for _ in range(8): # process multiple actions per tick\n            if len(action_queue):\n                # print(action_queue)\n                action_queue.pop(0)()\n        # TODO: immediate query if an input has just happened\n\n        # if there is no predicted event,\n        # or it's not for an auto voice,\n        # sample one:\n        if pending.gate and not pending.stopped and not pending.is_auto():\n            # with profile('auto_query', print=print, enable=profiler):\n            auto_query()\n\n        # if unmuted, predicted event is auto, and its time has passed,\n        # realize it\n        if (\n            not testing and\n            pending.gate and\n            pending.is_auto() and\n            pending.time_until() &lt;= 0\n            # pending.time_until() &lt;= estimated_feed_latency\n            ):\n            # with profile('auto_event', print=print, enable=profiler):\n            auto_event()\n\n\n            # query for new prediction\n            if not (pending.stopped or debug_query):\n                # with profile('auto_query', print=print, enable=profiler):\n                auto_query(immediate=True)\n        else:\n            # otherwise there is a pause, update the UI with next prediction\n            if pending.event != prediction_displayed[0]:\n                tui.defer(prediction=pending.event)\n                prediction_displayed[0] = pending.event\n            if punch_in:\n                maybe_punch_out()\n\n        # TODO note sure if this is needed anymore\n        # adaptive time resolution here -- yield to other threads when \n        # next event is not expected, but time precisely when next event\n        # is imminent\n        wait = 10e-3\n        if len(action_queue):\n            wait = 0\n        elif pending.is_auto():\n            wait = pending.time_until()\n        # print(f'{wait=}')\n        r = max(0, min(10e-3, wait))\n        return r\n\n    @tui.on\n    def mount(): \n        update_config()\n        update_presets()\n        print('MIDI handling:')\n        print(midi.get_docs())\n        if osc_port is not None:\n            print('OSC handling:')\n            print(osc.get_docs())\n        print(tui_doc)\n        print('For more detailed documentation, see:')\n        print('https://intelligent-instruments-lab.github.io/notochord/reference/notochord/app/homunculus/')\n        print('or run `notochord homunculus --help`')\n        print('to exit, use CTRL+C')\n\n        action_queue.append(noto_reset)\n\n        if initial_stop:\n            action_queue.append(noto_stop)\n        else:\n            action_queue.append(ft.partial(\n                auto_query, predict_input=False, predict_follow=False))\n\n    if use_tui:\n        tui.run()\n    else:\n        if initial_stop:\n            noto_stop()\n        else:\n            auto_query(predict_input=False, predict_follow=False)\n</code></pre>"},{"location":"reference/notochord/app/improviser/","title":"Improviser","text":"<p>Notochord MIDI co-improviser server. Notochord plays different instruments along with the player.</p> Authors <p>Victor Shepardson Intelligent Instruments Lab 2023</p>"},{"location":"reference/notochord/app/improviser/#notochord.app.improviser.NotoTUI","title":"<code>NotoTUI</code>","text":"<p>             Bases: <code>TUI</code></p> Source code in <code>src/notochord/app/improviser.py</code> <pre><code>class NotoTUI(TUI):\n    CSS_PATH = 'improviser.css'\n\n    BINDINGS = [\n        (\"m\", \"mute\", \"Mute Notochord\"),\n        (\"s\", \"sustain\", \"Mute without ending notes\"),\n        (\"q\", \"query\", \"Re-query Notochord\"),\n        (\"r\", \"reset\", \"Reset Notochord\")]\n\n    def compose(self):\n        \"\"\"Create child widgets for the app.\"\"\"\n        yield Header()\n        yield self.std_log\n        yield NotoLog(id='note')\n        yield NotoPrediction(id='prediction')\n        yield NotoControl()\n        yield Footer()\n</code></pre>"},{"location":"reference/notochord/app/improviser/#notochord.app.improviser.NotoTUI.compose","title":"<code>compose()</code>","text":"<p>Create child widgets for the app.</p> Source code in <code>src/notochord/app/improviser.py</code> <pre><code>def compose(self):\n    \"\"\"Create child widgets for the app.\"\"\"\n    yield Header()\n    yield self.std_log\n    yield NotoLog(id='note')\n    yield NotoPrediction(id='prediction')\n    yield NotoControl()\n    yield Footer()\n</code></pre>"},{"location":"reference/notochord/app/improviser/#notochord.app.improviser.main","title":"<code>main(checkpoint='notochord-latest.ckpt', player_config=None, noto_config=None, initial_mute=False, initial_query=False, midi_in=None, midi_out=None, thru=False, send_pc=False, dump_midi=False, balance_sample=False, n_recent=64, n_margin=8, max_note_len=5, max_time=None, nominal_time=False, osc_port=None, osc_host='', use_tui=True, predict_player=True, auto_query=True, testing=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <p>path to notochord model checkpoint.</p> <code>'notochord-latest.ckpt'</code> <code>player_config</code> <code>Dict[int, int]</code> <p>mapping from MIDI channels to MIDI instruments controlled by the player.</p> <code>None</code> <code>noto_config</code> <code>Dict[int, int]</code> <p>mapping from MIDI channels to MIDI instruments controlled by notochord. Both indexed from 1. instruments should be different from the player instruments. channels should be different unless different ports are used. MIDI channels and General MIDI instruments are indexed from 1.</p> <code>None</code> <code>initial_mute</code> <p>start Notochord muted so it won't play with input.</p> <code>False</code> <code>initial_query</code> <p>query Notochord immediately so it plays even without input.</p> <code>False</code> <code>midi_in</code> <code>Optional[str]</code> <p>MIDI ports for player input.  default is to use all input ports. can be comma-separated list of ports.</p> <code>None</code> <code>midi_out</code> <code>Optional[str]</code> <p>MIDI ports for Notochord output.  default is to use only virtual 'From iipyper' port. can be comma-separated list of ports.</p> <code>None</code> <code>thru</code> <p>if True, copy incoming MIDI to output ports. only makes sense if input and output ports are different.</p> <code>False</code> <code>send_pc</code> <p>if True, send MIDI program change messages to set the General MIDI instrument on each channel according to player_config and noto_config. useful when using a General MIDI synthesizer like fluidsynth.</p> <code>False</code> <code>dump_midi</code> <p>if True, print all incoming MIDI for debugging purposes</p> <code>False</code> <code>balance_sample</code> <p>choose instruments which have played less recently ensures that all configured instruments will play.</p> <code>False</code> <code>n_recent</code> <p>number of recent note-on events to consider for above</p> <code>64</code> <code>n_margin</code> <p>amount of 'slack' in the balance_sample calculation</p> <code>8</code> <code>max_note_len</code> <p>time in seconds after which to force-release sustained notochord notes.</p> <code>5</code> <code>max_time</code> <p>maximum time in seconds between predicted events. default is the Notochord model's maximum (usually 10 seconds).</p> <code>None</code> <code>nominal_time</code> <p>if True, feed Notochord with its own predicted times instead of the actual elapsed time. May make Notochord more likely to play chords.</p> <code>False</code> <code>osc_port</code> <p>optional. if supplied, listen for OSC to set controls</p> <code>None</code> <code>osc_host</code> <p>hostname or IP of OSC sender. leave this as empty string to get all traffic on the port</p> <code>''</code> <code>use_tui</code> <p>run textual UI.</p> <code>True</code> <code>predict_player</code> <p>forecasted next events can be for player. generally should be true, use balance_sample to force Notochord to play.</p> <code>True</code> Source code in <code>src/notochord/app/improviser.py</code> <pre><code>def main(\n    checkpoint=\"notochord-latest.ckpt\", # Notochord checkpoint\n    player_config:Dict[int,int]=None, # map MIDI channel : GM instrument\n    noto_config:Dict[int,int]=None, # map MIDI channel : GM instrument\n\n    initial_mute=False, # start with Notochord muted\n    initial_query=False, # let Notochord start playing immediately\n\n    midi_in:Optional[str]=None, # MIDI port for player input\n    midi_out:Optional[str]=None, # MIDI port for Notochord output\n    thru=False, # copy player input to output\n    send_pc=False, # send program change messages\n    dump_midi=False, # print all incoming MIDI\n\n    balance_sample=False, # choose instruments which have played less recently\n    n_recent=64, # number of recent note-on events to consider for above\n    n_margin=8, # amount of 'slack' in the balance_sample calculation\n\n    max_note_len=5, # in seconds, to auto-release stuck Notochord notes\n    max_time=None, # max time between events\n    nominal_time=False, #feed Notochord with nominal dt instead of actual\n\n    osc_port=None, # if supplied, listen for OSC to set controls on this port\n    osc_host='', # leave this as empty string to get all traffic on the port\n\n    use_tui=True, # run textual UI\n    predict_player=True, # forecasted next events can be for player (preserves model distribution, but can lead to Notochord deciding not to play)\n    auto_query=True, # query notochord whenever it is unmuted and there is no pending event. generally should be True except for testing purposes.\n    testing=False\n    ):\n    \"\"\"\n    Args:\n        checkpoint: path to notochord model checkpoint.\n\n        player_config: mapping from MIDI channels to MIDI instruments controlled\n            by the player.\n        noto_config: mapping from MIDI channels to MIDI instruments controlled\n            by notochord. Both indexed from 1.\n            instruments should be different from the player instruments.\n            channels should be different unless different ports are used.\n            MIDI channels and General MIDI instruments are indexed from 1.\n\n        initial_mute: start Notochord muted so it won't play with input.\n        initial_query: query Notochord immediately so it plays even without input.\n\n        midi_in: MIDI ports for player input. \n            default is to use all input ports.\n            can be comma-separated list of ports.\n        midi_out: MIDI ports for Notochord output. \n            default is to use only virtual 'From iipyper' port.\n            can be comma-separated list of ports.\n        thru: if True, copy incoming MIDI to output ports.\n            only makes sense if input and output ports are different.\n        send_pc: if True, send MIDI program change messages to set the General MIDI\n            instrument on each channel according to player_config and noto_config.\n            useful when using a General MIDI synthesizer like fluidsynth.\n        dump_midi: if True, print all incoming MIDI for debugging purposes\n\n        balance_sample: choose instruments which have played less recently\n            ensures that all configured instruments will play.\n        n_recent: number of recent note-on events to consider for above\n        n_margin: amount of 'slack' in the balance_sample calculation\n\n        max_note_len: time in seconds after which to force-release sustained\n            notochord notes.\n        max_time: maximum time in seconds between predicted events.\n            default is the Notochord model's maximum (usually 10 seconds).\n        nominal_time: if True, feed Notochord with its own predicted times\n            instead of the actual elapsed time.\n            May make Notochord more likely to play chords.\n\n        osc_port: optional. if supplied, listen for OSC to set controls\n        osc_host: hostname or IP of OSC sender.\n            leave this as empty string to get all traffic on the port\n\n        use_tui: run textual UI.\n        predict_player: forecasted next events can be for player.\n            generally should be true, use balance_sample to force Notochord to\n            play.\n        auto_query=True, # query notochord whenever it is unmuted and there is no pending event. generally should be True unless debugging.\n    \"\"\"\n    if osc_port is not None:\n        osc = OSC(osc_host, osc_port)\n    midi = MIDI(midi_in, midi_out)\n\n    ### Textual UI\n    tui = NotoTUI()\n    print = tui.print\n    ###\n\n    # default channel:instrument mappings\n    if player_config is None:\n        player_config = {1:1} # channel 1: grand piano\n    if noto_config is None:\n        noto_config = {2:257} # channel 2: anon\n\n    # convert 1-indexed MIDI channels to 0-indexed here\n    player_map = MIDIConfig({k-1:v for k,v in player_config.items()})\n    noto_map = MIDIConfig({k-1:v for k,v in noto_config.items()})\n\n    if len(player_map.insts &amp; noto_map.insts):\n        print(\"WARNING: Notochord and Player instruments shouldn't overlap\")\n        print('setting to an anonymous instrument')\n        # TODO: set to anon insts without changing mel/drum\n        # respecting anon insts selected for player\n        raise NotImplementedError\n    # TODO:\n    # check for repeated insts/channels\n\n    def warn_inst(i):\n        if i &gt; 128:\n            if i &lt; 257:\n                print(f\"WARNING: drum instrument {i} selected, be sure to select a drum bank in your synthesizer\")\n            else:\n                print(f\"WARNING: instrument {i} is not General MIDI\")\n\n    if send_pc:\n        for c,i in (player_map | noto_map).items():\n            warn_inst(i)\n            midi.program_change(channel=c, program=(i-1)%128)\n\n    # TODO: add arguments for this,\n    # and sensible defaults for drums etc\n    inst_pitch_map = {i: range(128) for i in noto_map.insts | player_map.insts}\n\n    # load notochord model\n    try:\n        noto = Notochord.from_checkpoint(checkpoint)\n        noto.eval()\n        noto.reset()\n    except Exception:\n        print(\"\"\"error loading notochord model\"\"\")\n        raise\n\n    # main stopwatch to track time difference between MIDI events\n    stopwatch = Stopwatch()\n\n    # simple class to hold pending event prediction\n    class Prediction:\n        def __init__(self):\n            self.event = None\n            self.gate = not initial_mute\n    pending = Prediction()\n\n    # query parameters controlled via MIDI / OSC\n    controls = {}\n\n    # tracks held notes, recently played instruments, etc\n    history = NotoPerformance()\n\n    def display_event(tag, memo, inst, pitch, vel, channel, **kw):\n        \"\"\"print an event to the terminal\"\"\"\n        if tag is None:\n            return\n        s = f'{tag}:\\t {inst=:4d}    {pitch=:4d}    {vel=:4d}    {channel=:3d}'\n        if memo is not None:\n            s += f'    ({memo})'\n        tui(note=s)\n\n    def play_event(event, channel, feed=True, send=True, tag=None, memo=None):\n        \"\"\"realize an event as MIDI, terminal display, and Notochord update\"\"\"\n        # normalize values\n        vel = event['vel'] = round(event['vel'])\n        dt = stopwatch.punch()\n        if 'time' not in event or not nominal_time:\n            event['time'] = dt\n\n        # send out as MIDI\n        if send:\n            midi.send(\n                'note_on' if vel &gt; 0 else 'note_off', \n                note=event['pitch'], velocity=vel, channel=channel)\n\n        # feed to NotoPerformance\n        # put a stopwatch in the held_note_data field for tracking note length\n        history.feed(held_note_data=Stopwatch(), channel=channel, **event)\n\n        # print\n        display_event(tag, memo=memo, channel=channel, **event)\n\n        # feed to Notochord\n        if feed:\n            noto.feed(**event)\n\n    # @lock\n    def noto_reset():\n        \"\"\"reset Notochord and end all of its held notes\"\"\"\n        print('RESET')\n\n        # cancel pending predictions\n        pending.event = None\n        tui(prediction=pending.event)\n\n        # end Notochord held notes\n        for (chan,inst,pitch) in history.note_triples:\n            if inst in noto_map.insts:\n                play_event(\n                    dict(inst=inst, pitch=pitch, vel=0),\n                    channel=chan, \n                    feed=False, # skip feeding Notochord since we are resetting it\n                    tag='NOTO', memo='reset')\n        # reset stopwatch\n        stopwatch.punch()\n        # reset notochord state\n        noto.reset()\n        # reset history\n        history.push()\n        # query the fresh notochord for a new prediction\n        if pending.gate:\n            noto_query()\n\n    # @lock\n    def noto_mute(sustain=False):\n        tui.query_one('#mute').label = 'UNMUTE' if pending.gate else 'MUTE'\n        # if sustain:\n        tui.query_one('#sustain').label = 'END SUSTAIN' if pending.gate else 'SUSTAIN'\n\n        pending.gate = not pending.gate\n\n        if sustain:\n            print('END SUSTAIN' if pending.gate else 'SUSTAIN')\n        else:\n            print('UNMUTE' if pending.gate else 'MUTE')\n        # if unmuting, we're done\n        if pending.gate:\n            if sustain:\n                noto_query()\n            return\n        # cancel pending predictions\n        pending.event = None\n        tui(prediction=pending.event)\n\n        if sustain:\n            return\n\n        # end+feed all held notes\n        for (chan,inst,pitch) in history.note_triples:\n            if chan in noto_map:\n                play_event(\n                    dict(inst=inst, pitch=pitch, vel=0), \n                    channel=chan, tag='NOTO', memo='mute')\n\n    # query Notochord for a new next event\n    # @lock\n    def noto_query():\n        # check for stuck notes\n        # and prioritize ending those\n        for (_, inst, pitch), sw in history.note_data.items():\n            if (\n                inst in noto_map.insts \n                and sw.read() &gt; max_note_len*(.1+controls.get('steer_duration', 1))\n                ):\n                # query for the end of a note with flexible timing\n                # with profile('query', print=print):\n                t = stopwatch.read()\n                pending.event = noto.query(\n                    next_inst=inst, next_pitch=pitch,\n                    next_vel=0, min_time=t, max_time=t+0.5)\n                print(f'END STUCK NOTE {inst=},{pitch=}')\n                return\n\n        counts = history.inst_counts(\n            n=n_recent, insts=noto_map.insts | player_map.insts)\n        print(counts)\n\n        all_insts = noto_map.insts \n        if predict_player:\n            all_insts = all_insts | player_map.insts\n\n        held_notes = history.held_inst_pitch_map(all_insts)\n\n        steer_time = 1-controls.get('steer_rate', 0.5)\n        steer_pitch = controls.get('steer_pitch', 0.5)\n        steer_density = controls.get('steer_density', 0.5)\n\n        tqt = (max(0,steer_time-0.5), min(1, steer_time+0.5))\n        tqp = (max(0,steer_pitch-0.5), min(1, steer_pitch+0.5))\n\n        # if using nominal time,\n        # *subtract* estimated feed latency to min_time; (TODO: really should\n        #   set no min time when querying, use stopwatch when re-querying...)\n        # if using actual time, *add* estimated query latency\n        time_offset = -5e-3 if nominal_time else 10e-3\n        min_time = stopwatch.read()+time_offset\n\n        # balance_sample: note-ons only from instruments which have played less\n        bal_insts = set(counts.index[counts &lt;= counts.min()+n_margin])\n        if balance_sample and len(bal_insts)&gt;0:\n            insts = bal_insts\n        else:\n            insts = all_insts\n\n        # VTIP is better for time interventions,\n        # VIPT is better for instrument interventions\n        # could decide probabilistically based on value of controls + insts...\n        if insts==all_insts:\n            query_method = noto.query_vtip\n        else:\n            query_method = noto.query_vipt\n\n        # print(f'considering {insts} for note_on')\n        # use only currently selected instruments\n        note_on_map = {\n            i: set(inst_pitch_map[i])-set(held_notes[i]) # exclude held notes\n            for i in insts\n        }\n        # use any instruments which are currently holding notes\n        note_off_map = {\n            i: set(ps)&amp;set(held_notes[i]) # only held notes\n            for i,ps in inst_pitch_map.items()\n        }\n\n        max_t = None if max_time is None else max(max_time, min_time+0.2)\n\n        pending.event = query_method(\n            note_on_map, note_off_map,\n            min_time=min_time, max_time=max_t,\n            truncate_quantile_time=tqt,\n            truncate_quantile_pitch=tqp,\n            steer_density=steer_density,\n        )\n\n        # display the predicted event\n        tui(prediction=pending.event)\n\n    #### MIDI handling\n\n    # print all incoming MIDI for debugging\n    if dump_midi:\n        @midi.handle\n        def _(msg):\n            print(msg)\n\n    @midi.handle(type='program_change')\n    def _(msg):\n        \"\"\"Program change events set instruments\"\"\"\n        if msg.channel in player_map:\n            player_map[msg.channel] = msg.program\n        if msg.channel in noto_map:\n            noto_map[msg.channel] = msg.program\n\n    @midi.handle(type='pitchwheel')\n    def _(msg):\n        controls['steer_pitch'] = (msg.pitch+8192)/16384\n        # print(controls)\n\n    # very basic CC handling for controls\n    @midi.handle(type='control_change')\n    def _(msg):\n        \"\"\"CC messages on any channel\"\"\"\n\n        if msg.control==1:\n            controls['steer_pitch'] = msg.value/127\n            print(f\"{controls['steer_pitch']=}\")\n        if msg.control==2:\n            controls['steer_density'] = msg.value/127\n            print(f\"{controls['steer_density']=}\")\n        if msg.control==3:\n            controls['steer_rate'] = msg.value/127\n            print(f\"{controls['steer_rate']=}\")\n\n        if msg.control==4:\n            noto_reset()\n        if msg.control==5:\n            noto_query()\n        if msg.control==6:\n            noto_mute()\n\n    # very basic OSC handling for controls\n    if osc_port is not None:\n        @osc.args('/notochord/improviser/*')\n        def _(route, *a):\n            print('OSC:', route, *a)\n            ctrl = route.split['/'][3]\n            if ctrl=='reset':\n                noto_reset()\n            elif ctrl=='query':\n                noto_query()\n            elif ctrl=='mute':\n                noto_mute()\n            else:\n                assert len(a)==0\n                arg = a[0]\n                assert isinstance(arg, Number)\n                controls[ctrl] = arg\n                print(controls)\n\n    @midi.handle(type=('note_on', 'note_off'))\n    def _(msg):\n        \"\"\"MIDI NoteOn events from the player\"\"\"\n        # if thru and msg.channel not in noto_map.channels:\n            # midi.send(msg)\n\n        if msg.channel not in player_map.channels:\n            return\n\n        inst = player_map[msg.channel]\n        pitch = msg.note\n        vel = msg.velocity if msg.type=='note_on' else 0\n\n        # feed event to Notochord\n        # with profile('feed', print=print):\n        play_event(\n            {'inst':inst, 'pitch':pitch, 'vel':vel}, \n            channel=msg.channel, send=thru, tag='PLAYER')\n\n        # query for new prediction\n        noto_query()\n\n        # send a MIDI reply for latency testing purposes:\n        # if testing: midi.cc(control=3, value=msg.note, channel=15)\n\n    def noto_event():\n        # notochord event happens:\n        event = pending.event\n        inst, pitch, vel = event['inst'], event['pitch'], round(event['vel'])\n\n        # note on which is already playing or note off which is not\n        if (vel&gt;0) == ((inst, pitch) in history.note_pairs): \n            print(f're-query for invalid {vel=}, {inst=}, {pitch=}')\n            noto_query()\n            return\n\n        chan = noto_map.inv(inst)\n        play_event(event, channel=chan, tag='NOTO')\n\n    @repeat(1e-3, lock=True)\n    def _():\n        \"\"\"Loop, checking if predicted next event happens\"\"\"\n        # check if current prediction has passed\n        if (\n            not testing and\n            pending.gate and\n            pending.event is not None and\n            stopwatch.read() &gt; pending.event['time']\n            ):\n            # if so, check if it is a notochord-controlled instrument\n            if pending.event['inst'] in noto_map.insts:\n                # prediction happens\n                noto_event()\n            # query for new prediction\n            if auto_query:\n                noto_query()\n\n    @cleanup\n    def _():\n        \"\"\"end any remaining notes\"\"\"\n        # print(f'cleanup: {notes=}')\n        for (chan,inst,pitch) in history.note_triples:\n        # for (inst,pitch) in notes:\n            if inst in noto_map.insts:\n                midi.note_on(note=pitch, velocity=0, channel=chan)\n\n    @tui.set_action\n    def mute():\n        noto_mute()\n\n    @tui.set_action\n    def sustain():\n        noto_mute(sustain=True)\n\n    @tui.set_action\n    def reset():\n        noto_reset()\n\n    @tui.set_action\n    def query():\n        noto_query()\n\n    if initial_query:\n        noto_query()\n\n    if use_tui:\n        tui.run()\n</code></pre>"},{"location":"reference/notochord/app/morse/","title":"Morse","text":"<p>Notochord + Language Model + Morse Code.</p> Authors <p>Victor Shepardson Intelligent Instruments Lab 2024</p>"},{"location":"reference/notochord/app/morse/#notochord.app.morse.MorseNode","title":"<code>MorseNode</code>","text":"Source code in <code>src/notochord/app/morse.py</code> <pre><code>class MorseNode:\n    def __init__(self, \n            char:str=None, \n            prob:float=0, \n            dit:'MorseNode'=None, \n            dah:'MorseNode'=None\n        ):\n        self.char = char\n        self.prob = prob\n        self.dit = dit\n        self.dah = dah\n\n    def subtree_prob(self):\n        prob = self.prob\n        for d in (self.dit, self.dah):\n            if d is not None: \n                prob += d.subtree_prob()\n        return prob\n\n    def children_prob(self):\n        return self.subtree_prob() - self.prob\n\n    def is_leaf(self):\n        return self.dit is None and self.dah is None\n\n    def nodes(self):\n        nodes = [self]\n        for d in (self.dit, self.dah):\n            if d is not None:\n                nodes.extend(d.nodes())\n        return nodes\n\n    def chars(self):\n        return ''.join(\n            MorseNode.char for MorseNode in self.nodes() if MorseNode.char is not None)\n\n    def children_chars(self):\n        chars = ''\n        for d in (self.dit, self.dah):\n            if d is not None:\n                chars += d.chars()\n        return chars\n\n    def traverse(self, *seq):\n        if not len(seq):\n            return self\n        b, *seq = seq\n        d = self.dit if b else self.dah\n        # print(b, seq, d)\n        return MorseNode() if d is None else d.traverse(*seq)\n\n    def __repr__(self):\n        s = f'{self.char} {self.prob}'\n        for d in (self.dit, self.dah):\n            if d is not None: \n                s += f' {d.char}'\n        return s\n\n    def make_code(self, path=None):\n        \"\"\"return a mapping from characters to morse\"\"\"\n        code = {}\n        if path is None: path = tuple()\n        if self.char is not None:\n            code[self.char] = path\n        for d,b in ((self.dit, True), (self.dah, False)):\n            if d is not None:\n                code.update(d.make_code(path+(b,)))\n        return code\n\n    @classmethod\n    def make_tree(cls):\n        return MorseNode(\n            dit=MorseNode('e', 12.7,\n                dit=MorseNode('i', 7.0,\n                    dit=MorseNode('s', 6.3,\n                        dit=MorseNode('h', 6.1,\n                            dit=MorseNode('5'),\n                            dah=MorseNode('4'),\n                        ),\n                        dah=MorseNode('v', 1.0,\n                            # dit=MorseNode('\u015d'),\n                            dah=MorseNode('3'),\n                        ),\n                    ),\n                    dah=MorseNode('u', 2.8,\n                        dit=MorseNode('f', 2.2,\n                            dit=MorseNode('\u00e9'),\n                            dah=MorseNode('4'),\n                        ),\n                        dah=MorseNode(#'\u00fc',\n                            dit=MorseNode('\u00f0',\n                                dit=MorseNode('?')\n                            ),\n                            dah=MorseNode('2'),\n                        ),\n                    ),\n                ),\n                dah=MorseNode('a', 8.2,\n                    dit=MorseNode('r', 6.0,\n                        dit=MorseNode('l', 4.0,\n                            # dit=MorseNode(),\n                            dah=MorseNode(#'\u00e8',\n                                dit=MorseNode('\"')\n                            ),\n                        ),\n                        dah=MorseNode('\u00e6',\n                            dit=MorseNode('+',\n                                dah=MorseNode('.')\n                            )\n                        ),\n                    ),\n                    dah=MorseNode('w', 2.4,\n                        dit=MorseNode('p', 1.9,\n                            dit=MorseNode('\u00fe'),\n                            dah=MorseNode(#'\u00e0',\n                                dit=MorseNode('@'),\n                            ),\n                        ),\n                        dah=MorseNode('j', 0.2,\n                            # dit=MorseNode('\u0135'),\n                            dah=MorseNode('1',\n                                dit=MorseNode(\"'\")\n                            ),\n                        ),\n                    ),\n                ),\n            ),\n            dah=MorseNode('t', 9.1,\n                dit=MorseNode('n', 6.7,\n                    dit=MorseNode('d', 4.3,\n                        dit=MorseNode('b', 1.5,\n                            dit=MorseNode('6',\n                                dah=MorseNode('-')\n                            ),\n                            dah=MorseNode('='),\n                        ),\n                        dah=MorseNode('x', 0.2,\n                            dit=MorseNode('/')\n                        ),\n                    ),\n                    dah=MorseNode('k', 0.8,\n                        dit=MorseNode('c', 2.8,\n                            # dit=MorseNode('\u00e7'),\n                            dah=MorseNode(#empty MorseNode\n                                dit=MorseNode(';'),\n                                dah=MorseNode('!')\n                            ),\n                        ),\n                        dah=MorseNode('y', 2.0,\n                            dit=MorseNode('(',\n                                dah=MorseNode(')')\n                            )\n                        ),\n                    ),\n                ),\n                dah=MorseNode('m', 2.4,\n                    dit=MorseNode('g', 2.0,\n                        dit=MorseNode('z', 0.1,\n                            dit=MorseNode('7'),\n                            dah=MorseNode(\n                                dah=MorseNode(',')\n                            ),\n                        ),\n                        dah=MorseNode('q', 0.1),\n                    ),\n                    dah=MorseNode('o', 7.5,\n                        dit=MorseNode('\u00f6',\n                            dit=MorseNode('8',\n                                dit=MorseNode(':')\n                            ),         \n                        ),\n                        dah=MorseNode(\n                            dit=MorseNode('9'),\n                            dah=MorseNode('0')\n                        ),\n                    ),\n                ),\n            )\n        )\n</code></pre>"},{"location":"reference/notochord/app/morse/#notochord.app.morse.MorseNode.make_code","title":"<code>make_code(path=None)</code>","text":"<p>return a mapping from characters to morse</p> Source code in <code>src/notochord/app/morse.py</code> <pre><code>def make_code(self, path=None):\n    \"\"\"return a mapping from characters to morse\"\"\"\n    code = {}\n    if path is None: path = tuple()\n    if self.char is not None:\n        code[self.char] = path\n    for d,b in ((self.dit, True), (self.dah, False)):\n        if d is not None:\n            code.update(d.make_code(path+(b,)))\n    return code\n</code></pre>"},{"location":"reference/notochord/app/prompt/","title":"Prompt","text":"Authors <p>Victor Shepardson Intelligent Instruments Lab 2024</p>"},{"location":"reference/notochord/app/prompt/#notochord.app.prompt.main","title":"<code>main(midi_file, checkpoint='notochord-latest.ckpt', start_tokens=True)</code>","text":"<p>Run the contents of a MIDI file through a Notochord model and cache the results.</p> <p>Parameters:</p> Name Type Description Default <code>midi_file</code> <p>path to a MIDI file to prompt with.</p> required <code>checkpoint</code> <p>path to Notochord model.</p> <code>'notochord-latest.ckpt'</code> <code>start_tokens</code> <p>whether to include send start-of-sequence tokens in prompt</p> <code>True</code> Source code in <code>src/notochord/app/prompt.py</code> <pre><code>def main(\n        midi_file, \n        checkpoint=\"notochord-latest.ckpt\",\n        start_tokens=True\n        ):\n    \"\"\"\n    Run the contents of a MIDI file through a Notochord model and cache the results.\n\n    Args:\n        midi_file: path to a MIDI file to prompt with.\n        checkpoint: path to Notochord model.\n        start_tokens: whether to include send start-of-sequence tokens in prompt\n    \"\"\"\n    print(f'{midi_file=} {checkpoint=}')\n    noto = Notochord.from_checkpoint(checkpoint)\n    noto.eval()\n    noto.reset(start=start_tokens)\n    _, inst_map = noto.prompt(midi_file)\n    print('MIDI channel to instrument:', inst_map)\n</code></pre>"},{"location":"reference/notochord/app/server/","title":"Server","text":"Authors <p>Victor Shepardson Jack Armitage Intelligent Instruments Lab 2022</p>"},{"location":"reference/notochord/app/simple_harmonizer/","title":"Simple harmonizer","text":"<p>Notochord MIDI harmonizer server. Each note from the player produces a harmonizing note from Notochord.</p> Authors <p>Victor Shepardson Intelligent Instruments Lab 2023</p>"},{"location":"reference/notochord/app/txalaparta/","title":"Txalaparta","text":"<p>Notochord MIDI co-improviser server. Notochord plays different instruments along with the player.</p> Authors <p>Victor Shepardson Intelligent Instruments Lab 2023</p>"},{"location":"reference/notochord/app/txalaparta/#notochord.app.txalaparta.NotoTUI","title":"<code>NotoTUI</code>","text":"<p>             Bases: <code>TUI</code></p> Source code in <code>src/notochord/app/txalaparta.py</code> <pre><code>class NotoTUI(TUI):\n    CSS_PATH = 'improviser.css'\n\n    BINDINGS = [\n        (\"m\", \"mute\", \"Mute Notochord\"),\n        (\"q\", \"query\", \"Re-query Notochord\"),\n        (\"r\", \"reset\", \"Reset Notochord\")]\n\n    def compose(self):\n        \"\"\"Create child widgets for the app.\"\"\"\n        yield Header()\n        yield self.std_log\n        yield NotoLog(id='note')\n        yield NotoPrediction(id='prediction')\n        yield NotoControl()\n        yield Footer()\n</code></pre>"},{"location":"reference/notochord/app/txalaparta/#notochord.app.txalaparta.NotoTUI.compose","title":"<code>compose()</code>","text":"<p>Create child widgets for the app.</p> Source code in <code>src/notochord/app/txalaparta.py</code> <pre><code>def compose(self):\n    \"\"\"Create child widgets for the app.\"\"\"\n    yield Header()\n    yield self.std_log\n    yield NotoLog(id='note')\n    yield NotoPrediction(id='prediction')\n    yield NotoControl()\n    yield Footer()\n</code></pre>"},{"location":"reference/notochord/app/txalaparta/#notochord.app.txalaparta.main","title":"<code>main(checkpoint='txala-latest.ckpt', player_config=None, noto_config=None, pitch_set=(41, 43, 45), vel_range=(80, 120), initial_mute=False, initial_query=False, midi_in=None, midi_out=None, thru=False, dump_midi=False, input_latency=0.005, rhythm_temp=0.9, timing_temp=0.05, steer_rate=0.55, auto_reset=True, start_after=2, max_run=5, min_run=1, min_time=0, max_time=None, nominal_time=False, backoff_time=0.05, osc_port=None, osc_host='', use_tui=True, predict_player=True, auto_query=True, testing=False, verbose=0)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <p>path to notochord model checkpoint.</p> <code>'txala-latest.ckpt'</code> <code>player_config</code> <code>Dict[int, int]</code> <p>mapping from MIDI channels to MIDI instruments controlled by the player.</p> <code>None</code> <code>noto_config</code> <code>Dict[int, int]</code> <p>mapping from MIDI channels to MIDI instruments controlled by notochord. Both indexed from 1. instruments should be different from the player instruments. channels should be different unless different ports are used. MIDI channels and General MIDI instruments are indexed from 1.</p> <code>None</code> <code>pitch_set</code> <p>collection of MIDI pitches for the txalaparta boards</p> <code>(41, 43, 45)</code> <code>vel_range</code> <p>range of velocities used</p> <code>(80, 120)</code> <code>initial_mute</code> <p>start Notochord muted so it won't play with input.</p> <code>False</code> <code>initial_query</code> <p>query Notochord immediately so it plays even without input.</p> <code>False</code> <code>midi_in</code> <code>Optional[str]</code> <p>MIDI ports for player input.  default is to use all input ports. can be comma-separated list of ports.</p> <code>None</code> <code>midi_out</code> <code>Optional[str]</code> <p>MIDI ports for Notochord output.  default is to use only virtual 'From iipyper' port. can be comma-separated list of ports.</p> <code>None</code> <code>thru</code> <p>if True, copy incoming MIDI to output ports. only makes sense if input and output ports are different.</p> <code>False</code> <code>dump_midi</code> <p>if True, print all incoming MIDI for debugging purposes</p> <code>False</code> <code>min_time</code> <p>minimum time in seconds between predicted events. default is 0.</p> <code>0</code> <code>max_time</code> <p>maximum time in seconds between predicted events. default is the Notochord model's maximum (usually 10 seconds).</p> <code>None</code> <code>nominal_time</code> <p>if True, feed Notochord with its own predicted times instead of the actual elapsed time.</p> <code>False</code> <code>backoff_time</code> <p>time in seconds to wait before querying, if a predicted player event doesn't occur</p> <code>0.05</code> <code>input_latency</code> <p>estimated input latency in seconds</p> <code>0.005</code> <code>rhythm_temp</code> <p>temperature for sampling time mixture component,</p> <code>0.9</code> <code>timing_temp</code> <p>temperature for sampling time component distribution</p> <code>0.05</code> <code>steer_rate</code> <p>0.5 is unbiased; values &gt;0.5 tend faster, &lt;0.5 tend slower</p> <code>0.55</code> <code>auto_reset</code> <p>reset the notochord model when it predicts end of sequence</p> <code>True</code> <code>start_after</code> <p>don't sample notochord until this many total events</p> <code>2</code> <code>max_run</code> <p>player of the next hit must change after this many  consecutive hits. can also be a dict with different values  for p1 and p2</p> <code>5</code> <code>min_run</code> <p>player of the next hit cannot change before this many  consecutive hits. can also be a dict with different values  for p1 and p2</p> <code>1</code> <code>osc_port</code> <p>optional. if supplied, listen for OSC to set controls</p> <code>None</code> <code>osc_host</code> <p>hostname or IP of OSC sender. leave this as empty string to get all traffic on the port</p> <code>''</code> <code>use_tui</code> <p>run textual UI.</p> <code>True</code> <code>predict_player</code> <p>forecasted next events can be for player. generally should be True; instead use balance_sample to force Notochord to play.</p> <code>True</code> <code>auto_query</code> <p>query notochord whenever it is unmuted and there is no pending event. generally should be True unless debugging.</p> <code>True</code> Source code in <code>src/notochord/app/txalaparta.py</code> <pre><code>def main(\n    checkpoint=\"txala-latest.ckpt\", # Notochord checkpoint\n    player_config:Dict[int,int]=None, # map MIDI channel : GM instrument\n    noto_config:Dict[int,int]=None, # map MIDI channel : GM instrument\n    pitch_set=(41,43,45), # MIDI pitches used\n    vel_range=(80,120),\n    # scale_vel=False,\n\n    initial_mute=False, # start with Notochord muted\n    initial_query=False, # let Notochord start playing immediately\n\n    midi_in:Optional[str]=None, # MIDI port for player input\n    midi_out:Optional[str]=None, # MIDI port for Notochord output\n    thru=False, # copy player input to output\n    dump_midi=False, # print all incoming MIDI\n\n    input_latency=0.005,\n    rhythm_temp=0.9,\n    timing_temp=0.05,\n    steer_rate=0.55,\n\n    auto_reset=True,\n\n    start_after=2, # don't sample notochord until this many total events\n    max_run=5,\n    min_run=1,\n\n    min_time=0,\n    max_time=None, # max time between events\n    nominal_time=False, #feed Notochord with nominal dt instead of actual\n    backoff_time=50e-3, #time to wait when a predicted player event doesn't happen\n\n    osc_port=None, # if supplied, listen for OSC to set controls on this port\n    osc_host='', # leave this as empty string to get all traffic on the port\n\n    use_tui=True, # run textual UI\n    predict_player=True, # forecasted next events can be for player (preserves model distribution, but can lead to Notochord deciding not to play)\n    auto_query=True, # query notochord whenever it is unmuted and there is no pending event. generally should be True except for testing purposes.\n    testing=False,\n    verbose=0\n    ):\n    \"\"\"\n    Args:\n        checkpoint: path to notochord model checkpoint.\n\n        player_config: mapping from MIDI channels to MIDI instruments controlled\n            by the player.\n        noto_config: mapping from MIDI channels to MIDI instruments controlled\n            by notochord. Both indexed from 1.\n            instruments should be different from the player instruments.\n            channels should be different unless different ports are used.\n            MIDI channels and General MIDI instruments are indexed from 1.\n\n        pitch_set: collection of MIDI pitches for the txalaparta boards\n        vel_range: range of velocities used\n\n        initial_mute: start Notochord muted so it won't play with input.\n        initial_query: query Notochord immediately so it plays even without input.\n\n        midi_in: MIDI ports for player input. \n            default is to use all input ports.\n            can be comma-separated list of ports.\n        midi_out: MIDI ports for Notochord output. \n            default is to use only virtual 'From iipyper' port.\n            can be comma-separated list of ports.\n        thru: if True, copy incoming MIDI to output ports.\n            only makes sense if input and output ports are different.\n        dump_midi: if True, print all incoming MIDI for debugging purposes\n\n        min_time: minimum time in seconds between predicted events.\n            default is 0.\n        max_time: maximum time in seconds between predicted events.\n            default is the Notochord model's maximum (usually 10 seconds).\n        nominal_time: if True, feed Notochord with its own predicted times\n            instead of the actual elapsed time.\n        backoff_time: time in seconds to wait before querying,\n            if a predicted player event doesn't occur\n\n        input_latency: estimated input latency in seconds\n        rhythm_temp: temperature for sampling time mixture component,\n        timing_temp: temperature for sampling time component distribution\n        steer_rate: 0.5 is unbiased; values &gt;0.5 tend faster, &lt;0.5 tend slower\n\n        auto_reset: reset the notochord model when it predicts end of sequence\n        start_after: don't sample notochord until this many total events\n        max_run: player of the next hit must change after this many \n            consecutive hits. can also be a dict with different values \n            for p1 and p2\n        min_run: player of the next hit cannot change before this many \n            consecutive hits. can also be a dict with different values \n            for p1 and p2\n\n        osc_port: optional. if supplied, listen for OSC to set controls\n        osc_host: hostname or IP of OSC sender.\n            leave this as empty string to get all traffic on the port\n\n        use_tui: run textual UI.\n        predict_player: forecasted next events can be for player.\n            generally should be True;\n            instead use balance_sample to force Notochord to play.\n        auto_query: query notochord whenever it is unmuted and there is no pending event. generally should be True unless debugging.\n    \"\"\"\n    if osc_port is not None:\n        osc = OSC(osc_host, osc_port)\n    midi = MIDI(midi_in, midi_out)\n\n    ### Textual UI\n    tui = NotoTUI()\n    print = tui.print\n    ###\n\n    if isinstance(max_run, Number):\n        max_run = {'p1':max_run, 'p2':max_run}\n    if isinstance(min_run, Number):\n        min_run = {'p1':min_run, 'p2':min_run}\n\n    max_run[None] = 99\n    min_run[None] = 0\n\n    if isinstance(max_time, Number) or max_time is None:\n        max_time = {'p1':max_time, 'p2':max_time}\n    if isinstance(min_time, Number) or min_time is None:\n        min_time = {'p1':min_time, 'p2':min_time}\n    max_time[None] = None\n    min_time[None] = 0\n\n    # default channel:instrument mappings\n    if player_config is None:\n        # player_config = {1:265,2:266}\n        player_config = {1:290,2:291}\n    if noto_config is None:\n        # noto_config = {3:267,4:268}\n        noto_config = {3:292,4:293}\n\n    state = {\n        'run_length': 0,\n        'total_count': 0,\n        'last_side': None\n    }\n\n    # assuming 2 player 2 notochord or else 4 notochord\n    if len(player_config)==2:\n        p1_insts = set(player_config.values())\n        p2_insts = set(noto_config.values())\n    elif len(noto_config)==4:\n        # if all notochord, ass\n        insts = sorted(noto_config.values())\n        p1_insts = set(insts[:2])\n        p2_insts = set(insts[2:])\n    else:\n        raise ValueError(\"\"\"\n            invalid config.\n            there should be two player and two notochord hands, \n            or four notochord hands.\n        \"\"\")\n\n    # convert 1-indexed MIDI channels to 0-indexed here\n    player_map = MIDIConfig({k-1:v for k,v in player_config.items()})\n    noto_map = MIDIConfig({k-1:v for k,v in noto_config.items()})\n\n    if len(player_map.insts &amp; noto_map.insts):\n        print(\"WARNING: Notochord and Player instruments shouldn't overlap\")\n        print('setting to an anonymous instrument')\n        # TODO: set to anon insts without changing mel/drum\n        # respecting anon insts selected for player\n        raise NotImplementedError\n    # TODO:\n    # check for repeated insts/channels\n\n    def warn_inst(i):\n        if i &gt; 128:\n            if i &lt; 257:\n                print(f\"WARNING: drum instrument {i} selected, be sure to select a drum bank in your synthesizer\")\n            else:\n                print(f\"WARNING: instrument {i} is not General MIDI\")\n\n    # load notochord model\n    try:\n        noto = Notochord.from_checkpoint(checkpoint)\n        noto.eval()\n        noto.reset()\n    except Exception:\n        print(\"\"\"error loading notochord model\"\"\")\n        raise\n\n    # main stopwatch to track time difference between MIDI events\n    stopwatch = Stopwatch()\n\n    # simple class to hold pending event prediction\n    class Prediction:\n        def __init__(self):\n            self.event = None\n            self.gate = not initial_mute\n    pending = Prediction()\n\n    # query parameters controlled via MIDI / OSC\n    controls = {}\n    if steer_rate is not None:\n        controls['steer_rate'] = steer_rate\n\n    # tracks held notes, recently played instruments, etc\n    # history = NotoPerformance()\n\n    def display_event(tag, memo, inst, pitch, vel, channel, **kw):\n        \"\"\"print an event to the terminal\"\"\"\n        if tag is None:\n            return\n        s = f'{tag}:\\t {inst=:4d}    {pitch=:4d}    {vel=:4d}    {channel=:3d}'\n        if memo is not None:\n            s += f'    ({memo})'\n        s += '\\n'\n        tui.defer(note=s)\n\n    def play_event(event, channel, feed=True, send=True, tag=None, memo=None):\n        \"\"\"realize an event as MIDI, terminal display, and Notochord update\"\"\"\n        # normalize values\n        vel = event['vel'] = round(event['vel'])\n        dt = stopwatch.punch(latency=input_latency if tag=='PLAYER' else 0)\n\n        if 'time' not in event or not nominal_time:\n            event['time'] = dt\n\n        # side = 'player' if event['inst'] in player_config.values() else 'noto'\n        side = 'p1' if event['inst'] in p1_insts else 'p2'\n        if side==state['last_side']:\n            state['run_length'] += 1\n        else:\n            state['run_length'] = 1\n\n        state['total_count'] += 1\n\n        state['last_side'] = side\n        if verbose &gt; 0:\n            print(f'{state[\"run_length\"]=}')\n\n        # send out as MIDI\n        if send:\n            midi.send(\n                'note_on' if vel &gt; 0 else 'note_off', \n                note=event['pitch'], velocity=vel, channel=channel)\n\n        # print\n        display_event(tag, memo=memo, channel=channel, **event)\n\n        # feed to Notochord\n        if feed:\n            noto.feed(**event)\n\n    # @lock\n    def noto_reset(query=True):\n        \"\"\"reset Notochord and end all of its held notes\"\"\"\n        print('RESET')\n\n        # cancel pending predictions\n        pending.event = None\n        tui.defer(prediction=pending.event)\n\n        # reset stopwatch\n        stopwatch.punch()\n        # reset notochord state\n        noto.reset()\n\n        state['total_count'] = 0\n        # reset history\n        # history.push()\n        # query the fresh notochord for a new prediction\n        if pending.gate and query:\n            noto_query()\n\n    # @lock\n    def noto_mute():\n        pending.gate = not pending.gate\n        print('UNMUTE' if pending.gate else 'MUTE')\n        # if unmuting, we're done\n        if pending.gate:\n            return\n        # cancel pending predictions\n        pending.event = None\n        tui(prediction=pending.event)\n\n    # query Notochord for a new next event\n    # @lock\n    def noto_query(delay=0, predict_noto=True):\n\n        all_insts = noto_map.insts \n        if predict_player:\n            all_insts |= player_map.insts\n\n        steer_time = 1-controls.get('steer_rate', 0.5)\n        tqt = (max(0,steer_time-0.5), min(1, steer_time+0.5))\n\n        if verbose &gt; 1:\n            print(tqt)\n\n        last_side = state['last_side']\n        # if using nominal time,\n        # *subtract* estimated feed latency to min_time; (TODO: really should\n        #   set no min time when querying, use stopwatch when re-querying...)\n        # if using actual time, *add* estimated query latency\n        time_offset = -5e-3 if nominal_time else 10e-3\n        min_t = max(min_time[last_side], stopwatch.read()+time_offset+delay)\n\n\n        # print(state)\n        if state['total_count'] &lt; start_after or not predict_noto:\n            insts = player_map.insts\n        else:\n            insts = all_insts\n            last_side_insts = p1_insts if last_side=='p1' else p2_insts\n            if state['run_length'] &gt;= max_run[last_side]:\n                # if state['last_side']=='noto':\n                    # s = set(noto_config.values()) \n                # else:\n                    # s = set(player_config.values()) \n                insts = insts - last_side_insts\n            elif state['run_length'] &lt; min_run[last_side]:\n                insts = last_side_insts\n\n\n        if len(insts)==0:\n            insts = all_insts\n\n        if verbose &gt; 1:\n            print(f'{insts=}')\n\n\n        if len(insts) &gt; 2:\n            # in this case *only* time is messed with,\n            # so if we sample time first,\n            # the rest can be properly conditioned on it\n            query_method = noto.query_tipv_onsets\n        else:\n            # in this case, instrument and time have both been constrained,\n            # and we can't sample the true joint distribution,\n            # but we sample instrument first\n            # under the assumption that the instrument constraint is\n            # 'stronger' than the time constraint\n            query_method = noto.query_itpv_onsets\n\n        max_t = max_time[last_side]\n        max_t = None if max_t is None else max(max_t, min_t+0.03)\n\n        if verbose &gt; 1:\n            print(f'{min_time=}, {max_t=}')\n\n        pending.event = query_method(\n            include_pitch=pitch_set,\n            include_inst=list(insts),\n            min_time=min_t, max_time=max_t,\n            truncate_quantile_time=tqt,\n            min_vel=vel_range[0], max_vel=vel_range[1],\n            rhythm_temp=rhythm_temp,\n            timing_temp=timing_temp,\n        )\n        # display the predicted event\n        tui.defer(prediction=pending.event)\n\n    #### MIDI handling\n\n    # print all incoming MIDI for debugging\n    if dump_midi:\n        @midi.handle\n        def _(msg):\n            print(msg)\n\n    # @midi.handle(type='program_change')\n    # def _(msg):\n    #     \"\"\"Program change events set instruments\"\"\"\n    #     if msg.channel in player_map:\n    #         player_map[msg.channel] = msg.program\n    #     if msg.channel in noto_map:\n    #         noto_map[msg.channel] = msg.program\n\n    # @midi.handle(type='pitchwheel')\n    # def _(msg):\n    #     controls['steer_pitch'] = (msg.pitch+8192)/16384\n        # print(controls)\n\n    # very basic CC handling for controls\n    @midi.handle(type='control_change')\n    def _(msg):\n        \"\"\"CC messages on any channel\"\"\"\n\n        # if msg.control==1:\n        #     controls['steer_pitch'] = msg.value/127\n        #     print(f\"{controls['steer_pitch']=}\")\n        # if msg.control==2:\n        #     controls['steer_density'] = msg.value/127\n        #     print(f\"{controls['steer_density']=}\")\n        if msg.control==3:\n            controls['steer_rate'] = msg.value/127\n            if verbose &gt; 0:\n                print(f\"{controls['steer_rate']=}\")\n\n        if msg.control==4:\n            noto_reset()\n        if msg.control==5:\n            noto_query()\n        if msg.control==6:\n            noto_mute()\n\n    # very basic OSC handling for controls\n    if osc_port is not None:\n        @osc.args('/notochord/improviser/*')\n        def _(route, *a):\n            print('OSC:', route, *a)\n            ctrl = route.split['/'][3]\n            if ctrl=='reset':\n                noto_reset()\n            elif ctrl=='query':\n                noto_query()\n            elif ctrl=='mute':\n                noto_mute()\n            else:\n                assert len(a)==0\n                arg = a[0]\n                assert isinstance(arg, Number)\n                controls[ctrl] = arg\n                print(controls)\n\n    # @midi.handle(type=('note_on', 'note_off'))\n    @midi.handle(type='note_on')\n    def _(msg):\n        \"\"\"MIDI NoteOn events from the player\"\"\"\n        # if thru and msg.channel not in noto_map.channels:\n            # midi.send(msg)\n\n        if msg.channel not in player_map.channels:\n            return\n\n        inst = player_map[msg.channel]\n        pitch = msg.note\n        vel = msg.velocity\n\n        if vel &gt; 0:\n            # feed event to Notochord\n            # with profile('feed', print=print):\n            play_event(\n                {'inst':inst, 'pitch':pitch, 'vel':vel}, \n                channel=msg.channel, send=thru, tag='PLAYER (p1)')\n\n            # query for new prediction\n            noto_query()\n\n        # send a MIDI reply for latency testing purposes:\n        # if testing: midi.cc(control=3, value=msg.note, channel=15)\n\n    def noto_event():\n        # notochord event happens:\n        event = pending.event\n        inst, pitch, vel = event['inst'], event['pitch'], round(event['vel'])\n\n        # note on which is already playing or note off which is not\n        # if (vel&gt;0) == ((inst, pitch) in history.note_pairs): \n        #     print(f're-query for invalid {vel=}, {inst=}, {pitch=}')\n        #     noto_query()\n        #     return\n\n        chan = noto_map.inv(inst)\n        p = 'p1' if inst in p1_insts else 'p2'\n        play_event(event, channel=chan, tag=f'NOTO ({p})')\n\n    @repeat(1e-3, lock=True)\n    def _():\n        \"\"\"Loop, checking if predicted next event happens\"\"\"\n        # check if current prediction has passed\n        if (\n            not testing and\n            pending.gate and\n            pending.event is not None and\n            stopwatch.read() &gt; pending.event['time']\n            ):\n            e = pending.event\n            if e['time'] &gt;= noto.time_dist.hi.item():\n                if auto_reset:\n                    noto_reset(query=False)\n            # if so, check if it is a notochord-controlled instrument\n            delay = 0\n            predict_noto = True\n            if e['inst'] in noto_map.insts:\n                # prediction happens\n                noto_event()\n            else:\n                # if last event was noto, still predict player only\n                if state['last_side']=='noto':\n                    predict_noto = False\n                else:\n                    # otherwise, hesitate before possibly predicting noto turn\n                    delay = backoff_time\n            # query for new prediction\n            # print(pending.event)\n            pending.event = None\n            if ('end' in e and random.random() &lt; e['end']):\n                print('END')\n                if auto_reset:\n                    noto_reset(query=False)\n            elif auto_query:\n                noto_query(delay=delay, predict_noto=predict_noto)\n\n    @cleanup\n    def _():\n        \"\"\"\"\"\"\n        pass\n        # print(f'cleanup: {notes=}')\n        # for (chan,inst,pitch) in history.note_triples:\n        # # for (inst,pitch) in notes:\n        #     if inst in noto_map.insts:\n        #         midi.note_on(note=pitch, velocity=0, channel=chan)\n\n    @tui.set_action\n    def mute():\n        noto_mute()\n\n    @tui.set_action\n    def reset():\n        noto_reset()\n\n    @tui.set_action\n    def query():\n        noto_query()\n\n    if initial_query:\n        noto_query()\n\n    if use_tui:\n        tui.run()\n</code></pre>"}]}